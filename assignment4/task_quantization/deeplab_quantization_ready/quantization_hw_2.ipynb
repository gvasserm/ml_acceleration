{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делаем Quantization Aware Training. Используем готовый трейнплуп от pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvasserm/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "from torch.ao.quantization.quantize_fx import convert_fx\n",
    "from torch.ao.quantization.quantize_fx import fuse_fx\n",
    "from torch.optim.lr_scheduler import PolynomialLR\n",
    "from torchvision.models.segmentation import DeepLabV3_MobileNet_V3_Large_Weights, deeplabv3_mobilenet_v3_large\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "from quantization_utils.fake_quantization import fake_quantization\n",
    "from quantization_utils.static_quantization import quantize_static\n",
    "from train import evaluate\n",
    "from train import get_dataset\n",
    "from train import train_one_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тут берём из train.py скрипт main() и вытаскиваем трейн луп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(inputs, target):\n",
    "    losses = {}\n",
    "    for name, x in inputs.items():\n",
    "        losses[name] = nn.functional.cross_entropy(x, target, ignore_index=255)\n",
    "\n",
    "    if len(losses) == 1:\n",
    "        return losses[\"out\"]\n",
    "\n",
    "    return losses[\"out\"] + 0.5 * losses[\"aux\"]\n",
    "\n",
    "def train_one_epoch(student_model, teacher_model, criterion, optimizer, data_loader, lr_scheduler, device, epoch, print_freq, scaler=None):\n",
    "    base_k = 0.5\n",
    "    KD_k = 0.5\n",
    "\n",
    "    student_model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "    for image, target in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            student_out = student_model(image)\n",
    "            if teacher_model is not None:\n",
    "                teacher_out = teacher_model(image)\n",
    "                #KD with last layer logits\n",
    "                KDloss1 = nn.functional.mse_loss(student_out['out'], teacher_out['out'])\n",
    "                #Lets use also auxilary loss in KD\n",
    "                KDloss2 = nn.functional.mse_loss(student_out['aux'], teacher_out['aux'])\n",
    "                KDloss = KDloss1 + 0.5*KDloss2\n",
    "            else:\n",
    "                KDloss = 0\n",
    "            loss = base_k*criterion(student_out, target) + KD_k*KDloss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=loss.item(), lr=optimizer.param_groups[0][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(student_model, teacher_model, args):\n",
    "\n",
    "    if args.output_dir:\n",
    "        utils.mkdir(args.output_dir)\n",
    "\n",
    "    utils.init_distributed_mode(args)\n",
    "\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    #torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "    dataset, num_classes = get_dataset(args, is_train=True)\n",
    "    dataset_test, _ = get_dataset(args, is_train=False)\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=args.workers,\n",
    "        collate_fn=utils.collate_fn,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, \n",
    "        batch_size=1, \n",
    "        sampler=test_sampler, \n",
    "        num_workers=args.workers, \n",
    "        collate_fn=utils.collate_fn\n",
    "    )\n",
    "\n",
    "    student_model.to(device)\n",
    "    if teacher_model is not None:\n",
    "        teacher_model.to(device)\n",
    "        teacher_model.eval()\n",
    "\n",
    "    model_without_ddp = student_model\n",
    "\n",
    "    params_to_optimize = [\n",
    "        {\"params\": [p for p in model_without_ddp.backbone.parameters() if p.requires_grad]},\n",
    "        {\"params\": [p for p in model_without_ddp.classifier.parameters() if p.requires_grad]},\n",
    "    ]\n",
    "    if args.aux_loss:\n",
    "        params = [p for p in model_without_ddp.aux_classifier.parameters() if p.requires_grad]\n",
    "        params_to_optimize.append({\"params\": params, \"lr\": args.lr * 10})\n",
    "    \n",
    "    optimizer = torch.optim.SGD(params_to_optimize, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler() if args.amp else None\n",
    "\n",
    "    iters_per_epoch = len(data_loader)\n",
    "    main_lr_scheduler = PolynomialLR(\n",
    "        optimizer, total_iters=iters_per_epoch * (args.epochs - args.lr_warmup_epochs), power=0.9\n",
    "    )\n",
    "\n",
    "    if args.lr_warmup_epochs > 0:\n",
    "        warmup_iters = iters_per_epoch * args.lr_warmup_epochs\n",
    "        args.lr_warmup_method = args.lr_warmup_method.lower()\n",
    "        if args.lr_warmup_method == \"linear\":\n",
    "            warmup_lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "                optimizer, start_factor=args.lr_warmup_decay, total_iters=warmup_iters\n",
    "            )\n",
    "        elif args.lr_warmup_method == \"constant\":\n",
    "            warmup_lr_scheduler = torch.optim.lr_scheduler.ConstantLR(\n",
    "                optimizer, factor=args.lr_warmup_decay, total_iters=warmup_iters\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"Invalid warmup lr method '{args.lr_warmup_method}'. Only linear and constant are supported.\"\n",
    "            )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "            optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[warmup_iters]\n",
    "        )\n",
    "    else:\n",
    "        lr_scheduler = main_lr_scheduler\n",
    "\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume, map_location=\"cpu\", weights_only=True)\n",
    "        model_without_ddp.load_state_dict(checkpoint[\"model\"], strict=not args.test_only)\n",
    "        if not args.test_only:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "            lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
    "            args.start_epoch = checkpoint[\"epoch\"] + 1\n",
    "            if args.amp:\n",
    "                scaler.load_state_dict(checkpoint[\"scaler\"])\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        if args.distributed:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "        train_one_epoch(student_model, teacher_model, criterion, optimizer, data_loader, lr_scheduler, device, epoch, args.print_freq, scaler)\n",
    "        confmat = evaluate(student_model, data_loader_test, device=device, num_classes=num_classes)\n",
    "        print(confmat)\n",
    "        checkpoint = {\n",
    "            \"model\": model_without_ddp.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"args\": args,\n",
    "        }\n",
    "        if args.amp:\n",
    "            checkpoint[\"scaler\"] = scaler.state_dict()\n",
    "        utils.save_on_master(checkpoint, os.path.join(args.output_dir, f\"model_{epoch}.pth\"))\n",
    "        utils.save_on_master(checkpoint, os.path.join(args.output_dir, \"checkpoint.pth\"))\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print(f\"Training time {total_time_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 16899571712, Reserved: 0, Allocated: 0, Free: 0\n",
      "Namespace(data_path='/home/gvasserm/data/coco2017/', dataset='coco', model='deeplabv3_mobilenet_v3_large', aux_loss=False, device='cuda', batch_size=24, epochs=1, workers=8, lr=0.01, momentum=0.9, weight_decay=0.0001, lr_warmup_epochs=0, lr_warmup_method='linear', lr_warmup_decay=0.01, print_freq=10, output_dir='.', resume='', start_epoch=0, test_only=False, use_deterministic_algorithms=False, world_size=1, dist_url='env://', weights=None, weights_backbone=None, amp=False, backend='pil', use_v2=False)\n",
      "Not using distributed mode\n",
      "loading annotations into memory...\n",
      "Done (t=0.28s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=7.55s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvasserm/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "loading annotations into memory...\n",
      "Done (t=8.08s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvasserm/.local/lib/python3.10/site-packages/torch/ao/quantization/fake_quantize.py:353: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/ReduceAllOps.cpp:72.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n",
      "/home/gvasserm/.local/lib/python3.10/site-packages/torch/ao/quantization/fake_quantize.py:353: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:677.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/3854]  eta: 3:30:55  lr: 0.009997664733582535  loss: 0.3979 (0.3979)  time: 3.2838  data: 1.4601  max mem: 13843\n",
      "Epoch: [0]  [  10/3854]  eta: 1:13:40  lr: 0.009974308733008223  loss: 0.4082 (0.3969)  time: 1.1499  data: 0.1387  max mem: 13869\n",
      "Epoch: [0]  [  20/3854]  eta: 1:07:09  lr: 0.009950946654092182  loss: 0.4076 (0.4009)  time: 0.9394  data: 0.0066  max mem: 13869\n",
      "Epoch: [0]  [  30/3854]  eta: 1:04:43  lr: 0.00992757847938837  loss: 0.3978 (0.4039)  time: 0.9417  data: 0.0067  max mem: 13869\n",
      "Epoch: [0]  [  40/3854]  eta: 1:03:28  lr: 0.009904204191354918  loss: 0.3998 (0.4007)  time: 0.9436  data: 0.0068  max mem: 13869\n",
      "Epoch: [0]  [  50/3854]  eta: 1:02:42  lr: 0.00988082377235332  loss: 0.3928 (0.4021)  time: 0.9478  data: 0.0068  max mem: 13869\n",
      "Epoch: [0]  [  60/3854]  eta: 1:02:08  lr: 0.009857437204647676  loss: 0.4004 (0.4104)  time: 0.9504  data: 0.0069  max mem: 13869\n",
      "Epoch: [0]  [  70/3854]  eta: 1:01:40  lr: 0.009834044470403858  loss: 0.4019 (0.4120)  time: 0.9494  data: 0.0068  max mem: 13869\n",
      "Epoch: [0]  [  80/3854]  eta: 1:01:17  lr: 0.009810645551688736  loss: 0.3791 (0.4090)  time: 0.9489  data: 0.0068  max mem: 13869\n",
      "Epoch: [0]  [  90/3854]  eta: 1:00:59  lr: 0.009787240430469343  loss: 0.3824 (0.4073)  time: 0.9522  data: 0.0070  max mem: 13869\n",
      "Epoch: [0]  [ 100/3854]  eta: 1:00:43  lr: 0.009763829088612074  loss: 0.3772 (0.4051)  time: 0.9555  data: 0.0069  max mem: 13869\n",
      "Epoch: [0]  [ 110/3854]  eta: 1:00:28  lr: 0.009740411507881836  loss: 0.3769 (0.4012)  time: 0.9560  data: 0.0070  max mem: 13869\n",
      "Epoch: [0]  [ 120/3854]  eta: 1:00:15  lr: 0.00971698766994122  loss: 0.3906 (0.4035)  time: 0.9561  data: 0.0070  max mem: 13869\n",
      "Epoch: [0]  [ 130/3854]  eta: 1:00:02  lr: 0.009693557556349642  loss: 0.4077 (0.4029)  time: 0.9566  data: 0.0070  max mem: 13869\n",
      "Epoch: [0]  [ 140/3854]  eta: 0:59:52  lr: 0.009670121148562484  loss: 0.3819 (0.4009)  time: 0.9610  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [ 150/3854]  eta: 0:59:43  lr: 0.009646678427930233  loss: 0.3568 (0.3973)  time: 0.9680  data: 0.0078  max mem: 13869\n",
      "Epoch: [0]  [ 160/3854]  eta: 1:00:00  lr: 0.009623229375697605  loss: 0.3839 (0.3991)  time: 1.0266  data: 0.0080  max mem: 13869\n",
      "Epoch: [0]  [ 170/3854]  eta: 1:00:17  lr: 0.00959977397300264  loss: 0.4077 (0.3974)  time: 1.0927  data: 0.0080  max mem: 13869\n",
      "Epoch: [0]  [ 180/3854]  eta: 1:00:36  lr: 0.009576312200875813  loss: 0.3890 (0.3991)  time: 1.1118  data: 0.0079  max mem: 13869\n",
      "Epoch: [0]  [ 190/3854]  eta: 1:01:02  lr: 0.009552844040239138  loss: 0.3867 (0.4007)  time: 1.1492  data: 0.0079  max mem: 13869\n",
      "Epoch: [0]  [ 200/3854]  eta: 1:01:17  lr: 0.009529369471905236  loss: 0.3867 (0.4000)  time: 1.1574  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [ 210/3854]  eta: 1:01:14  lr: 0.009505888476576406  loss: 0.3701 (0.3973)  time: 1.0922  data: 0.0072  max mem: 13869\n",
      "Epoch: [0]  [ 220/3854]  eta: 1:00:58  lr: 0.009482401034843688  loss: 0.3502 (0.3960)  time: 1.0093  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [ 230/3854]  eta: 1:00:42  lr: 0.00945890712718593  loss: 0.3806 (0.3954)  time: 0.9694  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [ 240/3854]  eta: 1:00:30  lr: 0.009435406733968792  loss: 0.3882 (0.3952)  time: 0.9812  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [ 250/3854]  eta: 1:00:21  lr: 0.009411899835443816  loss: 0.3841 (0.3950)  time: 1.0021  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [ 260/3854]  eta: 1:00:19  lr: 0.00938838641174741  loss: 0.3645 (0.3937)  time: 1.0363  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [ 270/3854]  eta: 1:00:07  lr: 0.009364866442899876  loss: 0.3469 (0.3926)  time: 1.0302  data: 0.0089  max mem: 13869\n",
      "Epoch: [0]  [ 280/3854]  eta: 1:00:07  lr: 0.009341339908804399  loss: 0.3723 (0.3916)  time: 1.0386  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [ 290/3854]  eta: 1:00:02  lr: 0.00931780678924603  loss: 0.3793 (0.3903)  time: 1.0683  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [ 300/3854]  eta: 0:59:59  lr: 0.009294267063890648  loss: 0.3792 (0.3908)  time: 1.0623  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [ 310/3854]  eta: 0:59:54  lr: 0.009270720712283942  loss: 0.3792 (0.3905)  time: 1.0616  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [ 320/3854]  eta: 0:59:47  lr: 0.009247167713850334  loss: 0.3857 (0.3903)  time: 1.0524  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [ 330/3854]  eta: 0:59:37  lr: 0.009223608047891942  loss: 0.3877 (0.3899)  time: 1.0331  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [ 340/3854]  eta: 0:59:29  lr: 0.009200041693587483  loss: 0.3877 (0.3908)  time: 1.0248  data: 0.0073  max mem: 13869\n",
      "Epoch: [0]  [ 350/3854]  eta: 0:59:17  lr: 0.009176468629991188  loss: 0.4206 (0.3918)  time: 1.0131  data: 0.0073  max mem: 13869\n",
      "Epoch: [0]  [ 360/3854]  eta: 0:59:06  lr: 0.00915288883603171  loss: 0.4147 (0.3915)  time: 0.9994  data: 0.0073  max mem: 13869\n",
      "Epoch: [0]  [ 370/3854]  eta: 0:58:58  lr: 0.009129302290510994  loss: 0.3435 (0.3917)  time: 1.0254  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [ 380/3854]  eta: 0:58:53  lr: 0.00910570897210316  loss: 0.3317 (0.3905)  time: 1.0591  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [ 390/3854]  eta: 0:58:47  lr: 0.009082108859353358  loss: 0.3641 (0.3906)  time: 1.0671  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [ 400/3854]  eta: 0:58:40  lr: 0.009058501930676617  loss: 0.3857 (0.3905)  time: 1.0568  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [ 410/3854]  eta: 0:58:33  lr: 0.009034888164356674  loss: 0.3478 (0.3897)  time: 1.0585  data: 0.0073  max mem: 13869\n",
      "Epoch: [0]  [ 420/3854]  eta: 0:58:23  lr: 0.009011267538544785  loss: 0.3580 (0.3897)  time: 1.0409  data: 0.0073  max mem: 13869\n",
      "Epoch: [0]  [ 430/3854]  eta: 0:58:17  lr: 0.00898764003125854  loss: 0.4006 (0.3899)  time: 1.0474  data: 0.0073  max mem: 13869\n",
      "Epoch: [0]  [ 440/3854]  eta: 0:58:12  lr: 0.008964005620380643  loss: 0.3482 (0.3882)  time: 1.0804  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [ 450/3854]  eta: 0:58:04  lr: 0.008940364283657702  loss: 0.3138 (0.3873)  time: 1.0688  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [ 460/3854]  eta: 0:57:53  lr: 0.008916715998698966  loss: 0.3468 (0.3870)  time: 1.0366  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [ 470/3854]  eta: 0:57:42  lr: 0.00889306074297509  loss: 0.3671 (0.3869)  time: 1.0156  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [ 480/3854]  eta: 0:57:32  lr: 0.008869398493816854  loss: 0.3730 (0.3866)  time: 1.0144  data: 0.0072  max mem: 13869\n",
      "Epoch: [0]  [ 490/3854]  eta: 0:57:21  lr: 0.008845729228413876  loss: 0.3813 (0.3864)  time: 1.0174  data: 0.0073  max mem: 13869\n",
      "Epoch: [0]  [ 500/3854]  eta: 0:57:11  lr: 0.00882205292381332  loss: 0.3881 (0.3870)  time: 1.0246  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [ 510/3854]  eta: 0:57:05  lr: 0.00879836955691857  loss: 0.4082 (0.3872)  time: 1.0548  data: 0.0072  max mem: 13869\n",
      "Epoch: [0]  [ 520/3854]  eta: 0:56:58  lr: 0.008774679104487903  loss: 0.3997 (0.3871)  time: 1.0757  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [ 530/3854]  eta: 0:56:48  lr: 0.008750981543133132  loss: 0.3593 (0.3870)  time: 1.0569  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [ 540/3854]  eta: 0:56:38  lr: 0.008727276849318242  loss: 0.3556 (0.3870)  time: 1.0355  data: 0.0073  max mem: 13869\n",
      "Epoch: [0]  [ 550/3854]  eta: 0:56:29  lr: 0.008703564999358013  loss: 0.3394 (0.3868)  time: 1.0310  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [ 560/3854]  eta: 0:56:19  lr: 0.008679845969416613  loss: 0.3719 (0.3868)  time: 1.0314  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [ 570/3854]  eta: 0:56:10  lr: 0.008656119735506182  loss: 0.3765 (0.3867)  time: 1.0389  data: 0.0071  max mem: 13869\n",
      "Epoch: [0]  [ 580/3854]  eta: 0:55:59  lr: 0.00863238627348539  loss: 0.3765 (0.3868)  time: 1.0375  data: 0.0072  max mem: 13869\n",
      "Epoch: [0]  [ 590/3854]  eta: 0:55:51  lr: 0.008608645559057994  loss: 0.3793 (0.3867)  time: 1.0446  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [ 600/3854]  eta: 0:55:44  lr: 0.008584897567771355  loss: 0.3765 (0.3865)  time: 1.0704  data: 0.0078  max mem: 13869\n",
      "Epoch: [0]  [ 610/3854]  eta: 0:55:34  lr: 0.00856114227501496  loss: 0.3892 (0.3868)  time: 1.0642  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [ 620/3854]  eta: 0:55:24  lr: 0.008537379656018907  loss: 0.3898 (0.3870)  time: 1.0386  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [ 630/3854]  eta: 0:55:16  lr: 0.008513609685852366  loss: 0.3641 (0.3869)  time: 1.0469  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [ 640/3854]  eta: 0:55:07  lr: 0.008489832339422045  loss: 0.3674 (0.3867)  time: 1.0604  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [ 650/3854]  eta: 0:55:01  lr: 0.008466047591470622  loss: 0.3811 (0.3865)  time: 1.0857  data: 0.0080  max mem: 13869\n",
      "Epoch: [0]  [ 660/3854]  eta: 0:54:53  lr: 0.008442255416575145  loss: 0.3811 (0.3867)  time: 1.1029  data: 0.0079  max mem: 13869\n",
      "Epoch: [0]  [ 670/3854]  eta: 0:54:47  lr: 0.00841845578914543  loss: 0.4032 (0.3869)  time: 1.1015  data: 0.0078  max mem: 13869\n",
      "Epoch: [0]  [ 680/3854]  eta: 0:54:38  lr: 0.00839464868342244  loss: 0.3948 (0.3864)  time: 1.0852  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [ 690/3854]  eta: 0:54:28  lr: 0.008370834073476623  loss: 0.3547 (0.3860)  time: 1.0479  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [ 700/3854]  eta: 0:54:18  lr: 0.00834701193320625  loss: 0.3422 (0.3857)  time: 1.0392  data: 0.0080  max mem: 13869\n",
      "Epoch: [0]  [ 710/3854]  eta: 0:54:08  lr: 0.008323182236335715  loss: 0.3422 (0.3853)  time: 1.0402  data: 0.0079  max mem: 13869\n",
      "Epoch: [0]  [ 720/3854]  eta: 0:54:00  lr: 0.008299344956413822  loss: 0.3833 (0.3858)  time: 1.0670  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [ 730/3854]  eta: 0:53:51  lr: 0.008275500066812047  loss: 0.4368 (0.3863)  time: 1.0772  data: 0.0078  max mem: 13869\n",
      "Epoch: [0]  [ 740/3854]  eta: 0:53:41  lr: 0.008251647540722783  loss: 0.3545 (0.3862)  time: 1.0487  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [ 750/3854]  eta: 0:53:30  lr: 0.008227787351157551  loss: 0.3328 (0.3856)  time: 1.0369  data: 0.0079  max mem: 13869\n",
      "Epoch: [0]  [ 760/3854]  eta: 0:53:21  lr: 0.008203919470945183  loss: 0.3555 (0.3859)  time: 1.0527  data: 0.0080  max mem: 13869\n",
      "Epoch: [0]  [ 770/3854]  eta: 0:53:12  lr: 0.008180043872730008  loss: 0.4012 (0.3856)  time: 1.0695  data: 0.0079  max mem: 13869\n",
      "Epoch: [0]  [ 780/3854]  eta: 0:53:02  lr: 0.008156160528969995  loss: 0.4061 (0.3859)  time: 1.0524  data: 0.0079  max mem: 13869\n",
      "Epoch: [0]  [ 790/3854]  eta: 0:52:52  lr: 0.008132269411934855  loss: 0.3637 (0.3854)  time: 1.0341  data: 0.0078  max mem: 13869\n",
      "Epoch: [0]  [ 800/3854]  eta: 0:52:42  lr: 0.00810837049370416  loss: 0.3433 (0.3852)  time: 1.0478  data: 0.0079  max mem: 13869\n",
      "Epoch: [0]  [ 810/3854]  eta: 0:52:32  lr: 0.00808446374616539  loss: 0.3517 (0.3850)  time: 1.0525  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [ 820/3854]  eta: 0:52:24  lr: 0.008060549141011989  loss: 0.3281 (0.3843)  time: 1.0710  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [ 830/3854]  eta: 0:52:14  lr: 0.008036626649741382  loss: 0.3281 (0.3841)  time: 1.0742  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [ 840/3854]  eta: 0:52:06  lr: 0.008012696243652945  loss: 0.3883 (0.3845)  time: 1.0692  data: 0.0079  max mem: 13869\n",
      "Epoch: [0]  [ 850/3854]  eta: 0:51:59  lr: 0.007988757893846005  loss: 0.4171 (0.3852)  time: 1.1058  data: 0.0078  max mem: 13869\n",
      "Epoch: [0]  [ 860/3854]  eta: 0:51:50  lr: 0.00796481157121773  loss: 0.3863 (0.3852)  time: 1.1012  data: 0.0078  max mem: 13869\n",
      "Epoch: [0]  [ 870/3854]  eta: 0:51:42  lr: 0.007940857246461071  loss: 0.3863 (0.3855)  time: 1.0951  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [ 880/3854]  eta: 0:51:32  lr: 0.007916894890062618  loss: 0.4074 (0.3858)  time: 1.0906  data: 0.0080  max mem: 13869\n",
      "Epoch: [0]  [ 890/3854]  eta: 0:51:22  lr: 0.007892924472300458  loss: 0.3512 (0.3853)  time: 1.0572  data: 0.0078  max mem: 13869\n",
      "Epoch: [0]  [ 900/3854]  eta: 0:51:11  lr: 0.007868945963241985  loss: 0.3512 (0.3855)  time: 1.0396  data: 0.0080  max mem: 13869\n",
      "Epoch: [0]  [ 910/3854]  eta: 0:51:01  lr: 0.007844959332741697  loss: 0.3855 (0.3854)  time: 1.0359  data: 0.0080  max mem: 13869\n",
      "Epoch: [0]  [ 920/3854]  eta: 0:50:52  lr: 0.007820964550438946  loss: 0.3524 (0.3850)  time: 1.0609  data: 0.0079  max mem: 13869\n",
      "Epoch: [0]  [ 930/3854]  eta: 0:50:44  lr: 0.007796961585755671  loss: 0.3322 (0.3847)  time: 1.0966  data: 0.0078  max mem: 13869\n",
      "Epoch: [0]  [ 940/3854]  eta: 0:50:34  lr: 0.007772950407894082  loss: 0.3410 (0.3848)  time: 1.0925  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [ 950/3854]  eta: 0:50:26  lr: 0.007748930985834328  loss: 0.3795 (0.3850)  time: 1.0917  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [ 960/3854]  eta: 0:50:17  lr: 0.0077249032883321255  loss: 0.3595 (0.3847)  time: 1.1062  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [ 970/3854]  eta: 0:50:08  lr: 0.007700867283916349  loss: 0.3542 (0.3851)  time: 1.0977  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [ 980/3854]  eta: 0:49:59  lr: 0.007676822940886593  loss: 0.3731 (0.3848)  time: 1.0892  data: 0.0078  max mem: 13869\n",
      "Epoch: [0]  [ 990/3854]  eta: 0:49:52  lr: 0.007652770227310703  loss: 0.3731 (0.3848)  time: 1.1230  data: 0.0080  max mem: 13869\n",
      "Epoch: [0]  [1000/3854]  eta: 0:49:45  lr: 0.007628709111022258  loss: 0.3821 (0.3848)  time: 1.1544  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [1010/3854]  eta: 0:49:35  lr: 0.007604639559618026  loss: 0.3840 (0.3848)  time: 1.1045  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [1020/3854]  eta: 0:49:24  lr: 0.007580561540455385  loss: 0.3744 (0.3847)  time: 1.0459  data: 0.0079  max mem: 13869\n",
      "Epoch: [0]  [1030/3854]  eta: 0:49:15  lr: 0.007556475020649699  loss: 0.3961 (0.3850)  time: 1.0629  data: 0.0080  max mem: 13869\n",
      "Epoch: [0]  [1040/3854]  eta: 0:49:08  lr: 0.00753237996707166  loss: 0.4053 (0.3853)  time: 1.1402  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [1050/3854]  eta: 0:49:00  lr: 0.007508276346344607  loss: 0.3873 (0.3849)  time: 1.1617  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [1060/3854]  eta: 0:48:50  lr: 0.00748416412484175  loss: 0.3451 (0.3848)  time: 1.1054  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [1070/3854]  eta: 0:48:40  lr: 0.007460043268683449  loss: 0.3570 (0.3846)  time: 1.0796  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [1080/3854]  eta: 0:48:32  lr: 0.007435913743734345  loss: 0.3731 (0.3848)  time: 1.1002  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [1090/3854]  eta: 0:48:23  lr: 0.007411775515600527  loss: 0.4099 (0.3849)  time: 1.1142  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [1100/3854]  eta: 0:48:14  lr: 0.007387628549626633  loss: 0.3586 (0.3846)  time: 1.1195  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [1110/3854]  eta: 0:48:05  lr: 0.007363472810892893  loss: 0.3533 (0.3844)  time: 1.1201  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [1120/3854]  eta: 0:47:57  lr: 0.007339308264212148  loss: 0.3770 (0.3846)  time: 1.1196  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [1130/3854]  eta: 0:47:48  lr: 0.0073151348741268135  loss: 0.3706 (0.3844)  time: 1.1230  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [1140/3854]  eta: 0:47:39  lr: 0.007290952604905806  loss: 0.3601 (0.3841)  time: 1.1200  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [1150/3854]  eta: 0:47:29  lr: 0.007266761420541402  loss: 0.3506 (0.3841)  time: 1.1087  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [1160/3854]  eta: 0:47:20  lr: 0.007242561284746086  loss: 0.3695 (0.3842)  time: 1.1126  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [1170/3854]  eta: 0:47:11  lr: 0.007218352160949312  loss: 0.3850 (0.3843)  time: 1.1185  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [1180/3854]  eta: 0:47:02  lr: 0.007194134012294242  loss: 0.3779 (0.3843)  time: 1.1085  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1190/3854]  eta: 0:46:52  lr: 0.007169906801634418  loss: 0.3752 (0.3842)  time: 1.1092  data: 0.0087  max mem: 13869\n",
      "Epoch: [0]  [1200/3854]  eta: 0:46:42  lr: 0.0071456704915303995  loss: 0.3434 (0.3838)  time: 1.0799  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [1210/3854]  eta: 0:46:32  lr: 0.007121425044246325  loss: 0.3428 (0.3838)  time: 1.0826  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1220/3854]  eta: 0:46:23  lr: 0.007097170421746457  loss: 0.3833 (0.3839)  time: 1.1161  data: 0.0087  max mem: 13869\n",
      "Epoch: [0]  [1230/3854]  eta: 0:46:13  lr: 0.007072906585691621  loss: 0.3833 (0.3839)  time: 1.1048  data: 0.0089  max mem: 13869\n",
      "Epoch: [0]  [1240/3854]  eta: 0:46:04  lr: 0.007048633497435647  loss: 0.3639 (0.3837)  time: 1.1000  data: 0.0090  max mem: 13869\n",
      "Epoch: [0]  [1250/3854]  eta: 0:45:55  lr: 0.007024351118021705  loss: 0.3613 (0.3837)  time: 1.1186  data: 0.0090  max mem: 13869\n",
      "Epoch: [0]  [1260/3854]  eta: 0:45:46  lr: 0.0070000594081786206  loss: 0.3639 (0.3835)  time: 1.1375  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [1270/3854]  eta: 0:45:37  lr: 0.006975758328317109  loss: 0.3524 (0.3834)  time: 1.1415  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [1280/3854]  eta: 0:45:28  lr: 0.0069514478385259496  loss: 0.3502 (0.3833)  time: 1.1276  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1290/3854]  eta: 0:45:17  lr: 0.006927127898568119  loss: 0.3780 (0.3835)  time: 1.0992  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1300/3854]  eta: 0:45:07  lr: 0.006902798467876846  loss: 0.3780 (0.3835)  time: 1.0728  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [1310/3854]  eta: 0:44:57  lr: 0.0068784595055516  loss: 0.3590 (0.3834)  time: 1.0834  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1320/3854]  eta: 0:44:47  lr: 0.006854110970354026  loss: 0.3405 (0.3830)  time: 1.1049  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1330/3854]  eta: 0:44:38  lr: 0.006829752820703816  loss: 0.3424 (0.3830)  time: 1.1045  data: 0.0087  max mem: 13869\n",
      "Epoch: [0]  [1340/3854]  eta: 0:44:29  lr: 0.006805385014674492  loss: 0.3607 (0.3830)  time: 1.1421  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [1350/3854]  eta: 0:44:19  lr: 0.006781007509989149  loss: 0.3572 (0.3829)  time: 1.1445  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [1360/3854]  eta: 0:44:10  lr: 0.006756620264016108  loss: 0.3559 (0.3827)  time: 1.1218  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [1370/3854]  eta: 0:44:01  lr: 0.0067322232337645  loss: 0.3399 (0.3825)  time: 1.1360  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1380/3854]  eta: 0:43:51  lr: 0.006707816375879782  loss: 0.3399 (0.3824)  time: 1.1155  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1390/3854]  eta: 0:43:41  lr: 0.006683399646639192  loss: 0.3707 (0.3825)  time: 1.0948  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1400/3854]  eta: 0:43:30  lr: 0.0066589730019470834  loss: 0.3630 (0.3825)  time: 1.0612  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [1410/3854]  eta: 0:43:18  lr: 0.006634536397330242  loss: 0.3534 (0.3823)  time: 1.0281  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1420/3854]  eta: 0:43:08  lr: 0.006610089787933077  loss: 0.3682 (0.3823)  time: 1.0437  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1430/3854]  eta: 0:42:57  lr: 0.006585633128512749  loss: 0.3669 (0.3823)  time: 1.0700  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [1440/3854]  eta: 0:42:47  lr: 0.006561166373434221  loss: 0.3842 (0.3823)  time: 1.0881  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [1450/3854]  eta: 0:42:38  lr: 0.006536689476665215  loss: 0.3842 (0.3822)  time: 1.1116  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [1460/3854]  eta: 0:42:28  lr: 0.006512202391771085  loss: 0.3602 (0.3822)  time: 1.1220  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [1470/3854]  eta: 0:42:18  lr: 0.006487705071909603  loss: 0.3624 (0.3820)  time: 1.1153  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [1480/3854]  eta: 0:42:09  lr: 0.006463197469825664  loss: 0.3432 (0.3819)  time: 1.1325  data: 0.0089  max mem: 13869\n",
      "Epoch: [0]  [1490/3854]  eta: 0:41:59  lr: 0.006438679537845873  loss: 0.3455 (0.3817)  time: 1.1282  data: 0.0090  max mem: 13869\n",
      "Epoch: [0]  [1500/3854]  eta: 0:41:49  lr: 0.006414151227873079  loss: 0.3534 (0.3816)  time: 1.1119  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [1510/3854]  eta: 0:41:40  lr: 0.006389612491380772  loss: 0.3502 (0.3814)  time: 1.1369  data: 0.0089  max mem: 13869\n",
      "Epoch: [0]  [1520/3854]  eta: 0:41:29  lr: 0.006365063279407416  loss: 0.3502 (0.3816)  time: 1.1172  data: 0.0089  max mem: 13869\n",
      "Epoch: [0]  [1530/3854]  eta: 0:41:18  lr: 0.006340503542550655  loss: 0.3801 (0.3817)  time: 1.0767  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1540/3854]  eta: 0:41:08  lr: 0.0063159332309614315  loss: 0.3781 (0.3818)  time: 1.0843  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1550/3854]  eta: 0:40:58  lr: 0.006291352294338003  loss: 0.3636 (0.3817)  time: 1.0971  data: 0.0090  max mem: 13869\n",
      "Epoch: [0]  [1560/3854]  eta: 0:40:47  lr: 0.0062667606819198475  loss: 0.3436 (0.3814)  time: 1.0780  data: 0.0089  max mem: 13869\n",
      "Epoch: [0]  [1570/3854]  eta: 0:40:37  lr: 0.006242158342481456  loss: 0.3597 (0.3813)  time: 1.0870  data: 0.0092  max mem: 13869\n",
      "Epoch: [0]  [1580/3854]  eta: 0:40:27  lr: 0.006217545224326012  loss: 0.3643 (0.3811)  time: 1.0921  data: 0.0094  max mem: 13869\n",
      "Epoch: [0]  [1590/3854]  eta: 0:40:16  lr: 0.006192921275278974  loss: 0.3437 (0.3808)  time: 1.0708  data: 0.0090  max mem: 13869\n",
      "Epoch: [0]  [1600/3854]  eta: 0:40:05  lr: 0.0061682864426815245  loss: 0.3484 (0.3810)  time: 1.0637  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1610/3854]  eta: 0:39:54  lr: 0.0061436406733838995  loss: 0.3900 (0.3812)  time: 1.0520  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [1620/3854]  eta: 0:39:44  lr: 0.006118983913738609  loss: 0.3435 (0.3809)  time: 1.0723  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1630/3854]  eta: 0:39:34  lr: 0.006094316109593527  loss: 0.3661 (0.3811)  time: 1.0968  data: 0.0087  max mem: 13869\n",
      "Epoch: [0]  [1640/3854]  eta: 0:39:23  lr: 0.00606963720628484  loss: 0.3829 (0.3812)  time: 1.0839  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [1650/3854]  eta: 0:39:12  lr: 0.006044947148629897  loss: 0.3653 (0.3810)  time: 1.0572  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1660/3854]  eta: 0:39:02  lr: 0.00602024588091989  loss: 0.3450 (0.3808)  time: 1.0698  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1670/3854]  eta: 0:38:51  lr: 0.0059955333469124215  loss: 0.3760 (0.3809)  time: 1.0910  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [1680/3854]  eta: 0:38:40  lr: 0.005970809489823921  loss: 0.3934 (0.3809)  time: 1.0647  data: 0.0087  max mem: 13869\n",
      "Epoch: [0]  [1690/3854]  eta: 0:38:29  lr: 0.005946074252321932  loss: 0.3541 (0.3807)  time: 1.0381  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [1700/3854]  eta: 0:38:18  lr: 0.005921327576517238  loss: 0.3530 (0.3806)  time: 1.0471  data: 0.0089  max mem: 13869\n",
      "Epoch: [0]  [1710/3854]  eta: 0:38:08  lr: 0.005896569403955838  loss: 0.3427 (0.3803)  time: 1.0599  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [1720/3854]  eta: 0:37:57  lr: 0.005871799675610783  loss: 0.3267 (0.3801)  time: 1.0773  data: 0.0090  max mem: 13869\n",
      "Epoch: [0]  [1730/3854]  eta: 0:37:47  lr: 0.005847018331873846  loss: 0.3510 (0.3799)  time: 1.0744  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [1740/3854]  eta: 0:37:36  lr: 0.005822225312547026  loss: 0.3562 (0.3798)  time: 1.0643  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [1750/3854]  eta: 0:37:25  lr: 0.005797420556833906  loss: 0.3677 (0.3797)  time: 1.0439  data: 0.0087  max mem: 13869\n",
      "Epoch: [0]  [1760/3854]  eta: 0:37:13  lr: 0.005772604003330813  loss: 0.3616 (0.3796)  time: 1.0134  data: 0.0087  max mem: 13869\n",
      "Epoch: [0]  [1770/3854]  eta: 0:37:02  lr: 0.005747775590017827  loss: 0.3673 (0.3797)  time: 1.0104  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [1780/3854]  eta: 0:36:51  lr: 0.005722935254249621  loss: 0.3679 (0.3796)  time: 1.0391  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [1790/3854]  eta: 0:36:41  lr: 0.0056980829327460845  loss: 0.3712 (0.3798)  time: 1.0635  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1800/3854]  eta: 0:36:30  lr: 0.0056732185615828  loss: 0.3702 (0.3797)  time: 1.0720  data: 0.0087  max mem: 13869\n",
      "Epoch: [0]  [1810/3854]  eta: 0:36:20  lr: 0.005648342076181301  loss: 0.3702 (0.3797)  time: 1.0848  data: 0.0089  max mem: 13869\n",
      "Epoch: [0]  [1820/3854]  eta: 0:36:09  lr: 0.005623453411299145  loss: 0.3504 (0.3795)  time: 1.0849  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1830/3854]  eta: 0:35:58  lr: 0.005598552501019803  loss: 0.3538 (0.3796)  time: 1.0740  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1840/3854]  eta: 0:35:49  lr: 0.005573639278742302  loss: 0.3711 (0.3797)  time: 1.1092  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [1850/3854]  eta: 0:35:38  lr: 0.005548713677170695  loss: 0.3711 (0.3797)  time: 1.1085  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [1860/3854]  eta: 0:35:28  lr: 0.005523775628303303  loss: 0.4084 (0.3801)  time: 1.0811  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1870/3854]  eta: 0:35:17  lr: 0.005498825063421737  loss: 0.4299 (0.3801)  time: 1.0923  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [1880/3854]  eta: 0:35:06  lr: 0.005473861913079686  loss: 0.3861 (0.3801)  time: 1.0524  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [1890/3854]  eta: 0:34:55  lr: 0.005448886107091467  loss: 0.3488 (0.3799)  time: 1.0413  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [1900/3854]  eta: 0:34:45  lr: 0.005423897574520365  loss: 0.3328 (0.3798)  time: 1.0720  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [1910/3854]  eta: 0:34:34  lr: 0.0053988962436667  loss: 0.3323 (0.3796)  time: 1.0599  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [1920/3854]  eta: 0:34:23  lr: 0.00537388204205565  loss: 0.3390 (0.3795)  time: 1.0215  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [1930/3854]  eta: 0:34:12  lr: 0.00534885489642481  loss: 0.3727 (0.3794)  time: 1.0166  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [1940/3854]  eta: 0:34:00  lr: 0.005323814732711507  loss: 0.3629 (0.3794)  time: 1.0192  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [1950/3854]  eta: 0:33:49  lr: 0.005298761476039819  loss: 0.3543 (0.3795)  time: 1.0235  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [1960/3854]  eta: 0:33:38  lr: 0.005273695050707315  loss: 0.3756 (0.3794)  time: 1.0383  data: 0.0079  max mem: 13869\n",
      "Epoch: [0]  [1970/3854]  eta: 0:33:28  lr: 0.0052486153801715365  loss: 0.3583 (0.3794)  time: 1.0585  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [1980/3854]  eta: 0:33:18  lr: 0.005223522387036151  loss: 0.3488 (0.3794)  time: 1.0940  data: 0.0087  max mem: 13869\n",
      "Epoch: [0]  [1990/3854]  eta: 0:33:07  lr: 0.005198415993036813  loss: 0.3600 (0.3794)  time: 1.1116  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [2000/3854]  eta: 0:32:57  lr: 0.005173296119026726  loss: 0.3588 (0.3792)  time: 1.1081  data: 0.0087  max mem: 13869\n",
      "Epoch: [0]  [2010/3854]  eta: 0:32:47  lr: 0.005148162684961868  loss: 0.3588 (0.3792)  time: 1.0917  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [2020/3854]  eta: 0:32:36  lr: 0.005123015609885896  loss: 0.3760 (0.3793)  time: 1.0809  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [2030/3854]  eta: 0:32:26  lr: 0.005097854811914716  loss: 0.3760 (0.3793)  time: 1.0839  data: 0.0087  max mem: 13869\n",
      "Epoch: [0]  [2040/3854]  eta: 0:32:15  lr: 0.005072680208220699  loss: 0.3744 (0.3792)  time: 1.0614  data: 0.0089  max mem: 13869\n",
      "Epoch: [0]  [2050/3854]  eta: 0:32:04  lr: 0.0050474917150165445  loss: 0.3500 (0.3792)  time: 1.0510  data: 0.0090  max mem: 13869\n",
      "Epoch: [0]  [2060/3854]  eta: 0:31:53  lr: 0.0050222892475387726  loss: 0.3362 (0.3790)  time: 1.0583  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [2070/3854]  eta: 0:31:42  lr: 0.0049970727200308466  loss: 0.3365 (0.3790)  time: 1.0442  data: 0.0087  max mem: 13869\n",
      "Epoch: [0]  [2080/3854]  eta: 0:31:31  lr: 0.004971842045725896  loss: 0.3555 (0.3790)  time: 1.0294  data: 0.0089  max mem: 13869\n",
      "Epoch: [0]  [2090/3854]  eta: 0:31:21  lr: 0.004946597136829049  loss: 0.3224 (0.3788)  time: 1.0464  data: 0.0090  max mem: 13869\n",
      "Epoch: [0]  [2100/3854]  eta: 0:31:10  lr: 0.00492133790449935  loss: 0.3686 (0.3789)  time: 1.0618  data: 0.0089  max mem: 13869\n",
      "Epoch: [0]  [2110/3854]  eta: 0:30:59  lr: 0.00489606425883126  loss: 0.4185 (0.3791)  time: 1.0797  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [2120/3854]  eta: 0:30:49  lr: 0.004870776108835707  loss: 0.4296 (0.3792)  time: 1.1019  data: 0.0092  max mem: 13869\n",
      "Epoch: [0]  [2130/3854]  eta: 0:30:38  lr: 0.00484547336242071  loss: 0.3953 (0.3792)  time: 1.0722  data: 0.0093  max mem: 13869\n",
      "Epoch: [0]  [2140/3854]  eta: 0:30:27  lr: 0.0048201559263715155  loss: 0.3867 (0.3794)  time: 1.0471  data: 0.0093  max mem: 13869\n",
      "Epoch: [0]  [2150/3854]  eta: 0:30:16  lr: 0.004794823706330283  loss: 0.3867 (0.3793)  time: 1.0399  data: 0.0093  max mem: 13869\n",
      "Epoch: [0]  [2160/3854]  eta: 0:30:05  lr: 0.004769476606775265  loss: 0.3681 (0.3793)  time: 1.0168  data: 0.0093  max mem: 13869\n",
      "Epoch: [0]  [2170/3854]  eta: 0:29:54  lr: 0.004744114530999497  loss: 0.3584 (0.3792)  time: 1.0101  data: 0.0093  max mem: 13869\n",
      "Epoch: [0]  [2180/3854]  eta: 0:29:43  lr: 0.004718737381088943  loss: 0.3589 (0.3791)  time: 1.0143  data: 0.0092  max mem: 13869\n",
      "Epoch: [0]  [2190/3854]  eta: 0:29:32  lr: 0.004693345057900139  loss: 0.3569 (0.3789)  time: 1.0172  data: 0.0093  max mem: 13869\n",
      "Epoch: [0]  [2200/3854]  eta: 0:29:21  lr: 0.004667937461037262  loss: 0.3381 (0.3788)  time: 1.0314  data: 0.0094  max mem: 13869\n",
      "Epoch: [0]  [2210/3854]  eta: 0:29:10  lr: 0.004642514488828625  loss: 0.3428 (0.3787)  time: 1.0340  data: 0.0091  max mem: 13869\n",
      "Epoch: [0]  [2220/3854]  eta: 0:28:59  lr: 0.004617076038302613  loss: 0.3539 (0.3787)  time: 1.0112  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [2230/3854]  eta: 0:28:48  lr: 0.00459162200516299  loss: 0.3351 (0.3785)  time: 0.9880  data: 0.0087  max mem: 13869\n",
      "Epoch: [0]  [2240/3854]  eta: 0:28:37  lr: 0.004566152283763586  loss: 0.3425 (0.3785)  time: 1.0066  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [2250/3854]  eta: 0:28:26  lr: 0.0045406667670823495  loss: 0.3603 (0.3785)  time: 1.0410  data: 0.0091  max mem: 13869\n",
      "Epoch: [0]  [2260/3854]  eta: 0:28:16  lr: 0.004515165346694731  loss: 0.3125 (0.3781)  time: 1.0777  data: 0.0089  max mem: 13869\n",
      "Epoch: [0]  [2270/3854]  eta: 0:28:05  lr: 0.004489647912746378  loss: 0.3158 (0.3782)  time: 1.0656  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [2280/3854]  eta: 0:27:55  lr: 0.004464114353925127  loss: 0.3842 (0.3781)  time: 1.0689  data: 0.0087  max mem: 13869\n",
      "Epoch: [0]  [2290/3854]  eta: 0:27:44  lr: 0.004438564557432266  loss: 0.3321 (0.3779)  time: 1.0731  data: 0.0087  max mem: 13869\n",
      "Epoch: [0]  [2300/3854]  eta: 0:27:33  lr: 0.004412998408953032  loss: 0.3291 (0.3777)  time: 1.0476  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [2310/3854]  eta: 0:27:22  lr: 0.004387415792626346  loss: 0.3209 (0.3776)  time: 1.0425  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [2320/3854]  eta: 0:27:12  lr: 0.004361816591013737  loss: 0.3327 (0.3776)  time: 1.0354  data: 0.0087  max mem: 13869\n",
      "Epoch: [0]  [2330/3854]  eta: 0:27:02  lr: 0.00433620068506743  loss: 0.3300 (0.3775)  time: 1.1134  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [2340/3854]  eta: 0:26:51  lr: 0.004310567954097572  loss: 0.3387 (0.3776)  time: 1.1198  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [2350/3854]  eta: 0:26:40  lr: 0.004284918275738583  loss: 0.3657 (0.3777)  time: 1.0627  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [2360/3854]  eta: 0:26:31  lr: 0.004259251525914583  loss: 0.3657 (0.3777)  time: 1.1248  data: 0.0090  max mem: 13869\n",
      "Epoch: [0]  [2370/3854]  eta: 0:26:20  lr: 0.0042335675788038535  loss: 0.3575 (0.3776)  time: 1.1335  data: 0.0092  max mem: 13869\n",
      "Epoch: [0]  [2380/3854]  eta: 0:26:09  lr: 0.004207866306802342  loss: 0.3785 (0.3776)  time: 1.0499  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [2390/3854]  eta: 0:25:59  lr: 0.0041821475804861424  loss: 0.3726 (0.3776)  time: 1.0518  data: 0.0089  max mem: 13869\n",
      "Epoch: [0]  [2400/3854]  eta: 0:25:48  lr: 0.0041564112685729005  loss: 0.3719 (0.3775)  time: 1.1174  data: 0.0095  max mem: 13869\n",
      "Epoch: [0]  [2410/3854]  eta: 0:25:38  lr: 0.004130657237882169  loss: 0.3406 (0.3774)  time: 1.1314  data: 0.0096  max mem: 13869\n",
      "Epoch: [0]  [2420/3854]  eta: 0:25:27  lr: 0.004104885353294602  loss: 0.3271 (0.3772)  time: 1.1007  data: 0.0093  max mem: 13869\n",
      "Epoch: [0]  [2430/3854]  eta: 0:25:17  lr: 0.0040790954777100005  loss: 0.3587 (0.3772)  time: 1.0600  data: 0.0089  max mem: 13869\n",
      "Epoch: [0]  [2440/3854]  eta: 0:25:06  lr: 0.00405328747200414  loss: 0.3533 (0.3770)  time: 1.0413  data: 0.0089  max mem: 13869\n",
      "Epoch: [0]  [2450/3854]  eta: 0:24:55  lr: 0.004027461194984354  loss: 0.3533 (0.3772)  time: 1.0329  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [2460/3854]  eta: 0:24:44  lr: 0.004001616503343805  loss: 0.3768 (0.3772)  time: 1.0077  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [2470/3854]  eta: 0:24:33  lr: 0.003975753251614422  loss: 0.3751 (0.3772)  time: 0.9871  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [2480/3854]  eta: 0:24:22  lr: 0.003949871292118438  loss: 0.3751 (0.3772)  time: 0.9737  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [2490/3854]  eta: 0:24:11  lr: 0.003923970474918476  loss: 0.3303 (0.3771)  time: 0.9936  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [2500/3854]  eta: 0:24:00  lr: 0.0038980506477661347  loss: 0.3308 (0.3769)  time: 0.9996  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [2510/3854]  eta: 0:23:49  lr: 0.003872111656049021  loss: 0.3533 (0.3769)  time: 0.9882  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [2520/3854]  eta: 0:23:38  lr: 0.0038461533427361595  loss: 0.3573 (0.3769)  time: 0.9894  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [2530/3854]  eta: 0:23:27  lr: 0.0038201755483217212  loss: 0.3573 (0.3768)  time: 0.9941  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [2540/3854]  eta: 0:23:16  lr: 0.003794178110767009  loss: 0.3383 (0.3767)  time: 0.9961  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [2550/3854]  eta: 0:23:05  lr: 0.003768160865440637  loss: 0.3365 (0.3765)  time: 0.9926  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [2560/3854]  eta: 0:22:54  lr: 0.003742123645056815  loss: 0.3403 (0.3764)  time: 0.9927  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [2570/3854]  eta: 0:22:43  lr: 0.0037160662796116837  loss: 0.3403 (0.3765)  time: 0.9907  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [2580/3854]  eta: 0:22:32  lr: 0.003689988596317615  loss: 0.3329 (0.3765)  time: 0.9873  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [2590/3854]  eta: 0:22:21  lr: 0.003663890419535389  loss: 0.3586 (0.3765)  time: 0.9928  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [2600/3854]  eta: 0:22:10  lr: 0.0036377715707041707  loss: 0.3713 (0.3765)  time: 1.0487  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [2610/3854]  eta: 0:22:00  lr: 0.003611631868269198  loss: 0.3688 (0.3765)  time: 1.0587  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [2620/3854]  eta: 0:21:49  lr: 0.003585471127607086  loss: 0.3439 (0.3764)  time: 1.0068  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [2630/3854]  eta: 0:21:38  lr: 0.0035592891609486296  loss: 0.3390 (0.3762)  time: 0.9987  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [2640/3854]  eta: 0:21:27  lr: 0.0035330857772990385  loss: 0.3370 (0.3763)  time: 1.0019  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [2650/3854]  eta: 0:21:16  lr: 0.003506860782355462  loss: 0.3671 (0.3762)  time: 0.9924  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [2660/3854]  eta: 0:21:05  lr: 0.0034806139784217052  loss: 0.3746 (0.3762)  time: 0.9933  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [2670/3854]  eta: 0:20:54  lr: 0.0034543451643200104  loss: 0.3566 (0.3761)  time: 1.0280  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [2680/3854]  eta: 0:20:44  lr: 0.0034280541352997834  loss: 0.3559 (0.3761)  time: 1.0322  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [2690/3854]  eta: 0:20:33  lr: 0.0034017406829431225  loss: 0.3581 (0.3761)  time: 1.0005  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [2700/3854]  eta: 0:20:22  lr: 0.0033754045950670263  loss: 0.3691 (0.3761)  time: 0.9964  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [2710/3854]  eta: 0:20:11  lr: 0.0033490456556221075  loss: 0.3864 (0.3762)  time: 0.9995  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [2720/3854]  eta: 0:20:00  lr: 0.0033226636445876853  loss: 0.3634 (0.3763)  time: 0.9927  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [2730/3854]  eta: 0:19:49  lr: 0.0032962583378630663  loss: 0.3572 (0.3761)  time: 0.9758  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [2740/3854]  eta: 0:19:38  lr: 0.0032698295071548686  loss: 0.3193 (0.3760)  time: 0.9659  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [2750/3854]  eta: 0:19:27  lr: 0.0032433769198601846  loss: 0.3524 (0.3760)  time: 0.9686  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [2760/3854]  eta: 0:19:16  lr: 0.0032169003389453934  loss: 0.3792 (0.3759)  time: 0.9720  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [2770/3854]  eta: 0:19:05  lr: 0.0031903995228204376  loss: 0.3575 (0.3759)  time: 0.9745  data: 0.0080  max mem: 13869\n",
      "Epoch: [0]  [2780/3854]  eta: 0:18:55  lr: 0.00316387422520833  loss: 0.3331 (0.3757)  time: 0.9733  data: 0.0080  max mem: 13869\n",
      "Epoch: [0]  [2790/3854]  eta: 0:18:44  lr: 0.003137324195009674  loss: 0.3512 (0.3758)  time: 0.9759  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [2800/3854]  eta: 0:18:33  lr: 0.003110749176161953  loss: 0.3621 (0.3758)  time: 1.0242  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [2810/3854]  eta: 0:18:22  lr: 0.0030841489074933384  loss: 0.3368 (0.3757)  time: 1.0238  data: 0.0088  max mem: 13869\n",
      "Epoch: [0]  [2820/3854]  eta: 0:18:11  lr: 0.0030575231225707525  loss: 0.3446 (0.3756)  time: 0.9760  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [2830/3854]  eta: 0:18:01  lr: 0.0030308715495418947  loss: 0.3479 (0.3755)  time: 0.9697  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [2840/3854]  eta: 0:17:50  lr: 0.003004193910970927  loss: 0.3479 (0.3755)  time: 0.9670  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [2850/3854]  eta: 0:17:39  lr: 0.0029774899236675147  loss: 0.3196 (0.3752)  time: 0.9652  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [2860/3854]  eta: 0:17:28  lr: 0.002950759298508871  loss: 0.3103 (0.3752)  time: 0.9654  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [2870/3854]  eta: 0:17:17  lr: 0.002924001740254447  loss: 0.3448 (0.3752)  time: 0.9654  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [2880/3854]  eta: 0:17:06  lr: 0.002897216947352893  loss: 0.3684 (0.3752)  time: 0.9645  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [2890/3854]  eta: 0:16:55  lr: 0.002870404611740872  loss: 0.3684 (0.3752)  time: 0.9640  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [2900/3854]  eta: 0:16:45  lr: 0.002843564418633317  loss: 0.3581 (0.3752)  time: 0.9642  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [2910/3854]  eta: 0:16:34  lr: 0.0028166960463046446  loss: 0.3432 (0.3751)  time: 0.9655  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [2920/3854]  eta: 0:16:23  lr: 0.0027897991658604594  loss: 0.3207 (0.3749)  time: 0.9644  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [2930/3854]  eta: 0:16:12  lr: 0.0027628734409992127  loss: 0.3330 (0.3748)  time: 0.9640  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [2940/3854]  eta: 0:16:01  lr: 0.0027359185277632796  loss: 0.3715 (0.3748)  time: 0.9652  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [2950/3854]  eta: 0:15:51  lr: 0.002708934074278828  loss: 0.3453 (0.3747)  time: 0.9650  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [2960/3854]  eta: 0:15:40  lr: 0.0026819197204839013  loss: 0.3302 (0.3746)  time: 0.9645  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [2970/3854]  eta: 0:15:29  lr: 0.002654875097843974  loss: 0.3281 (0.3747)  time: 0.9640  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [2980/3854]  eta: 0:15:18  lr: 0.0026277998290543166  loss: 0.3534 (0.3746)  time: 0.9638  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [2990/3854]  eta: 0:15:07  lr: 0.0026006935277283604  loss: 0.3529 (0.3746)  time: 0.9638  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [3000/3854]  eta: 0:14:57  lr: 0.0025735557980712536  loss: 0.3454 (0.3745)  time: 0.9635  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [3010/3854]  eta: 0:14:46  lr: 0.002546386234537705  loss: 0.3591 (0.3745)  time: 0.9627  data: 0.0082  max mem: 13869\n",
      "Epoch: [0]  [3020/3854]  eta: 0:14:35  lr: 0.0025191844214731856  loss: 0.3271 (0.3743)  time: 0.9631  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [3030/3854]  eta: 0:14:24  lr: 0.0024919499327374544  loss: 0.3470 (0.3743)  time: 0.9635  data: 0.0085  max mem: 13869\n",
      "Epoch: [0]  [3040/3854]  eta: 0:14:14  lr: 0.0024646823313093156  loss: 0.3740 (0.3743)  time: 0.9636  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [3050/3854]  eta: 0:14:03  lr: 0.0024373811688714287  loss: 0.3628 (0.3743)  time: 0.9638  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [3060/3854]  eta: 0:13:52  lr: 0.0024100459853738957  loss: 0.3483 (0.3743)  time: 0.9629  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [3070/3854]  eta: 0:13:42  lr: 0.002382676308575284  loss: 0.3483 (0.3744)  time: 0.9630  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [3080/3854]  eta: 0:13:31  lr: 0.002355271653559589  loss: 0.3686 (0.3744)  time: 0.9628  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [3090/3854]  eta: 0:13:20  lr: 0.0023278315222275536  loss: 0.3623 (0.3744)  time: 0.9620  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [3100/3854]  eta: 0:13:09  lr: 0.002300355402760631  loss: 0.3491 (0.3742)  time: 0.9609  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [3110/3854]  eta: 0:12:59  lr: 0.002272842769055746  loss: 0.3398 (0.3742)  time: 0.9613  data: 0.0084  max mem: 13869\n",
      "Epoch: [0]  [3120/3854]  eta: 0:12:48  lr: 0.002245293080128834  loss: 0.3548 (0.3742)  time: 0.9603  data: 0.0086  max mem: 13869\n",
      "Epoch: [0]  [3130/3854]  eta: 0:12:37  lr: 0.002217705779484993  loss: 0.3630 (0.3743)  time: 0.9480  data: 0.0083  max mem: 13869\n",
      "Epoch: [0]  [3140/3854]  eta: 0:12:27  lr: 0.002190080294452879  loss: 0.3848 (0.3743)  time: 0.9364  data: 0.0080  max mem: 13869\n",
      "Epoch: [0]  [3150/3854]  eta: 0:12:16  lr: 0.0021624160354808074  loss: 0.3572 (0.3743)  time: 0.9338  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [3160/3854]  eta: 0:12:05  lr: 0.002134712395391755  loss: 0.3365 (0.3742)  time: 0.9325  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [3170/3854]  eta: 0:11:55  lr: 0.002106968748594257  loss: 0.3485 (0.3742)  time: 0.9317  data: 0.0078  max mem: 13869\n",
      "Epoch: [0]  [3180/3854]  eta: 0:11:44  lr: 0.00207918445024588  loss: 0.3277 (0.3741)  time: 0.9316  data: 0.0079  max mem: 13869\n",
      "Epoch: [0]  [3190/3854]  eta: 0:11:33  lr: 0.002051358835365692  loss: 0.3503 (0.3741)  time: 0.9300  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [3200/3854]  eta: 0:11:22  lr: 0.0020234912178918028  loss: 0.3654 (0.3740)  time: 0.9301  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [3210/3854]  eta: 0:11:12  lr: 0.0019955808896796605  loss: 0.3598 (0.3740)  time: 0.9302  data: 0.0078  max mem: 13869\n",
      "Epoch: [0]  [3220/3854]  eta: 0:11:01  lr: 0.0019676271194364273  loss: 0.3493 (0.3740)  time: 0.9292  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3230/3854]  eta: 0:10:50  lr: 0.0019396291515862529  loss: 0.3673 (0.3740)  time: 0.9283  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3240/3854]  eta: 0:10:40  lr: 0.001911586205060804  loss: 0.3625 (0.3739)  time: 0.9277  data: 0.0078  max mem: 13869\n",
      "Epoch: [0]  [3250/3854]  eta: 0:10:29  lr: 0.0018834974720088166  loss: 0.3362 (0.3739)  time: 0.9279  data: 0.0078  max mem: 13869\n",
      "Epoch: [0]  [3260/3854]  eta: 0:10:19  lr: 0.0018553621164178148  loss: 0.3394 (0.3738)  time: 0.9270  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [3270/3854]  eta: 0:10:08  lr: 0.0018271792726404239  loss: 0.3529 (0.3737)  time: 0.9268  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [3280/3854]  eta: 0:09:57  lr: 0.001798948043816947  loss: 0.3764 (0.3738)  time: 0.9265  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [3290/3854]  eta: 0:09:47  lr: 0.0017706675001849337  loss: 0.3695 (0.3738)  time: 0.9256  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [3300/3854]  eta: 0:09:36  lr: 0.00174233667726552  loss: 0.3547 (0.3737)  time: 0.9256  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [3310/3854]  eta: 0:09:25  lr: 0.0017139545739151235  loss: 0.3647 (0.3738)  time: 0.9255  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3320/3854]  eta: 0:09:15  lr: 0.0016855201502298759  loss: 0.3831 (0.3739)  time: 0.9250  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3330/3854]  eta: 0:09:04  lr: 0.0016570323252886477  loss: 0.3815 (0.3738)  time: 0.9258  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [3340/3854]  eta: 0:08:54  lr: 0.001628489974718933  loss: 0.3374 (0.3737)  time: 0.9261  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [3350/3854]  eta: 0:08:43  lr: 0.001599891928067931  loss: 0.3335 (0.3737)  time: 0.9258  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3360/3854]  eta: 0:08:33  lr: 0.0015712369659590783  loss: 0.3335 (0.3736)  time: 0.9261  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3370/3854]  eta: 0:08:22  lr: 0.0015425238170117827  loss: 0.3380 (0.3736)  time: 0.9267  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3380/3854]  eta: 0:08:12  lr: 0.0015137511544993248  loss: 0.3640 (0.3736)  time: 0.9266  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3390/3854]  eta: 0:08:01  lr: 0.0014849175927166684  loss: 0.3713 (0.3736)  time: 0.9265  data: 0.0079  max mem: 13869\n",
      "Epoch: [0]  [3400/3854]  eta: 0:07:50  lr: 0.001456021683026147  loss: 0.3348 (0.3735)  time: 0.9258  data: 0.0078  max mem: 13869\n",
      "Epoch: [0]  [3410/3854]  eta: 0:07:40  lr: 0.00142706190954474  loss: 0.3527 (0.3735)  time: 0.9250  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3420/3854]  eta: 0:07:29  lr: 0.0013980366844315872  loss: 0.3553 (0.3734)  time: 0.9246  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3430/3854]  eta: 0:07:19  lr: 0.00136894434272859  loss: 0.3287 (0.3734)  time: 0.9246  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3440/3854]  eta: 0:07:08  lr: 0.0013397831367001115  loss: 0.3287 (0.3733)  time: 0.9249  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [3450/3854]  eta: 0:06:58  lr: 0.0013105512296098323  loss: 0.3405 (0.3733)  time: 0.9248  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3460/3854]  eta: 0:06:47  lr: 0.001281246688863391  loss: 0.3550 (0.3733)  time: 0.9251  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [3470/3854]  eta: 0:06:37  lr: 0.001251867478434361  loss: 0.3908 (0.3733)  time: 0.9257  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [3480/3854]  eta: 0:06:27  lr: 0.0012224114504779332  loss: 0.3736 (0.3733)  time: 0.9271  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [3490/3854]  eta: 0:06:16  lr: 0.0011928763360210452  loss: 0.3699 (0.3732)  time: 0.9265  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [3500/3854]  eta: 0:06:06  lr: 0.0011632597345989104  loss: 0.3467 (0.3732)  time: 0.9258  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [3510/3854]  eta: 0:05:55  lr: 0.0011335591026854107  loss: 0.3652 (0.3733)  time: 0.9270  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [3520/3854]  eta: 0:05:45  lr: 0.00110377174073761  loss: 0.3651 (0.3732)  time: 0.9271  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [3530/3854]  eta: 0:05:34  lr: 0.00107389477864165  loss: 0.3400 (0.3732)  time: 0.9254  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [3540/3854]  eta: 0:05:24  lr: 0.0010439251593070025  loss: 0.3503 (0.3733)  time: 0.9247  data: 0.0073  max mem: 13869\n",
      "Epoch: [0]  [3550/3854]  eta: 0:05:13  lr: 0.00101385962010661  loss: 0.3621 (0.3733)  time: 0.9256  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [3560/3854]  eta: 0:05:03  lr: 0.0009836946717993954  loss: 0.3603 (0.3733)  time: 0.9257  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [3570/3854]  eta: 0:04:53  lr: 0.0009534265744956942  loss: 0.3674 (0.3733)  time: 0.9251  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [3580/3854]  eta: 0:04:42  lr: 0.0009230513101311964  loss: 0.3352 (0.3732)  time: 0.9239  data: 0.0078  max mem: 13869\n",
      "Epoch: [0]  [3590/3854]  eta: 0:04:32  lr: 0.0008925645507951355  loss: 0.3176 (0.3732)  time: 0.9228  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3600/3854]  eta: 0:04:21  lr: 0.0008619616221062575  loss: 0.3292 (0.3731)  time: 0.9233  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [3610/3854]  eta: 0:04:11  lr: 0.0008312374606349043  loss: 0.3186 (0.3730)  time: 0.9239  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3620/3854]  eta: 0:04:01  lr: 0.0008003865641172138  loss: 0.3510 (0.3730)  time: 0.9241  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [3630/3854]  eta: 0:03:50  lr: 0.0007694029328777658  loss: 0.3434 (0.3729)  time: 0.9249  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [3640/3854]  eta: 0:03:40  lr: 0.0007382800004420024  loss: 0.3456 (0.3729)  time: 0.9248  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [3650/3854]  eta: 0:03:30  lr: 0.0007070105507389426  loss: 0.3625 (0.3729)  time: 0.9238  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [3660/3854]  eta: 0:03:19  lr: 0.0006755866185098478  loss: 0.3491 (0.3728)  time: 0.9227  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [3670/3854]  eta: 0:03:09  lr: 0.0006439993684631422  loss: 0.3561 (0.3729)  time: 0.9227  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [3680/3854]  eta: 0:02:59  lr: 0.0006122389472209407  loss: 0.3748 (0.3728)  time: 0.9230  data: 0.0073  max mem: 13869\n",
      "Epoch: [0]  [3690/3854]  eta: 0:02:48  lr: 0.0005802942999899406  loss: 0.3608 (0.3727)  time: 0.9228  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [3700/3854]  eta: 0:02:38  lr: 0.0005481529408501487  loss: 0.3695 (0.3728)  time: 0.9225  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [3710/3854]  eta: 0:02:28  lr: 0.0005158006610940487  loss: 0.3706 (0.3728)  time: 0.9233  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [3720/3854]  eta: 0:02:17  lr: 0.00048322115335394953  loss: 0.3706 (0.3728)  time: 0.9234  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3730/3854]  eta: 0:02:07  lr: 0.0004503955189517024  loss: 0.3753 (0.3728)  time: 0.9228  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3740/3854]  eta: 0:01:57  lr: 0.00041730160958883635  loss: 0.3430 (0.3727)  time: 0.9237  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [3750/3854]  eta: 0:01:46  lr: 0.0003839131277998243  loss: 0.3258 (0.3726)  time: 0.9237  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [3760/3854]  eta: 0:01:36  lr: 0.0003501983652301097  loss: 0.3258 (0.3725)  time: 0.9229  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [3770/3854]  eta: 0:01:26  lr: 0.00031611837722726483  loss: 0.3199 (0.3724)  time: 0.9227  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [3780/3854]  eta: 0:01:15  lr: 0.00028162424135408923  loss: 0.3556 (0.3724)  time: 0.9225  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3790/3854]  eta: 0:01:05  lr: 0.0002466527462094655  loss: 0.3205 (0.3722)  time: 0.9232  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [3800/3854]  eta: 0:00:55  lr: 0.00021111920574718216  loss: 0.3248 (0.3723)  time: 0.9236  data: 0.0076  max mem: 13869\n",
      "Epoch: [0]  [3810/3854]  eta: 0:00:45  lr: 0.0001749045352156147  loss: 0.3750 (0.3723)  time: 0.9233  data: 0.0077  max mem: 13869\n",
      "Epoch: [0]  [3820/3854]  eta: 0:00:34  lr: 0.00013782944475472546  loss: 0.3676 (0.3723)  time: 0.9237  data: 0.0075  max mem: 13869\n",
      "Epoch: [0]  [3830/3854]  eta: 0:00:24  lr: 9.959430679216072e-05  loss: 0.3667 (0.3722)  time: 0.9233  data: 0.0074  max mem: 13869\n",
      "Epoch: [0]  [3840/3854]  eta: 0:00:14  lr: 5.959755976034371e-05  loss: 0.3764 (0.3723)  time: 0.9246  data: 0.0081  max mem: 13869\n",
      "Epoch: [0]  [3850/3854]  eta: 0:00:04  lr: 1.592533548344288e-05  loss: 0.3849 (0.3722)  time: 0.9254  data: 0.0083  max mem: 13869\n",
      "Epoch: [0] Total time: 1:05:47\n",
      "Test:  [   0/5000]  eta: 0:48:46    time: 0.5853  data: 0.4239  max mem: 13869\n",
      "Test:  [ 100/5000]  eta: 0:03:35    time: 0.0372  data: 0.0015  max mem: 13869\n",
      "Test:  [ 200/5000]  eta: 0:03:03    time: 0.0328  data: 0.0014  max mem: 13869\n",
      "Test:  [ 300/5000]  eta: 0:02:53    time: 0.0334  data: 0.0014  max mem: 13869\n",
      "Test:  [ 400/5000]  eta: 0:02:47    time: 0.0382  data: 0.0014  max mem: 13869\n",
      "Test:  [ 500/5000]  eta: 0:02:41    time: 0.0340  data: 0.0015  max mem: 13869\n",
      "Test:  [ 600/5000]  eta: 0:02:35    time: 0.0325  data: 0.0014  max mem: 13869\n",
      "Test:  [ 700/5000]  eta: 0:02:29    time: 0.0309  data: 0.0013  max mem: 13869\n",
      "Test:  [ 800/5000]  eta: 0:02:25    time: 0.0325  data: 0.0014  max mem: 13869\n",
      "Test:  [ 900/5000]  eta: 0:02:20    time: 0.0310  data: 0.0013  max mem: 13869\n",
      "Test:  [1000/5000]  eta: 0:02:16    time: 0.0336  data: 0.0014  max mem: 13869\n",
      "Test:  [1100/5000]  eta: 0:02:12    time: 0.0351  data: 0.0013  max mem: 13869\n",
      "Test:  [1200/5000]  eta: 0:02:08    time: 0.0329  data: 0.0013  max mem: 13869\n",
      "Test:  [1300/5000]  eta: 0:02:05    time: 0.0344  data: 0.0013  max mem: 13869\n",
      "Test:  [1400/5000]  eta: 0:02:02    time: 0.0328  data: 0.0013  max mem: 13869\n",
      "Test:  [1500/5000]  eta: 0:01:58    time: 0.0329  data: 0.0013  max mem: 13869\n",
      "Test:  [1600/5000]  eta: 0:01:55    time: 0.0347  data: 0.0013  max mem: 13869\n",
      "Test:  [1700/5000]  eta: 0:01:51    time: 0.0335  data: 0.0013  max mem: 13869\n",
      "Test:  [1800/5000]  eta: 0:01:48    time: 0.0382  data: 0.0013  max mem: 13869\n",
      "Test:  [1900/5000]  eta: 0:01:45    time: 0.0336  data: 0.0013  max mem: 13869\n",
      "Test:  [2000/5000]  eta: 0:01:42    time: 0.0343  data: 0.0013  max mem: 13869\n",
      "Test:  [2100/5000]  eta: 0:01:38    time: 0.0338  data: 0.0012  max mem: 13869\n",
      "Test:  [2200/5000]  eta: 0:01:35    time: 0.0344  data: 0.0012  max mem: 13869\n",
      "Test:  [2300/5000]  eta: 0:01:31    time: 0.0342  data: 0.0013  max mem: 13869\n",
      "Test:  [2400/5000]  eta: 0:01:28    time: 0.0349  data: 0.0012  max mem: 13869\n",
      "Test:  [2500/5000]  eta: 0:01:25    time: 0.0365  data: 0.0013  max mem: 13869\n",
      "Test:  [2600/5000]  eta: 0:01:21    time: 0.0368  data: 0.0013  max mem: 13869\n",
      "Test:  [2700/5000]  eta: 0:01:18    time: 0.0349  data: 0.0012  max mem: 13869\n",
      "Test:  [2800/5000]  eta: 0:01:15    time: 0.0341  data: 0.0012  max mem: 13869\n",
      "Test:  [2900/5000]  eta: 0:01:11    time: 0.0349  data: 0.0013  max mem: 13869\n",
      "Test:  [3000/5000]  eta: 0:01:08    time: 0.0343  data: 0.0012  max mem: 13869\n",
      "Test:  [3100/5000]  eta: 0:01:05    time: 0.0351  data: 0.0012  max mem: 13869\n",
      "Test:  [3200/5000]  eta: 0:01:01    time: 0.0351  data: 0.0012  max mem: 13869\n",
      "Test:  [3300/5000]  eta: 0:00:58    time: 0.0356  data: 0.0012  max mem: 13869\n",
      "Test:  [3400/5000]  eta: 0:00:54    time: 0.0346  data: 0.0012  max mem: 13869\n",
      "Test:  [3500/5000]  eta: 0:00:51    time: 0.0351  data: 0.0012  max mem: 13869\n",
      "Test:  [3600/5000]  eta: 0:00:48    time: 0.0355  data: 0.0012  max mem: 13869\n",
      "Test:  [3700/5000]  eta: 0:00:44    time: 0.0355  data: 0.0012  max mem: 13869\n",
      "Test:  [3800/5000]  eta: 0:00:41    time: 0.0353  data: 0.0012  max mem: 13869\n",
      "Test:  [3900/5000]  eta: 0:00:37    time: 0.0357  data: 0.0012  max mem: 13869\n",
      "Test:  [4000/5000]  eta: 0:00:34    time: 0.0362  data: 0.0013  max mem: 13869\n",
      "Test:  [4100/5000]  eta: 0:00:31    time: 0.0354  data: 0.0012  max mem: 13869\n",
      "Test:  [4200/5000]  eta: 0:00:27    time: 0.0377  data: 0.0013  max mem: 13869\n",
      "Test:  [4300/5000]  eta: 0:00:24    time: 0.0355  data: 0.0012  max mem: 13869\n",
      "Test:  [4400/5000]  eta: 0:00:20    time: 0.0368  data: 0.0012  max mem: 13869\n",
      "Test:  [4500/5000]  eta: 0:00:17    time: 0.0358  data: 0.0012  max mem: 13869\n",
      "Test:  [4600/5000]  eta: 0:00:13    time: 0.0368  data: 0.0013  max mem: 13869\n",
      "Test:  [4700/5000]  eta: 0:00:10    time: 0.0373  data: 0.0013  max mem: 13869\n",
      "Test:  [4800/5000]  eta: 0:00:06    time: 0.0365  data: 0.0012  max mem: 13869\n",
      "Test:  [4900/5000]  eta: 0:00:03    time: 0.0369  data: 0.0013  max mem: 13869\n",
      "Test: Total time: 0:02:54\n",
      "global correct: 89.9\n",
      "average row correct: ['92.6', '84.4', '71.2', '69.5', '60.1', '47.1', '77.1', '54.2', '90.3', '42.8', '80.0', '70.4', '78.3', '79.3', '82.2', '86.5', '47.9', '86.8', '63.3', '78.8', '60.8']\n",
      "IoU: ['88.7', '64.2', '55.8', '46.2', '39.2', '33.6', '69.1', '45.4', '73.3', '31.2', '58.8', '32.8', '61.1', '62.6', '68.0', '75.0', '26.2', '56.3', '45.3', '69.3', '47.8']\n",
      "mean IoU: 54.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvasserm/dev/ml_acceleration/assignment4/task_quantization/deeplab_quantization_ready/utils.py:295: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 1:08:42\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Print current GPU memory usage\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "\n",
    "print(f'Total: {t}, Reserved: {r}, Allocated: {a}, Free: {f}')\n",
    "\n",
    "# Вытащил дефолтные аргументы, чтобы не упражняться с argparse в ноутбуке\n",
    "with Path('./torch_default_args.pickle').open('rb') as file:\n",
    "    args = pickle.load(file)\n",
    "\n",
    "# Подобирайте под ваше железо\n",
    "args.data_path = '/home/gvasserm/data/coco2017/'\n",
    "args.epochs = 1\n",
    "args.batch_size = 24\n",
    "args.workers = 8\n",
    "\n",
    "print(args)\n",
    "\n",
    "model = deeplabv3_mobilenet_v3_large(weights=DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "if args.output_dir:\n",
    "    utils.mkdir(args.output_dir)\n",
    "\n",
    "utils.init_distributed_mode(args)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "dataset_test, num_classes = get_dataset(args, is_train=False)\n",
    "\n",
    "dataset_train, num_classes = get_dataset(args, is_train=True)\n",
    "\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "train_sampler = torch.utils.data.SequentialSampler(dataset_train)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=16, sampler=test_sampler, num_workers=args.workers, collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=24, sampler=train_sampler, num_workers=args.workers, collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "qat_model = fake_quantization(model, data_loader_train)\n",
    "qat_model.cuda()\n",
    "\n",
    "train(qat_model, None, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инференс делаем на cpu, предварительно конвертируя модельку на CPU\n",
    "qat_model.cpu()\n",
    "int_qat_model = convert_fx(qat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [  0/313]  eta: 0:36:51    time: 7.0646  data: 0.8907  max mem: 13869\n",
      "Test:  [100/313]  eta: 0:18:17    time: 4.8674  data: 0.0012  max mem: 13869\n",
      "Test:  [200/313]  eta: 0:09:41    time: 5.0602  data: 0.0013  max mem: 13869\n",
      "Test:  [300/313]  eta: 0:01:07    time: 5.1654  data: 0.0012  max mem: 13869\n",
      "Test: Total time: 0:26:56\n",
      "global correct: 90.3\n",
      "average row correct: ['94.8', '79.3', '62.8', '65.0', '49.5', '47.3', '70.5', '49.8', '91.2', '34.0', '75.8', '43.5', '75.0', '72.2', '74.7', '85.6', '43.2', '79.4', '46.6', '64.6', '59.0']\n",
      "IoU: ['89.5', '53.6', '53.1', '40.7', '31.7', '28.0', '62.6', '41.5', '65.6', '26.3', '57.3', '28.7', '58.0', '61.4', '62.8', '73.8', '21.6', '56.1', '38.7', '57.8', '43.4']\n",
      "mean IoU: 50.1\n"
     ]
    }
   ],
   "source": [
    "# Точность модели fake quant и квантованной после конвертации будут разные\n",
    "# Так и должно быть, всё таки мы эмулировали квантование.\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=16, sampler=test_sampler, num_workers=args.workers, collate_fn=utils.collate_fn\n",
    ")\n",
    "int_qat_model.cpu()\n",
    "confmat = evaluate(int_qat_model, data_loader_test, device='cpu', num_classes=num_classes)\n",
    "print(confmat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
