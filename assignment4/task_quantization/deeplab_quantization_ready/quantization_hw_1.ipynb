{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14864c63-1ec1-456d-a026-786f650f4f16",
   "metadata": {},
   "source": [
    "### Квантует DeepLabV3 MobilenetV3\n",
    "\n",
    "Стартуем с трейнлупа, который нам выдали pytorch\n",
    "\n",
    "Датасет COCO, https://cocodataset.org/#download \n",
    "Качаем train2017 и val2017\n",
    "\n",
    "Можно использовать [сабсет](https://drive.google.com/file/d/1qdtAbK-iOsgJZxjbBva0pw2Vi5penjPc/view?usp=sharing) трейна на 20000, но тогда заранее залезте в класс датасета, и добавте работу с пропущенными картинками\n",
    "\n",
    "Баллы: 20 баллов Static Quantization + 20 баллов Quantization Aware Training + 10 баллов Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29bf7a00-f092-413e-9789-7fe81046d248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvasserm/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "from torch.ao.quantization.quantize_fx import convert_fx\n",
    "from torch.ao.quantization.quantize_fx import fuse_fx\n",
    "from torch.optim.lr_scheduler import PolynomialLR\n",
    "from torchvision.models.segmentation import DeepLabV3_MobileNet_V3_Large_Weights, deeplabv3_mobilenet_v3_large\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "from quantization_utils.fake_quantization import fake_quantization\n",
    "from quantization_utils.static_quantization import quantize_static\n",
    "from train import evaluate\n",
    "from train import get_dataset\n",
    "from train import train_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf831e0a-003e-40fb-9b3f-ad1a225751c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вытащил дефолтные аргументы, чтобы не упражняться с argparse в ноутбуке\n",
    "with Path('./torch_default_args.pickle').open('rb') as file:\n",
    "    args = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2acb1e03-f5fc-4ec4-a089-54f464c17b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подобирайте под ваше железо\n",
    "args.data_path = '/home/gvasserm/data/coco2017/'\n",
    "args.epochs = 1\n",
    "args.batch_size = 32\n",
    "args.workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d5b93ea-21fc-4e72-8bab-e16d8722fb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data_path='/home/gvasserm/data/coco2017/', dataset='coco', model='deeplabv3_mobilenet_v3_large', aux_loss=False, device='cuda', batch_size=32, epochs=1, workers=8, lr=0.01, momentum=0.9, weight_decay=0.0001, lr_warmup_epochs=0, lr_warmup_method='linear', lr_warmup_decay=0.01, print_freq=10, output_dir='.', resume='', start_epoch=0, test_only=False, use_deterministic_algorithms=False, world_size=1, dist_url='env://', weights=None, weights_backbone=None, amp=False, backend='pil', use_v2=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d07a33-dd10-41d5-84f2-6c3c6cc22971",
   "metadata": {},
   "source": [
    "### Сначала просто валидация обычной сетки, прям на гпу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73ddaa10-5909-451b-ae90-6ee0f013da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = deeplabv3_mobilenet_v3_large(weights=DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b15585f-78f7-4437-8f14-aea099b82f3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "loading annotations into memory...\n",
      "Done (t=0.27s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "if args.output_dir:\n",
    "    utils.mkdir(args.output_dir)\n",
    "\n",
    "utils.init_distributed_mode(args)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "dataset_test, num_classes = get_dataset(args, is_train=False)\n",
    "\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=16, sampler=test_sampler, num_workers=args.workers, collate_fn=utils.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec3c78a1-6a74-48ec-938c-d511918645f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [  0/209]  eta: 0:08:53    time: 2.5547  data: 1.3405  max mem: 3971\n",
      "Test:  [100/209]  eta: 0:01:02    time: 0.4956  data: 0.0267  max mem: 11393\n",
      "Test:  [200/209]  eta: 0:00:05    time: 0.5780  data: 0.0276  max mem: 11393\n",
      "Test: Total time: 0:01:57\n",
      "global correct: 91.2\n",
      "average row correct: ['94.8', '84.0', '69.2', '70.9', '58.8', '45.3', '74.2', '59.5', '92.3', '31.4', '78.1', '54.0', '80.2', '78.6', '78.9', '87.7', '47.4', '87.2', '51.7', '84.3', '61.4']\n",
      "IoU: ['90.3', '67.7', '56.0', '53.1', '42.7', '34.8', '67.1', '48.2', '74.7', '26.7', '63.3', '32.7', '61.8', '65.8', '66.8', '76.9', '24.9', '68.4', '42.4', '68.9', '50.8']\n",
      "mean IoU: 56.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvasserm/dev/ml_acceleration/assignment4/task_quantization/deeplab_quantization_ready/utils.py:295: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(val)\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "model.cuda()\n",
    "confmat = evaluate(model, data_loader_test, device=device, num_classes=num_classes)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465c3c9-0eb8-46d4-ae0d-79336cd1b7c9",
   "metadata": {},
   "source": [
    "### Заквантуем статические сетку, посмотрим на точность и скорость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ebfca70-948a-4e42-b7c9-2a9984108f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=7.48s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvasserm/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Квантуем\n",
    "# Делаем fuse, делаем quantize_static и quantize_utils (посмотрите что там с кодом)\n",
    "# Можно покрутить параметр num_batches, чтобы посмотреть сколько нужно данных на калибровку\n",
    "model_fused = fuse_fx(model)\n",
    "dataset_train, num_classes = get_dataset(args, is_train=True)\n",
    "train_sampler = torch.utils.data.SequentialSampler(dataset_train)\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=24, sampler=train_sampler, num_workers=args.workers, collate_fn=utils.collate_fn\n",
    "    )\n",
    "q_model = quantize_static(model_fused, data_loader_train, num_batches=24, device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81e8eda0-19fc-4a50-8a7a-9aa9f7f79391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Замерим скорость квантованной модели на CPU\n",
    "# Не забываем, от размера батча будет зависить буст!\n",
    "def profile(model, bs=24, device='cpu'):\n",
    "    # Ensure the model is in evaluation mode to disable layers like dropout and batchnorm during inference\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    input_tensor = torch.rand(bs, 3, 520, 520, dtype=torch.float)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    # Warm-up (optional, but recommended for more accurate timing, especially on GPU)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(input_tensor)\n",
    "\n",
    "    # Timing starts here\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Forward pass\n",
    "    n = 10\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            output = model(input_tensor)\n",
    "\n",
    "    # Timing ends here\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate and print the elapsed time in milliseconds\n",
    "    elapsed_time_ms = (end_time - start_time) * 1000/n/bs\n",
    "    print(f\"Elapsed time for the forward pass: {elapsed_time_ms:.2f}ms, batch size: {bs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae59f5e2-9fa3-4d39-ba25-13817a65a755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for the forward pass: 127.67ms, batch size: 24\n"
     ]
    }
   ],
   "source": [
    "# Замеряем с бачтом 1, буста нет\n",
    "# Замеряем с батчом 32, буст есть\n",
    "# Мораль, latency != throughput. В сетке всегда есть накладные расходы, кроме перемалывания матричек\n",
    "profile(q_model, bs=24, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bafbf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for the forward pass: 147.68ms, batch size: 1\n"
     ]
    }
   ],
   "source": [
    "profile(q_model, bs=1, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "087a60bf-6fb1-4b14-b44e-d06efb9c9f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for the forward pass: 98.06ms, batch size: 24\n"
     ]
    }
   ],
   "source": [
    "# Замерим скорость оригинальной модели на CPU\n",
    "# У меня на intel i9 при батчсайзе 32 получился x2 буст\n",
    "profile(model, bs=24, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fad1ef",
   "metadata": {},
   "source": [
    "### Here I've received faster runtime for fp32 then for quantized model. Currently no idea how to improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3e66e00-55a3-4ddf-b376-07e402ff9bbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [  0/313]  eta: 0:42:53    time: 8.2228  data: 1.6694  max mem: 1000\n",
      "Test:  [100/313]  eta: 0:19:24    time: 5.0432  data: 0.0012  max mem: 1000\n",
      "Test:  [200/313]  eta: 0:10:16    time: 5.4540  data: 0.0011  max mem: 1000\n",
      "Test:  [300/313]  eta: 0:01:11    time: 5.3760  data: 0.0013  max mem: 1000\n",
      "Test: Total time: 0:28:29\n",
      "global correct: 89.1\n",
      "average row correct: ['94.5', '64.8', '60.4', '60.6', '32.8', '36.4', '62.4', '43.6', '79.0', '25.1', '44.8', '56.6', '52.6', '45.9', '62.4', '79.6', '32.2', '66.9', '26.5', '70.5', '53.2']\n",
      "IoU: ['88.2', '58.0', '50.5', '33.7', '29.4', '27.5', '58.8', '37.7', '59.4', '21.8', '37.9', '31.4', '41.4', '42.8', '56.6', '69.4', '21.8', '54.7', '23.4', '61.7', '28.6']\n",
      "mean IoU: 44.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvasserm/dev/ml_acceleration/assignment4/task_quantization/deeplab_quantization_ready/utils.py:295: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(val)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nglobal correct: 89.8\\naverage row correct: ['94.9', '77.5', '66.5', '67.9', '44.0', '25.9', '61.0', '45.2', '83.1', '20.9', '68.2', '57.0', '63.6', '62.1', '69.6', '80.9', '37.2', '75.0', '32.1', '67.7', '49.2']\\nIoU: ['88.9', '64.5', '52.6', '32.2', '34.5', '22.7', '57.8', '38.7', '62.0', '19.0', '55.6', '31.9', '51.9', '55.3', '61.5', '71.3', '22.3', '60.6', '29.0', '61.0', '44.0']\\nmean IoU: 48.4\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посчитаем метрики квантованной модели\n",
    "# У меня была просадка где-то до 56 IoU\n",
    "q_model.cpu()\n",
    "confmat = evaluate(q_model, data_loader_test, device='cpu', num_classes=num_classes)\n",
    "print(confmat)\n",
    "\n",
    "'''\n",
    "global correct: 89.8\n",
    "average row correct: ['94.9', '77.5', '66.5', '67.9', '44.0', '25.9', '61.0', '45.2', '83.1', '20.9', '68.2', '57.0', '63.6', '62.1', '69.6', '80.9', '37.2', '75.0', '32.1', '67.7', '49.2']\n",
    "IoU: ['88.9', '64.5', '52.6', '32.2', '34.5', '22.7', '57.8', '38.7', '62.0', '19.0', '55.6', '31.9', '51.9', '55.3', '61.5', '71.3', '22.3', '60.6', '29.0', '61.0', '44.0']\n",
    "mean IoU: 48.4\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f1a2f-76c2-48d5-ac99-040460da12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "average row correct: ['94.0', '83.5', '71.0', '63.7', '56.1', '46.3', '77.0', '57.3', '89.2', '38.3', '76.4', '68.4', '77.8', '77.9', '78.5', '86.5', '46.9', '82.4', '66.6', '84.0', '64.9']\n",
    "IoU: ['90.0', '67.6', '58.2', '53.5', '43.8', '34.8', '67.9', '45.9', '74.0', '28.9', '63.0', '37.2', '62.2', '63.9', '69.0', '75.5', '31.0', '63.4', '48.1', '67.5', '53.7']\n",
    "mean IoU: 57.1\n",
    "'''\n",
    "'''\n",
    "global correct: 90.9\n",
    "average row correct: ['94.8', '78.2', '65.8', '60.7', '49.3', '44.9', '72.8', '56.1', '89.9', '31.3', '73.4', '59.6', '74.3', '74.0', '76.1', '84.9', '40.3', '77.6', '58.4', '79.2', '65.9']\n",
    "IoU: ['90.0', '63.1', '54.7', '52.3', '40.8', '32.4', '65.4', '45.8', '67.7', '25.5', '61.7', '35.4', '55.4', '62.7', '65.7', '75.0', '24.7', '67.0', '44.1', '66.2', '50.4']\n",
    "mean IoU: 54.6\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
