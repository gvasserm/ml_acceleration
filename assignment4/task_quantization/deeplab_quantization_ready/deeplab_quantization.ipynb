{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14864c63-1ec1-456d-a026-786f650f4f16",
   "metadata": {},
   "source": [
    "### Квантует DeepLabV3 MobilenetV3\n",
    "\n",
    "Стартуем с трейнлупа, который нам выдали pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29bf7a00-f092-413e-9789-7fe81046d248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d.chudakov/miniconda3/envs/road/lib/python3.8/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (None)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\n",
      "/home/d.chudakov/miniconda3/envs/road/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "from torch.ao.quantization.quantize_fx import convert_fx\n",
    "from torch.ao.quantization.quantize_fx import fuse_fx\n",
    "from torch.optim.lr_scheduler import PolynomialLR\n",
    "from torchvision.models.segmentation import DeepLabV3_MobileNet_V3_Large_Weights, deeplabv3_mobilenet_v3_large, deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "from quantization_utils.fake_quantization import fake_quantization\n",
    "from quantization_utils.static_quantization import quantize_static\n",
    "from train import evaluate\n",
    "from train import get_dataset\n",
    "from train import train_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf831e0a-003e-40fb-9b3f-ad1a225751c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вытащил дефолтные аргументы, чтобы не упражняться с argparse в ноутбуке\n",
    "with Path('./torch_default_args.pickle').open('rb') as file:\n",
    "    args = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2acb1e03-f5fc-4ec4-a089-54f464c17b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подобирайте под ваше железо\n",
    "args.data_path = '/home/d.chudakov/datasets/coco/'\n",
    "args.epochs = 1\n",
    "args.batch_size = 32\n",
    "args.workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d5b93ea-21fc-4e72-8bab-e16d8722fb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(amp=False, aux_loss=False, backend='pil', batch_size=32, data_path='/home/d.chudakov/datasets/coco/', dataset='coco', device='cuda', dist_url='env://', epochs=1, lr=0.01, lr_warmup_decay=0.01, lr_warmup_epochs=0, lr_warmup_method='linear', model='deeplabv3_mobilenet_v3_large', momentum=0.9, output_dir='.', print_freq=10, resume='', start_epoch=0, test_only=False, use_deterministic_algorithms=False, use_v2=False, weight_decay=0.0001, weights=None, weights_backbone=None, workers=8, world_size=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d07a33-dd10-41d5-84f2-6c3c6cc22971",
   "metadata": {},
   "source": [
    "### Сначала просто валидация обычной сетки, прям на гпу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73ddaa10-5909-451b-ae90-6ee0f013da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = deeplabv3_mobilenet_v3_large(weights=DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b15585f-78f7-4437-8f14-aea099b82f3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "loading annotations into memory...\n",
      "Done (t=0.28s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "if args.output_dir:\n",
    "    utils.mkdir(args.output_dir)\n",
    "\n",
    "utils.init_distributed_mode(args)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "dataset_test, num_classes = get_dataset(args, is_train=False)\n",
    "\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=8, sampler=test_sampler, num_workers=args.workers, collate_fn=utils.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec3c78a1-6a74-48ec-938c-d511918645f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d.chudakov/miniconda3/envs/road/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1708025829503/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [  0/625]  eta: 0:18:48    time: 1.8059  data: 0.7778  max mem: 1021\n",
      "Test:  [100/625]  eta: 0:01:05    time: 0.1060  data: 0.0050  max mem: 2480\n",
      "Test:  [200/625]  eta: 0:00:49    time: 0.1123  data: 0.0059  max mem: 2480\n",
      "Test:  [300/625]  eta: 0:00:36    time: 0.1054  data: 0.0057  max mem: 2665\n",
      "Test:  [400/625]  eta: 0:00:25    time: 0.1093  data: 0.0059  max mem: 2665\n",
      "Test:  [500/625]  eta: 0:00:13    time: 0.1108  data: 0.0062  max mem: 2749\n",
      "Test:  [600/625]  eta: 0:00:02    time: 0.1061  data: 0.0063  max mem: 3378\n",
      "Test: Total time: 0:01:08\n",
      "global correct: 91.4\n",
      "average row correct: ['94.6', '84.3', '71.1', '72.8', '60.2', '49.3', '74.4', '61.5', '92.1', '35.9', '79.4', '58.7', '81.4', '80.3', '81.5', '88.0', '54.2', '87.6', '56.9', '84.7', '62.6']\n",
      "IoU: ['90.4', '68.8', '56.4', '58.4', '45.8', '36.6', '67.6', '49.9', '76.7', '29.7', '64.2', '34.4', '62.7', '66.9', '68.8', '77.4', '29.4', '68.7', '46.3', '68.8', '52.3']\n",
      "mean IoU: 58.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d.chudakov/wrk/deepschool/deeplab_quantization/utils.py:295: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(val)\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "model.cuda()\n",
    "confmat = evaluate(model, data_loader_test, device=device, num_classes=num_classes)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465c3c9-0eb8-46d4-ae0d-79336cd1b7c9",
   "metadata": {},
   "source": [
    "### Заквантуем статические сетку, посмотрим на точность и скорость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ebfca70-948a-4e42-b7c9-2a9984108f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d.chudakov/miniconda3/envs/road/lib/python3.8/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Квантуем\n",
    "q_model = quantize_static(deepcopy(model), data_loader_test, num_batches=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81e8eda0-19fc-4a50-8a7a-9aa9f7f79391",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# Замерим скорость квантованной модели на CPU\n",
    "sample = next(iter(data_loader_test))\n",
    "q_model.cpu()\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(2)):\n",
    "        q_model(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "087a60bf-6fb1-4b14-b44e-d06efb9c9f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "# Замерим скорость оригинальной модели на CPU\n",
    "sample = next(iter(data_loader_test))\n",
    "model.cpu()\n",
    "model = fuse_fx(model)\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(2)):\n",
    "        model(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3e66e00-55a3-4ddf-b376-07e402ff9bbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [   0/5000]  eta: 0:45:39    time: 0.5479  data: 0.3260  max mem: 222\n",
      "Test:  [ 100/5000]  eta: 0:13:15    time: 0.1529  data: 0.0011  max mem: 222\n",
      "Test:  [ 200/5000]  eta: 0:12:52    time: 0.1610  data: 0.0012  max mem: 222\n",
      "Test:  [ 300/5000]  eta: 0:12:34    time: 0.1648  data: 0.0010  max mem: 222\n",
      "Test:  [ 400/5000]  eta: 0:12:21    time: 0.1670  data: 0.0009  max mem: 222\n",
      "Test:  [ 500/5000]  eta: 0:12:07    time: 0.1594  data: 0.0008  max mem: 222\n",
      "Test:  [ 600/5000]  eta: 0:11:51    time: 0.1653  data: 0.0010  max mem: 222\n",
      "Test:  [ 700/5000]  eta: 0:11:36    time: 0.1636  data: 0.0010  max mem: 222\n",
      "Test:  [ 800/5000]  eta: 0:11:22    time: 0.1674  data: 0.0009  max mem: 222\n",
      "Test:  [ 900/5000]  eta: 0:11:07    time: 0.1657  data: 0.0013  max mem: 222\n",
      "Test:  [1000/5000]  eta: 0:10:51    time: 0.1724  data: 0.0009  max mem: 222\n",
      "Test:  [1100/5000]  eta: 0:10:36    time: 0.1676  data: 0.0010  max mem: 222\n",
      "Test:  [1200/5000]  eta: 0:10:20    time: 0.1670  data: 0.0011  max mem: 222\n",
      "Test:  [1300/5000]  eta: 0:10:03    time: 0.1629  data: 0.0010  max mem: 222\n",
      "Test:  [1400/5000]  eta: 0:09:45    time: 0.1608  data: 0.0009  max mem: 222\n",
      "Test:  [1500/5000]  eta: 0:09:28    time: 0.1657  data: 0.0010  max mem: 222\n",
      "Test:  [1600/5000]  eta: 0:09:11    time: 0.1669  data: 0.0009  max mem: 222\n",
      "Test:  [1700/5000]  eta: 0:08:55    time: 0.1647  data: 0.0010  max mem: 222\n",
      "Test:  [1800/5000]  eta: 0:08:38    time: 0.1701  data: 0.0008  max mem: 222\n",
      "Test:  [1900/5000]  eta: 0:08:22    time: 0.1595  data: 0.0009  max mem: 222\n",
      "Test:  [2000/5000]  eta: 0:08:05    time: 0.1649  data: 0.0009  max mem: 222\n",
      "Test:  [2100/5000]  eta: 0:07:48    time: 0.1506  data: 0.0009  max mem: 222\n",
      "Test:  [2200/5000]  eta: 0:07:31    time: 0.1610  data: 0.0008  max mem: 222\n",
      "Test:  [2300/5000]  eta: 0:07:15    time: 0.1560  data: 0.0010  max mem: 222\n",
      "Test:  [2400/5000]  eta: 0:06:59    time: 0.1564  data: 0.0009  max mem: 222\n",
      "Test:  [2500/5000]  eta: 0:06:42    time: 0.1591  data: 0.0010  max mem: 222\n",
      "Test:  [2600/5000]  eta: 0:06:26    time: 0.1717  data: 0.0009  max mem: 222\n",
      "Test:  [2700/5000]  eta: 0:06:10    time: 0.1674  data: 0.0009  max mem: 222\n",
      "Test:  [2800/5000]  eta: 0:05:54    time: 0.1588  data: 0.0010  max mem: 222\n",
      "Test:  [2900/5000]  eta: 0:05:38    time: 0.1598  data: 0.0009  max mem: 222\n",
      "Test:  [3000/5000]  eta: 0:05:22    time: 0.1592  data: 0.0010  max mem: 222\n",
      "Test:  [3100/5000]  eta: 0:05:06    time: 0.1597  data: 0.0010  max mem: 222\n",
      "Test:  [3200/5000]  eta: 0:04:49    time: 0.1577  data: 0.0008  max mem: 222\n",
      "Test:  [3300/5000]  eta: 0:04:33    time: 0.1639  data: 0.0009  max mem: 222\n",
      "Test:  [3400/5000]  eta: 0:04:17    time: 0.1522  data: 0.0008  max mem: 222\n",
      "Test:  [3500/5000]  eta: 0:04:01    time: 0.1531  data: 0.0010  max mem: 222\n",
      "Test:  [3600/5000]  eta: 0:03:44    time: 0.1522  data: 0.0009  max mem: 222\n",
      "Test:  [3700/5000]  eta: 0:03:28    time: 0.1618  data: 0.0009  max mem: 222\n",
      "Test:  [3800/5000]  eta: 0:03:12    time: 0.1605  data: 0.0008  max mem: 222\n",
      "Test:  [3900/5000]  eta: 0:02:56    time: 0.1564  data: 0.0010  max mem: 222\n",
      "Test:  [4000/5000]  eta: 0:02:40    time: 0.1609  data: 0.0010  max mem: 222\n",
      "Test:  [4100/5000]  eta: 0:02:24    time: 0.1660  data: 0.0010  max mem: 222\n",
      "Test:  [4200/5000]  eta: 0:02:08    time: 0.1667  data: 0.0009  max mem: 222\n",
      "Test:  [4300/5000]  eta: 0:01:52    time: 0.1610  data: 0.0009  max mem: 222\n",
      "Test:  [4400/5000]  eta: 0:01:36    time: 0.1633  data: 0.0010  max mem: 222\n",
      "Test:  [4500/5000]  eta: 0:01:20    time: 0.1594  data: 0.0010  max mem: 222\n",
      "Test:  [4600/5000]  eta: 0:01:04    time: 0.1606  data: 0.0009  max mem: 222\n",
      "Test:  [4700/5000]  eta: 0:00:48    time: 0.1640  data: 0.0009  max mem: 222\n",
      "Test:  [4800/5000]  eta: 0:00:32    time: 0.1580  data: 0.0008  max mem: 222\n",
      "Test:  [4900/5000]  eta: 0:00:16    time: 0.1626  data: 0.0010  max mem: 222\n",
      "Test: Total time: 0:13:23\n",
      "global correct: 90.8\n",
      "average row correct: ['94.7', '79.4', '72.5', '68.3', '55.3', '41.5', '73.1', '58.7', '87.9', '34.8', '75.4', '61.8', '77.7', '75.3', '77.7', '83.6', '38.9', '82.3', '54.3', '79.1', '56.1']\n",
      "IoU: ['89.8', '68.3', '56.1', '51.9', '46.3', '34.3', '66.9', '47.6', '74.2', '27.4', '61.2', '34.3', '63.7', '63.2', '66.8', '74.3', '25.4', '62.6', '44.4', '68.4', '47.6']\n",
      "mean IoU: 55.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d.chudakov/wrk/vision/references/segmentation/utils.py:295: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(val)\n"
     ]
    }
   ],
   "source": [
    "# Посчитаем метрики квантованной модели\n",
    "q_model.cpu()\n",
    "confmat = evaluate(q_model, data_loader_test, device='cpu', num_classes=num_classes)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8e149e-08b7-4f90-b3a7-e312adb6ece8",
   "metadata": {},
   "source": [
    "### Делаем Quantization Aware Training. Используем готовый трейнплуп от pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de375f28-bf82-4c3a-924d-03c3a75376e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d.chudakov/miniconda3/envs/road/lib/python3.8/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Делаем фейк квантование\n",
    "qat_model = fake_quantization(model, data_loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6d85d-82cc-4d99-81de-3d12a9e6e375",
   "metadata": {},
   "source": [
    "### Тут берём из train.py скрипт main() и вытаскиваем трейн луп"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d671589-a9e9-4459-a59a-b4cea6f653b5",
   "metadata": {},
   "source": [
    "1. Не забыть провалидировать модель fake quant до qat\n",
    "2. Не забыть провалидировать модель после обучения\n",
    "3. Конвертировать модель из fake quant в обычный quant\n",
    "4. Проверить точность и скорость модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "581b78a0-75db-470a-823a-c840acccf8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(inputs, target):\n",
    "    losses = {}\n",
    "    for name, x in inputs.items():\n",
    "        losses[name] = nn.functional.cross_entropy(x, target, ignore_index=255)\n",
    "\n",
    "    if len(losses) == 1:\n",
    "        return losses[\"out\"]\n",
    "\n",
    "    return losses[\"out\"] + 0.5 * losses[\"aux\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8d8b72c-76c2-4480-a1b1-8f7ee33e5c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfa71d29-c5bf-47fd-9d62-593345f76cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d.chudakov/miniconda3/envs/road/lib/python3.8/site-packages/torch/ao/quantization/utils.py:339: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n",
      "/home/d.chudakov/miniconda3/envs/road/lib/python3.8/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "qat_model.cpu()\n",
    "int_qat_model = convert_fx(qat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8be1e0ec-2760-4b20-b130-2e977a17d510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:16<00:00,  8.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# Замерим скорость квантованной модели на CPU\n",
    "sample = next(iter(data_loader_test))\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(2)):\n",
    "        int_qat_model(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f033ce72-f5e0-4dd3-8591-713fe1746003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qat_model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2807fc09-625d-4fdf-8a9f-d2e4804fb370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d.chudakov/miniconda3/envs/road/lib/python3.8/site-packages/torch/ao/quantization/fake_quantize.py:353: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025829503/work/aten/src/ATen/native/ReduceAllOps.cpp:72.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n",
      "/home/d.chudakov/miniconda3/envs/road/lib/python3.8/site-packages/torch/ao/quantization/fake_quantize.py:353: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025829503/work/aten/src/ATen/native/TensorCompare.cpp:677.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [  0/625]  eta: 0:13:42    time: 1.3167  data: 0.9676  max mem: 3378\n",
      "Test:  [100/625]  eta: 0:01:17    time: 0.1317  data: 0.0029  max mem: 3378\n",
      "Test:  [200/625]  eta: 0:00:59    time: 0.1341  data: 0.0030  max mem: 3378\n",
      "Test:  [300/625]  eta: 0:00:44    time: 0.1295  data: 0.0030  max mem: 3626\n",
      "Test:  [400/625]  eta: 0:00:30    time: 0.1331  data: 0.0032  max mem: 3626\n",
      "Test:  [500/625]  eta: 0:00:17    time: 0.1393  data: 0.0032  max mem: 3732\n",
      "Test:  [600/625]  eta: 0:00:03    time: 0.1300  data: 0.0031  max mem: 4657\n",
      "Test: Total time: 0:01:24\n",
      "global correct: 88.7\n",
      "average row correct: ['95.9', '58.5', '60.2', '51.7', '35.1', '19.9', '62.6', '41.4', '65.7', '13.7', '42.4', '42.5', '44.9', '46.7', '59.2', '71.8', '15.7', '50.1', '21.0', '70.9', '36.4']\n",
      "IoU: ['87.9', '52.6', '49.2', '27.8', '30.1', '18.0', '57.6', '34.6', '55.6', '12.6', '36.0', '27.8', '35.1', '41.7', '53.4', '63.4', '12.0', '43.9', '19.4', '62.5', '24.3']\n",
      "mean IoU: 40.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d.chudakov/wrk/deepschool/deeplab_quantization/utils.py:295: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(val)\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "confmat = evaluate(qat_model, data_loader_test, device=device, num_classes=num_classes)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb437f8c-c0c6-4944-8219-9453ad50592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr = args.lr * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de5a4053-98af-4036-85cd-986a3a90dfa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=8.80s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.23s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d.chudakov/miniconda3/envs/road/lib/python3.8/site-packages/torch/ao/quantization/fake_quantize.py:353: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025829503/work/aten/src/ATen/native/ReduceAllOps.cpp:72.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n",
      "/home/d.chudakov/miniconda3/envs/road/lib/python3.8/site-packages/torch/ao/quantization/fake_quantize.py:353: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025829503/work/aten/src/ATen/native/TensorCompare.cpp:677.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n",
      "/home/d.chudakov/miniconda3/envs/road/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1708025829503/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/2891]  eta: 3:20:04  lr: 9.996886836501132e-05  loss: 0.9736 (0.9736)  time: 4.1523  data: 2.1492  max mem: 18302\n",
      "Epoch: [0]  [  10/2891]  eta: 0:37:58  lr: 9.96574926992653e-05  loss: 0.9876 (0.9725)  time: 0.7909  data: 0.2028  max mem: 18331\n",
      "Epoch: [0]  [  20/2891]  eta: 0:29:45  lr: 9.934600889796806e-05  loss: 0.9120 (0.9233)  time: 0.4453  data: 0.0055  max mem: 18331\n",
      "Epoch: [0]  [  30/2891]  eta: 0:26:46  lr: 9.903441654658893e-05  loss: 0.8040 (0.8882)  time: 0.4354  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [  40/2891]  eta: 0:25:11  lr: 9.87227152275529e-05  loss: 0.8103 (0.8723)  time: 0.4342  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [  50/2891]  eta: 0:24:14  lr: 9.841090452020753e-05  loss: 0.8446 (0.8769)  time: 0.4351  data: 0.0034  max mem: 18331\n",
      "Epoch: [0]  [  60/2891]  eta: 0:23:35  lr: 9.809898400078932e-05  loss: 0.8446 (0.8681)  time: 0.4381  data: 0.0037  max mem: 18331\n",
      "Epoch: [0]  [  70/2891]  eta: 0:23:06  lr: 9.778695324238973e-05  loss: 0.7695 (0.8566)  time: 0.4390  data: 0.0039  max mem: 18331\n",
      "Epoch: [0]  [  80/2891]  eta: 0:22:41  lr: 9.747481181492043e-05  loss: 0.7564 (0.8445)  time: 0.4364  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [  90/2891]  eta: 0:22:22  lr: 9.716255928507848e-05  loss: 0.7717 (0.8419)  time: 0.4355  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 100/2891]  eta: 0:22:05  lr: 9.68501952163104e-05  loss: 0.7833 (0.8334)  time: 0.4363  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 110/2891]  eta: 0:21:50  lr: 9.65377191687763e-05  loss: 0.7993 (0.8319)  time: 0.4354  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 120/2891]  eta: 0:21:37  lr: 9.622513069931312e-05  loss: 0.8103 (0.8333)  time: 0.4352  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 130/2891]  eta: 0:21:25  lr: 9.591242936139748e-05  loss: 0.8478 (0.8400)  time: 0.4350  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 140/2891]  eta: 0:21:15  lr: 9.559961470510785e-05  loss: 0.9035 (0.8457)  time: 0.4351  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 150/2891]  eta: 0:21:05  lr: 9.528668627708636e-05  loss: 0.7750 (0.8411)  time: 0.4350  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 160/2891]  eta: 0:20:56  lr: 9.497364362049972e-05  loss: 0.7624 (0.8404)  time: 0.4357  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [ 170/2891]  eta: 0:20:47  lr: 9.466048627500001e-05  loss: 0.7937 (0.8396)  time: 0.4358  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 180/2891]  eta: 0:20:40  lr: 9.434721377668437e-05  loss: 0.7901 (0.8357)  time: 0.4356  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 190/2891]  eta: 0:20:32  lr: 9.403382565805451e-05  loss: 0.7901 (0.8319)  time: 0.4365  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 200/2891]  eta: 0:20:25  lr: 9.37203214479753e-05  loss: 0.7697 (0.8279)  time: 0.4363  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 210/2891]  eta: 0:20:18  lr: 9.340670067163293e-05  loss: 0.7537 (0.8250)  time: 0.4354  data: 0.0034  max mem: 18331\n",
      "Epoch: [0]  [ 220/2891]  eta: 0:20:11  lr: 9.309296285049231e-05  loss: 0.7537 (0.8236)  time: 0.4352  data: 0.0034  max mem: 18331\n",
      "Epoch: [0]  [ 230/2891]  eta: 0:20:04  lr: 9.277910750225378e-05  loss: 0.7800 (0.8245)  time: 0.4354  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [ 240/2891]  eta: 0:19:58  lr: 9.246513414080938e-05  loss: 0.7893 (0.8232)  time: 0.4356  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 250/2891]  eta: 0:19:52  lr: 9.215104227619828e-05  loss: 0.7871 (0.8236)  time: 0.4367  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 260/2891]  eta: 0:19:46  lr: 9.183683141456136e-05  loss: 0.8210 (0.8242)  time: 0.4375  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 270/2891]  eta: 0:19:40  lr: 9.152250105809535e-05  loss: 0.8393 (0.8249)  time: 0.4374  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [ 280/2891]  eta: 0:19:34  lr: 9.120805070500629e-05  loss: 0.8089 (0.8260)  time: 0.4371  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 290/2891]  eta: 0:19:29  lr: 9.089347984946188e-05  loss: 0.7890 (0.8243)  time: 0.4368  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 300/2891]  eta: 0:19:23  lr: 9.057878798154349e-05  loss: 0.7316 (0.8218)  time: 0.4379  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 310/2891]  eta: 0:19:18  lr: 9.026397458719719e-05  loss: 0.7659 (0.8220)  time: 0.4386  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 320/2891]  eta: 0:19:12  lr: 8.994903914818402e-05  loss: 0.7226 (0.8191)  time: 0.4376  data: 0.0034  max mem: 18331\n",
      "Epoch: [0]  [ 330/2891]  eta: 0:19:07  lr: 8.963398114202948e-05  loss: 0.7170 (0.8164)  time: 0.4387  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 340/2891]  eta: 0:19:02  lr: 8.93188000419724e-05  loss: 0.7644 (0.8159)  time: 0.4382  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 350/2891]  eta: 0:18:57  lr: 8.900349531691258e-05  loss: 0.7595 (0.8148)  time: 0.4371  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 360/2891]  eta: 0:18:52  lr: 8.868806643135802e-05  loss: 0.7388 (0.8137)  time: 0.4384  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 370/2891]  eta: 0:18:47  lr: 8.837251284537095e-05  loss: 0.7388 (0.8127)  time: 0.4389  data: 0.0034  max mem: 18331\n",
      "Epoch: [0]  [ 380/2891]  eta: 0:18:41  lr: 8.805683401451324e-05  loss: 0.7700 (0.8128)  time: 0.4381  data: 0.0034  max mem: 18331\n",
      "Epoch: [0]  [ 390/2891]  eta: 0:18:36  lr: 8.774102938979073e-05  loss: 0.7724 (0.8129)  time: 0.4375  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 400/2891]  eta: 0:18:31  lr: 8.742509841759673e-05  loss: 0.7524 (0.8117)  time: 0.4372  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 410/2891]  eta: 0:18:26  lr: 8.710904053965462e-05  loss: 0.7357 (0.8113)  time: 0.4373  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 420/2891]  eta: 0:18:21  lr: 8.679285519295948e-05  loss: 0.8112 (0.8114)  time: 0.4372  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 430/2891]  eta: 0:18:16  lr: 8.647654180971872e-05  loss: 0.7701 (0.8106)  time: 0.4366  data: 0.0029  max mem: 18331\n",
      "Epoch: [0]  [ 440/2891]  eta: 0:18:11  lr: 8.616009981729159e-05  loss: 0.7701 (0.8126)  time: 0.4367  data: 0.0029  max mem: 18331\n",
      "Epoch: [0]  [ 450/2891]  eta: 0:18:06  lr: 8.584352863812809e-05  loss: 0.8014 (0.8116)  time: 0.4363  data: 0.0029  max mem: 18331\n",
      "Epoch: [0]  [ 460/2891]  eta: 0:18:02  lr: 8.552682768970636e-05  loss: 0.7514 (0.8102)  time: 0.4377  data: 0.0029  max mem: 18331\n",
      "Epoch: [0]  [ 470/2891]  eta: 0:17:57  lr: 8.520999638446936e-05  loss: 0.7519 (0.8092)  time: 0.4389  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [ 480/2891]  eta: 0:17:52  lr: 8.48930341297603e-05  loss: 0.7582 (0.8076)  time: 0.4387  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 490/2891]  eta: 0:17:47  lr: 8.457594032775702e-05  loss: 0.7418 (0.8060)  time: 0.4375  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 500/2891]  eta: 0:17:42  lr: 8.425871437540531e-05  loss: 0.7288 (0.8048)  time: 0.4354  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 510/2891]  eta: 0:17:37  lr: 8.394135566435101e-05  loss: 0.7661 (0.8051)  time: 0.4361  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 520/2891]  eta: 0:17:33  lr: 8.362386358087098e-05  loss: 0.7758 (0.8050)  time: 0.4377  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 530/2891]  eta: 0:17:28  lr: 8.330623750580276e-05  loss: 0.8145 (0.8059)  time: 0.4381  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 540/2891]  eta: 0:17:23  lr: 8.298847681447325e-05  loss: 0.8487 (0.8073)  time: 0.4378  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [ 550/2891]  eta: 0:17:19  lr: 8.26705808766259e-05  loss: 0.8577 (0.8073)  time: 0.4369  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 560/2891]  eta: 0:17:14  lr: 8.235254905634672e-05  loss: 0.8263 (0.8074)  time: 0.4360  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 570/2891]  eta: 0:17:09  lr: 8.203438071198904e-05  loss: 0.7637 (0.8069)  time: 0.4362  data: 0.0029  max mem: 18331\n",
      "Epoch: [0]  [ 580/2891]  eta: 0:17:04  lr: 8.171607519609696e-05  loss: 0.7968 (0.8092)  time: 0.4370  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 590/2891]  eta: 0:17:00  lr: 8.139763185532717e-05  loss: 0.7831 (0.8087)  time: 0.4384  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 600/2891]  eta: 0:16:55  lr: 8.107905003036974e-05  loss: 0.7531 (0.8088)  time: 0.4385  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 610/2891]  eta: 0:16:50  lr: 8.076032905586734e-05  loss: 0.8155 (0.8093)  time: 0.4377  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 620/2891]  eta: 0:16:46  lr: 8.044146826033286e-05  loss: 0.7925 (0.8086)  time: 0.4381  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 630/2891]  eta: 0:16:41  lr: 8.012246696606585e-05  loss: 0.7625 (0.8074)  time: 0.4384  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 640/2891]  eta: 0:16:37  lr: 7.98033244890671e-05  loss: 0.7888 (0.8081)  time: 0.4375  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 650/2891]  eta: 0:16:32  lr: 7.948404013895202e-05  loss: 0.7728 (0.8072)  time: 0.4360  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [ 660/2891]  eta: 0:16:27  lr: 7.916461321886211e-05  loss: 0.7623 (0.8076)  time: 0.4360  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 670/2891]  eta: 0:16:23  lr: 7.8845043025375e-05  loss: 0.7424 (0.8065)  time: 0.4373  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 680/2891]  eta: 0:16:18  lr: 7.852532884841275e-05  loss: 0.7674 (0.8079)  time: 0.4369  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 690/2891]  eta: 0:16:13  lr: 7.820546997114848e-05  loss: 0.7597 (0.8067)  time: 0.4366  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 700/2891]  eta: 0:16:09  lr: 7.788546566991125e-05  loss: 0.7382 (0.8064)  time: 0.4376  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 710/2891]  eta: 0:16:04  lr: 7.756531521408913e-05  loss: 0.7142 (0.8055)  time: 0.4371  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 720/2891]  eta: 0:16:00  lr: 7.724501786603054e-05  loss: 0.7833 (0.8057)  time: 0.4364  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 730/2891]  eta: 0:15:55  lr: 7.692457288094357e-05  loss: 0.7430 (0.8044)  time: 0.4377  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 740/2891]  eta: 0:15:51  lr: 7.660397950679358e-05  loss: 0.6999 (0.8033)  time: 0.4386  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 750/2891]  eta: 0:15:46  lr: 7.62832369841987e-05  loss: 0.7010 (0.8024)  time: 0.4372  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 760/2891]  eta: 0:15:41  lr: 7.596234454632328e-05  loss: 0.7363 (0.8020)  time: 0.4371  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [ 770/2891]  eta: 0:15:37  lr: 7.564130141876949e-05  loss: 0.7363 (0.8016)  time: 0.4371  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [ 780/2891]  eta: 0:15:32  lr: 7.532010681946674e-05  loss: 0.7197 (0.8011)  time: 0.4363  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [ 790/2891]  eta: 0:15:28  lr: 7.499875995855879e-05  loss: 0.8025 (0.8013)  time: 0.4360  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [ 800/2891]  eta: 0:15:23  lr: 7.46772600382889e-05  loss: 0.7860 (0.8011)  time: 0.4372  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [ 810/2891]  eta: 0:15:19  lr: 7.435560625288267e-05  loss: 0.7860 (0.8009)  time: 0.4379  data: 0.0036  max mem: 18331\n",
      "Epoch: [0]  [ 820/2891]  eta: 0:15:14  lr: 7.403379778842828e-05  loss: 0.7948 (0.8008)  time: 0.4369  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 830/2891]  eta: 0:15:10  lr: 7.371183382275486e-05  loss: 0.7597 (0.8004)  time: 0.4369  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 840/2891]  eta: 0:15:05  lr: 7.338971352530797e-05  loss: 0.7597 (0.8002)  time: 0.4381  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [ 850/2891]  eta: 0:15:01  lr: 7.306743605702295e-05  loss: 0.7679 (0.8004)  time: 0.4379  data: 0.0029  max mem: 18331\n",
      "Epoch: [0]  [ 860/2891]  eta: 0:14:56  lr: 7.27450005701954e-05  loss: 0.7800 (0.8002)  time: 0.4364  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [ 870/2891]  eta: 0:14:52  lr: 7.242240620834942e-05  loss: 0.7303 (0.7995)  time: 0.4361  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 880/2891]  eta: 0:14:47  lr: 7.209965210610282e-05  loss: 0.7427 (0.7991)  time: 0.4364  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 890/2891]  eta: 0:14:42  lr: 7.177673738902962e-05  loss: 0.7738 (0.8002)  time: 0.4360  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [ 900/2891]  eta: 0:14:38  lr: 7.145366117352009e-05  loss: 0.8482 (0.8005)  time: 0.4370  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 910/2891]  eta: 0:14:34  lr: 7.113042256663746e-05  loss: 0.7749 (0.8004)  time: 0.4381  data: 0.0034  max mem: 18331\n",
      "Epoch: [0]  [ 920/2891]  eta: 0:14:29  lr: 7.080702066597196e-05  loss: 0.7749 (0.8004)  time: 0.4378  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [ 930/2891]  eta: 0:14:25  lr: 7.048345455949155e-05  loss: 0.7580 (0.8000)  time: 0.4383  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 940/2891]  eta: 0:14:20  lr: 7.015972332538986e-05  loss: 0.7284 (0.7987)  time: 0.4384  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 950/2891]  eta: 0:14:16  lr: 6.983582603193082e-05  loss: 0.6991 (0.7985)  time: 0.4379  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [ 960/2891]  eta: 0:14:11  lr: 6.951176173728962e-05  loss: 0.8216 (0.7992)  time: 0.4374  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 970/2891]  eta: 0:14:07  lr: 6.918752948939093e-05  loss: 0.8216 (0.7993)  time: 0.4365  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [ 980/2891]  eta: 0:14:02  lr: 6.886312832574327e-05  loss: 0.7862 (0.7991)  time: 0.4360  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [ 990/2891]  eta: 0:13:58  lr: 6.853855727326985e-05  loss: 0.7862 (0.7992)  time: 0.4376  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1000/2891]  eta: 0:13:53  lr: 6.821381534813575e-05  loss: 0.7233 (0.7986)  time: 0.4392  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1010/2891]  eta: 0:13:49  lr: 6.788890155557175e-05  loss: 0.6962 (0.7984)  time: 0.4382  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [1020/2891]  eta: 0:13:44  lr: 6.756381488969346e-05  loss: 0.7583 (0.7985)  time: 0.4382  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [1030/2891]  eta: 0:13:40  lr: 6.723855433331748e-05  loss: 0.7683 (0.7982)  time: 0.4417  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1040/2891]  eta: 0:13:36  lr: 6.691311885777276e-05  loss: 0.7313 (0.7976)  time: 0.4430  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [1050/2891]  eta: 0:13:31  lr: 6.658750742270813e-05  loss: 0.7138 (0.7969)  time: 0.4434  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [1060/2891]  eta: 0:13:27  lr: 6.626171897589539e-05  loss: 0.7159 (0.7964)  time: 0.4410  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1070/2891]  eta: 0:13:22  lr: 6.593575245302817e-05  loss: 0.7575 (0.7966)  time: 0.4368  data: 0.0028  max mem: 18331\n",
      "Epoch: [0]  [1080/2891]  eta: 0:13:18  lr: 6.560960677751613e-05  loss: 0.7739 (0.7966)  time: 0.4359  data: 0.0029  max mem: 18331\n",
      "Epoch: [0]  [1090/2891]  eta: 0:13:13  lr: 6.528328086027439e-05  loss: 0.7739 (0.7966)  time: 0.4362  data: 0.0028  max mem: 18331\n",
      "Epoch: [0]  [1100/2891]  eta: 0:13:09  lr: 6.49567735995084e-05  loss: 0.7518 (0.7963)  time: 0.4366  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [1110/2891]  eta: 0:13:04  lr: 6.463008388049369e-05  loss: 0.7571 (0.7963)  time: 0.4365  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1120/2891]  eta: 0:13:00  lr: 6.430321057535061e-05  loss: 0.7791 (0.7962)  time: 0.4371  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [1130/2891]  eta: 0:12:55  lr: 6.3976152542814e-05  loss: 0.7791 (0.7959)  time: 0.4365  data: 0.0028  max mem: 18331\n",
      "Epoch: [0]  [1140/2891]  eta: 0:12:51  lr: 6.364890862799722e-05  loss: 0.7829 (0.7959)  time: 0.4356  data: 0.0029  max mem: 18331\n",
      "Epoch: [0]  [1150/2891]  eta: 0:12:46  lr: 6.332147766215076e-05  loss: 0.7568 (0.7958)  time: 0.4361  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [1160/2891]  eta: 0:12:42  lr: 6.29938584624152e-05  loss: 0.7403 (0.7960)  time: 0.4365  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [1170/2891]  eta: 0:12:38  lr: 6.266604983156848e-05  loss: 0.8290 (0.7968)  time: 0.4374  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [1180/2891]  eta: 0:12:33  lr: 6.233805055776662e-05  loss: 0.8184 (0.7966)  time: 0.4378  data: 0.0034  max mem: 18331\n",
      "Epoch: [0]  [1190/2891]  eta: 0:12:29  lr: 6.200985941427887e-05  loss: 0.7534 (0.7963)  time: 0.4369  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [1200/2891]  eta: 0:12:24  lr: 6.1681475159216e-05  loss: 0.7235 (0.7957)  time: 0.4356  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [1210/2891]  eta: 0:12:20  lr: 6.135289653525214e-05  loss: 0.7212 (0.7951)  time: 0.4354  data: 0.0029  max mem: 18331\n",
      "Epoch: [0]  [1220/2891]  eta: 0:12:15  lr: 6.102412226933995e-05  loss: 0.7263 (0.7950)  time: 0.4360  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [1230/2891]  eta: 0:12:11  lr: 6.0695151072418614e-05  loss: 0.7050 (0.7942)  time: 0.4373  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [1240/2891]  eta: 0:12:06  lr: 6.036598163911477e-05  loss: 0.7483 (0.7945)  time: 0.4379  data: 0.0034  max mem: 18331\n",
      "Epoch: [0]  [1250/2891]  eta: 0:12:02  lr: 6.003661264743579e-05  loss: 0.7483 (0.7939)  time: 0.4368  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [1260/2891]  eta: 0:11:57  lr: 5.970704275845566e-05  loss: 0.7296 (0.7940)  time: 0.4362  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1270/2891]  eta: 0:11:53  lr: 5.937727061599283e-05  loss: 0.7813 (0.7935)  time: 0.4367  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [1280/2891]  eta: 0:11:49  lr: 5.9047294846279796e-05  loss: 0.7229 (0.7929)  time: 0.4367  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [1290/2891]  eta: 0:11:44  lr: 5.8717114057624484e-05  loss: 0.7288 (0.7927)  time: 0.4359  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [1300/2891]  eta: 0:11:40  lr: 5.8386726840062814e-05  loss: 0.7546 (0.7926)  time: 0.4367  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1310/2891]  eta: 0:11:35  lr: 5.8056131765002285e-05  loss: 0.7399 (0.7922)  time: 0.4395  data: 0.0034  max mem: 18331\n",
      "Epoch: [0]  [1320/2891]  eta: 0:11:31  lr: 5.7725327384856414e-05  loss: 0.7990 (0.7926)  time: 0.4398  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1330/2891]  eta: 0:11:26  lr: 5.7394312232669535e-05  loss: 0.7691 (0.7920)  time: 0.4383  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [1340/2891]  eta: 0:11:22  lr: 5.706308482173181e-05  loss: 0.7473 (0.7918)  time: 0.4380  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1350/2891]  eta: 0:11:18  lr: 5.673164364518389e-05  loss: 0.7568 (0.7918)  time: 0.4378  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [1360/2891]  eta: 0:11:13  lr: 5.639998717561134e-05  loss: 0.7673 (0.7918)  time: 0.4371  data: 0.0034  max mem: 18331\n",
      "Epoch: [0]  [1370/2891]  eta: 0:11:09  lr: 5.6068113864627876e-05  loss: 0.7673 (0.7917)  time: 0.4367  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [1380/2891]  eta: 0:11:04  lr: 5.573602214244755e-05  loss: 0.7331 (0.7912)  time: 0.4383  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1390/2891]  eta: 0:11:00  lr: 5.540371041744524e-05  loss: 0.7122 (0.7910)  time: 0.4378  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [1400/2891]  eta: 0:10:55  lr: 5.5071177075705195e-05  loss: 0.7150 (0.7905)  time: 0.4364  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1410/2891]  eta: 0:10:51  lr: 5.473842048055703e-05  loss: 0.7227 (0.7901)  time: 0.4366  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1420/2891]  eta: 0:10:47  lr: 5.4405438972099e-05  loss: 0.7515 (0.7901)  time: 0.4376  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1430/2891]  eta: 0:10:42  lr: 5.4072230866707824e-05  loss: 0.7526 (0.7897)  time: 0.4379  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [1440/2891]  eta: 0:10:38  lr: 5.373879445653495e-05  loss: 0.7390 (0.7894)  time: 0.4364  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [1450/2891]  eta: 0:10:33  lr: 5.340512800898828e-05  loss: 0.7834 (0.7897)  time: 0.4383  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [1460/2891]  eta: 0:10:29  lr: 5.307122976619945e-05  loss: 0.7894 (0.7893)  time: 0.4405  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [1470/2891]  eta: 0:10:25  lr: 5.273709794447553e-05  loss: 0.7049 (0.7889)  time: 0.4391  data: 0.0036  max mem: 18331\n",
      "Epoch: [0]  [1480/2891]  eta: 0:10:20  lr: 5.240273073373517e-05  loss: 0.6923 (0.7883)  time: 0.4380  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [1490/2891]  eta: 0:10:16  lr: 5.206812629692804e-05  loss: 0.7416 (0.7890)  time: 0.4385  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [1500/2891]  eta: 0:10:11  lr: 5.173328276943767e-05  loss: 0.8133 (0.7890)  time: 0.4382  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [1510/2891]  eta: 0:10:07  lr: 5.139819825846615e-05  loss: 0.7494 (0.7889)  time: 0.4377  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [1520/2891]  eta: 0:10:02  lr: 5.1062870842401e-05  loss: 0.7696 (0.7888)  time: 0.4384  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [1530/2891]  eta: 0:09:58  lr: 5.0727298570162826e-05  loss: 0.7696 (0.7887)  time: 0.4373  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1540/2891]  eta: 0:09:54  lr: 5.0391479460533436e-05  loss: 0.7070 (0.7883)  time: 0.4374  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [1550/2891]  eta: 0:09:49  lr: 5.005541150146341e-05  loss: 0.7762 (0.7881)  time: 0.4377  data: 0.0030  max mem: 18331\n",
      "Epoch: [0]  [1560/2891]  eta: 0:09:45  lr: 4.971909264935861e-05  loss: 0.7836 (0.7879)  time: 0.4362  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1570/2891]  eta: 0:09:40  lr: 4.9382520828344546e-05  loss: 0.7519 (0.7878)  time: 0.4363  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1580/2891]  eta: 0:09:36  lr: 4.904569392950808e-05  loss: 0.7446 (0.7874)  time: 0.4366  data: 0.0029  max mem: 18331\n",
      "Epoch: [0]  [1590/2891]  eta: 0:09:31  lr: 4.870860981011517e-05  loss: 0.7243 (0.7873)  time: 0.4367  data: 0.0029  max mem: 18331\n",
      "Epoch: [0]  [1600/2891]  eta: 0:09:27  lr: 4.8371266292804046e-05  loss: 0.7750 (0.7877)  time: 0.4368  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [1610/2891]  eta: 0:09:23  lr: 4.803366116475276e-05  loss: 0.7626 (0.7877)  time: 0.4376  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1620/2891]  eta: 0:09:18  lr: 4.769579217681997e-05  loss: 0.7201 (0.7871)  time: 0.4375  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [1630/2891]  eta: 0:09:14  lr: 4.7357657042658034e-05  loss: 0.7201 (0.7876)  time: 0.4360  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1640/2891]  eta: 0:09:09  lr: 4.7019253437797236e-05  loss: 0.8322 (0.7876)  time: 0.4359  data: 0.0031  max mem: 18331\n",
      "Epoch: [0]  [1650/2891]  eta: 0:09:05  lr: 4.66805789986999e-05  loss: 0.8030 (0.7880)  time: 0.4367  data: 0.0032  max mem: 18331\n",
      "Epoch: [0]  [1660/2891]  eta: 0:09:01  lr: 4.634163132178332e-05  loss: 0.7693 (0.7878)  time: 0.4371  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [1670/2891]  eta: 0:08:56  lr: 4.600240796240992e-05  loss: 0.7546 (0.7878)  time: 0.4378  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [1680/2891]  eta: 0:08:52  lr: 4.566290643384379e-05  loss: 0.8008 (0.7878)  time: 0.4390  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [1690/2891]  eta: 0:08:47  lr: 4.53231242061714e-05  loss: 0.7564 (0.7876)  time: 0.4383  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [1700/2891]  eta: 0:08:43  lr: 4.4983058705185874e-05  loss: 0.7199 (0.7873)  time: 0.4367  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [1710/2891]  eta: 0:08:39  lr: 4.4642707311232415e-05  loss: 0.7336 (0.7872)  time: 0.4370  data: 0.0034  max mem: 18331\n",
      "Epoch: [0]  [1720/2891]  eta: 0:08:34  lr: 4.4302067358013905e-05  loss: 0.7144 (0.7869)  time: 0.4373  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [1730/2891]  eta: 0:08:30  lr: 4.396113613135441e-05  loss: 0.7084 (0.7865)  time: 0.4377  data: 0.0036  max mem: 18331\n",
      "Epoch: [0]  [1740/2891]  eta: 0:08:25  lr: 4.361991086791917e-05  loss: 0.7195 (0.7865)  time: 0.4380  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [1750/2891]  eta: 0:08:21  lr: 4.32783887538886e-05  loss: 0.7230 (0.7862)  time: 0.4387  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [1760/2891]  eta: 0:08:17  lr: 4.293656692358502e-05  loss: 0.7230 (0.7860)  time: 0.4380  data: 0.0036  max mem: 18331\n",
      "Epoch: [0]  [1770/2891]  eta: 0:08:12  lr: 4.259444245804914e-05  loss: 0.7615 (0.7859)  time: 0.4376  data: 0.0036  max mem: 18331\n",
      "Epoch: [0]  [1780/2891]  eta: 0:08:08  lr: 4.225201238356457e-05  loss: 0.7629 (0.7860)  time: 0.4384  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [1790/2891]  eta: 0:08:03  lr: 4.1909273670127936e-05  loss: 0.7190 (0.7857)  time: 0.4405  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [1800/2891]  eta: 0:07:59  lr: 4.1566223229861626e-05  loss: 0.7111 (0.7856)  time: 0.4396  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [1810/2891]  eta: 0:07:55  lr: 4.122285791536713e-05  loss: 0.8008 (0.7860)  time: 0.4381  data: 0.0034  max mem: 18331\n",
      "Epoch: [0]  [1820/2891]  eta: 0:07:50  lr: 4.087917451801561e-05  loss: 0.7469 (0.7854)  time: 0.4410  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [1830/2891]  eta: 0:07:46  lr: 4.0535169766172966e-05  loss: 0.7048 (0.7850)  time: 0.4401  data: 0.0037  max mem: 18331\n",
      "Epoch: [0]  [1840/2891]  eta: 0:07:41  lr: 4.01908403233564e-05  loss: 0.7505 (0.7850)  time: 0.4399  data: 0.0039  max mem: 18331\n",
      "Epoch: [0]  [1850/2891]  eta: 0:07:37  lr: 3.9846182786318624e-05  loss: 0.7547 (0.7848)  time: 0.4392  data: 0.0037  max mem: 18331\n",
      "Epoch: [0]  [1860/2891]  eta: 0:07:33  lr: 3.9501193683056884e-05  loss: 0.7894 (0.7852)  time: 0.4382  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [1870/2891]  eta: 0:07:28  lr: 3.915586947074232e-05  loss: 0.8181 (0.7854)  time: 0.4382  data: 0.0036  max mem: 18331\n",
      "Epoch: [0]  [1880/2891]  eta: 0:07:24  lr: 3.881020653356631e-05  loss: 0.7732 (0.7852)  time: 0.4375  data: 0.0034  max mem: 18331\n",
      "Epoch: [0]  [1890/2891]  eta: 0:07:19  lr: 3.846420118049925e-05  loss: 0.7667 (0.7853)  time: 0.4387  data: 0.0036  max mem: 18331\n",
      "Epoch: [0]  [1900/2891]  eta: 0:07:15  lr: 3.811784964295737e-05  loss: 0.7705 (0.7855)  time: 0.4389  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [1910/2891]  eta: 0:07:11  lr: 3.777114807237284e-05  loss: 0.7622 (0.7855)  time: 0.4371  data: 0.0033  max mem: 18331\n",
      "Epoch: [0]  [1920/2891]  eta: 0:07:06  lr: 3.742409253766232e-05  loss: 0.7451 (0.7857)  time: 0.4381  data: 0.0036  max mem: 18331\n",
      "Epoch: [0]  [1930/2891]  eta: 0:07:02  lr: 3.7076679022588146e-05  loss: 0.7720 (0.7858)  time: 0.4398  data: 0.0037  max mem: 18331\n",
      "Epoch: [0]  [1940/2891]  eta: 0:06:57  lr: 3.672890342300717e-05  loss: 0.7098 (0.7854)  time: 0.4391  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [1950/2891]  eta: 0:06:53  lr: 3.638076154400029e-05  loss: 0.7226 (0.7853)  time: 0.4400  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [1960/2891]  eta: 0:06:49  lr: 3.603224909687716e-05  loss: 0.7522 (0.7851)  time: 0.4407  data: 0.0036  max mem: 18331\n",
      "Epoch: [0]  [1970/2891]  eta: 0:06:44  lr: 3.5683361696048496e-05  loss: 0.7200 (0.7847)  time: 0.4390  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [1980/2891]  eta: 0:06:40  lr: 3.533409485575895e-05  loss: 0.7020 (0.7844)  time: 0.4388  data: 0.0037  max mem: 18331\n",
      "Epoch: [0]  [1990/2891]  eta: 0:06:35  lr: 3.4984443986672814e-05  loss: 0.7267 (0.7844)  time: 0.4394  data: 0.0036  max mem: 18331\n",
      "Epoch: [0]  [2000/2891]  eta: 0:06:31  lr: 3.4634404392303885e-05  loss: 0.7931 (0.7847)  time: 0.4392  data: 0.0036  max mem: 18331\n",
      "Epoch: [0]  [2010/2891]  eta: 0:06:27  lr: 3.4283971265280934e-05  loss: 0.7581 (0.7844)  time: 0.4388  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [2020/2891]  eta: 0:06:22  lr: 3.393313968343894e-05  loss: 0.7067 (0.7844)  time: 0.4387  data: 0.0039  max mem: 18331\n",
      "Epoch: [0]  [2030/2891]  eta: 0:06:18  lr: 3.3581904605726016e-05  loss: 0.7404 (0.7843)  time: 0.4382  data: 0.0037  max mem: 18331\n",
      "Epoch: [0]  [2040/2891]  eta: 0:06:13  lr: 3.323026086791523e-05  loss: 0.7459 (0.7843)  time: 0.4382  data: 0.0037  max mem: 18331\n",
      "Epoch: [0]  [2050/2891]  eta: 0:06:09  lr: 3.287820317810909e-05  loss: 0.7459 (0.7840)  time: 0.4388  data: 0.0040  max mem: 18331\n",
      "Epoch: [0]  [2060/2891]  eta: 0:06:05  lr: 3.252572611202479e-05  loss: 0.7569 (0.7843)  time: 0.4384  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [2070/2891]  eta: 0:06:00  lr: 3.217282410804626e-05  loss: 0.7640 (0.7843)  time: 0.4380  data: 0.0035  max mem: 18331\n",
      "Epoch: [0]  [2080/2891]  eta: 0:05:56  lr: 3.18194914620286e-05  loss: 0.7744 (0.7843)  time: 0.4386  data: 0.0037  max mem: 18331\n",
      "Epoch: [0]  [2090/2891]  eta: 0:05:51  lr: 3.146572232183936e-05  loss: 0.7836 (0.7845)  time: 0.4379  data: 0.0036  max mem: 18331\n",
      "Epoch: [0]  [2100/2891]  eta: 0:05:47  lr: 3.111151068161993e-05  loss: 0.7852 (0.7846)  time: 0.4384  data: 0.0037  max mem: 18331\n",
      "Epoch: [0]  [2110/2891]  eta: 0:05:43  lr: 3.075685037574872e-05  loss: 0.7702 (0.7844)  time: 0.4394  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [2120/2891]  eta: 0:05:38  lr: 3.0401735072486962e-05  loss: 0.7206 (0.7845)  time: 0.4384  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [2130/2891]  eta: 0:05:34  lr: 3.004615826728574e-05  loss: 0.7206 (0.7843)  time: 0.4384  data: 0.0037  max mem: 18331\n",
      "Epoch: [0]  [2140/2891]  eta: 0:05:29  lr: 2.9690113275731746e-05  loss: 0.7600 (0.7842)  time: 0.4393  data: 0.0040  max mem: 18331\n",
      "Epoch: [0]  [2150/2891]  eta: 0:05:25  lr: 2.9333593226107128e-05  loss: 0.7199 (0.7840)  time: 0.4395  data: 0.0041  max mem: 18331\n",
      "Epoch: [0]  [2160/2891]  eta: 0:05:21  lr: 2.897659105153669e-05  loss: 0.7229 (0.7843)  time: 0.4372  data: 0.0037  max mem: 18331\n",
      "Epoch: [0]  [2170/2891]  eta: 0:05:16  lr: 2.861909948169373e-05  loss: 0.7536 (0.7841)  time: 0.4383  data: 0.0037  max mem: 18331\n",
      "Epoch: [0]  [2180/2891]  eta: 0:05:12  lr: 2.8261111034033146e-05  loss: 0.7686 (0.7842)  time: 0.4392  data: 0.0039  max mem: 18331\n",
      "Epoch: [0]  [2190/2891]  eta: 0:05:07  lr: 2.7902618004517852e-05  loss: 0.7676 (0.7839)  time: 0.4400  data: 0.0042  max mem: 18331\n",
      "Epoch: [0]  [2200/2891]  eta: 0:05:03  lr: 2.7543612457801614e-05  loss: 0.7044 (0.7839)  time: 0.4397  data: 0.0041  max mem: 18331\n",
      "Epoch: [0]  [2210/2891]  eta: 0:04:59  lr: 2.7184086216827917e-05  loss: 0.7759 (0.7838)  time: 0.4387  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [2220/2891]  eta: 0:04:54  lr: 2.6824030851801253e-05  loss: 0.7759 (0.7837)  time: 0.4394  data: 0.0041  max mem: 18331\n",
      "Epoch: [0]  [2230/2891]  eta: 0:04:50  lr: 2.6463437668482755e-05  loss: 0.7749 (0.7838)  time: 0.4390  data: 0.0042  max mem: 18331\n",
      "Epoch: [0]  [2240/2891]  eta: 0:04:46  lr: 2.6102297695758207e-05  loss: 0.7616 (0.7834)  time: 0.4407  data: 0.0041  max mem: 18331\n",
      "Epoch: [0]  [2250/2891]  eta: 0:04:41  lr: 2.5740601672421074e-05  loss: 0.7356 (0.7832)  time: 0.4399  data: 0.0040  max mem: 18331\n",
      "Epoch: [0]  [2260/2891]  eta: 0:04:37  lr: 2.5378340033107943e-05  loss: 0.7356 (0.7832)  time: 0.4380  data: 0.0040  max mem: 18331\n",
      "Epoch: [0]  [2270/2891]  eta: 0:04:32  lr: 2.5015502893317785e-05  loss: 0.7386 (0.7833)  time: 0.4396  data: 0.0041  max mem: 18331\n",
      "Epoch: [0]  [2280/2891]  eta: 0:04:28  lr: 2.4652080033439627e-05  loss: 0.7649 (0.7833)  time: 0.4403  data: 0.0039  max mem: 18331\n",
      "Epoch: [0]  [2290/2891]  eta: 0:04:24  lr: 2.428806088170542e-05  loss: 0.7564 (0.7834)  time: 0.4404  data: 0.0039  max mem: 18331\n",
      "Epoch: [0]  [2300/2891]  eta: 0:04:19  lr: 2.3923434495976946e-05  loss: 0.7745 (0.7836)  time: 0.4408  data: 0.0039  max mem: 18331\n",
      "Epoch: [0]  [2310/2891]  eta: 0:04:15  lr: 2.355818954426557e-05  loss: 0.7873 (0.7837)  time: 0.4409  data: 0.0041  max mem: 18331\n",
      "Epoch: [0]  [2320/2891]  eta: 0:04:10  lr: 2.3192314283873514e-05  loss: 0.7743 (0.7838)  time: 0.4402  data: 0.0044  max mem: 18331\n",
      "Epoch: [0]  [2330/2891]  eta: 0:04:06  lr: 2.2825796539033146e-05  loss: 0.7272 (0.7837)  time: 0.4400  data: 0.0046  max mem: 18331\n",
      "Epoch: [0]  [2340/2891]  eta: 0:04:02  lr: 2.2458623676907093e-05  loss: 0.7226 (0.7835)  time: 0.4394  data: 0.0043  max mem: 18331\n",
      "Epoch: [0]  [2350/2891]  eta: 0:03:57  lr: 2.209078258179738e-05  loss: 0.7568 (0.7835)  time: 0.4396  data: 0.0040  max mem: 18331\n",
      "Epoch: [0]  [2360/2891]  eta: 0:03:53  lr: 2.172225962739387e-05  loss: 0.7436 (0.7832)  time: 0.4404  data: 0.0041  max mem: 18331\n",
      "Epoch: [0]  [2370/2891]  eta: 0:03:48  lr: 2.1353040646873397e-05  loss: 0.6678 (0.7827)  time: 0.4387  data: 0.0041  max mem: 18331\n",
      "Epoch: [0]  [2380/2891]  eta: 0:03:44  lr: 2.098311090063833e-05  loss: 0.6667 (0.7825)  time: 0.4389  data: 0.0040  max mem: 18331\n",
      "Epoch: [0]  [2390/2891]  eta: 0:03:40  lr: 2.0612455041458132e-05  loss: 0.7233 (0.7824)  time: 0.4400  data: 0.0040  max mem: 18331\n",
      "Epoch: [0]  [2400/2891]  eta: 0:03:35  lr: 2.0241057076749053e-05  loss: 0.7532 (0.7822)  time: 0.4396  data: 0.0040  max mem: 18331\n",
      "Epoch: [0]  [2410/2891]  eta: 0:03:31  lr: 1.9868900327693045e-05  loss: 0.7136 (0.7819)  time: 0.4385  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [2420/2891]  eta: 0:03:26  lr: 1.9495967384860205e-05  loss: 0.7364 (0.7818)  time: 0.4389  data: 0.0039  max mem: 18331\n",
      "Epoch: [0]  [2430/2891]  eta: 0:03:22  lr: 1.9122240059954412e-05  loss: 0.7545 (0.7819)  time: 0.4392  data: 0.0040  max mem: 18331\n",
      "Epoch: [0]  [2440/2891]  eta: 0:03:18  lr: 1.8747699333251666e-05  loss: 0.7545 (0.7820)  time: 0.4390  data: 0.0040  max mem: 18331\n",
      "Epoch: [0]  [2450/2891]  eta: 0:03:13  lr: 1.8372325296242474e-05  loss: 0.7386 (0.7818)  time: 0.4402  data: 0.0041  max mem: 18331\n",
      "Epoch: [0]  [2460/2891]  eta: 0:03:09  lr: 1.7996097088921265e-05  loss: 0.6958 (0.7818)  time: 0.4406  data: 0.0044  max mem: 18331\n",
      "Epoch: [0]  [2470/2891]  eta: 0:03:04  lr: 1.7618992831086888e-05  loss: 0.7002 (0.7816)  time: 0.4410  data: 0.0045  max mem: 18331\n",
      "Epoch: [0]  [2480/2891]  eta: 0:03:00  lr: 1.7240989546925962e-05  loss: 0.7410 (0.7817)  time: 0.4411  data: 0.0043  max mem: 18331\n",
      "Epoch: [0]  [2490/2891]  eta: 0:02:56  lr: 1.6862063082041688e-05  loss: 0.7393 (0.7815)  time: 0.4398  data: 0.0042  max mem: 18331\n",
      "Epoch: [0]  [2500/2891]  eta: 0:02:51  lr: 1.648218801196348e-05  loss: 0.7128 (0.7814)  time: 0.4388  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [2510/2891]  eta: 0:02:47  lr: 1.6101337541021186e-05  loss: 0.7488 (0.7813)  time: 0.4397  data: 0.0040  max mem: 18331\n",
      "Epoch: [0]  [2520/2891]  eta: 0:02:43  lr: 1.5719483390288023e-05  loss: 0.7641 (0.7813)  time: 0.4396  data: 0.0043  max mem: 18331\n",
      "Epoch: [0]  [2530/2891]  eta: 0:02:38  lr: 1.5336595673082793e-05  loss: 0.7311 (0.7813)  time: 0.4390  data: 0.0040  max mem: 18331\n",
      "Epoch: [0]  [2540/2891]  eta: 0:02:34  lr: 1.4952642756264839e-05  loss: 0.7555 (0.7813)  time: 0.4397  data: 0.0040  max mem: 18331\n",
      "Epoch: [0]  [2550/2891]  eta: 0:02:29  lr: 1.4567591105247425e-05  loss: 0.7697 (0.7812)  time: 0.4403  data: 0.0042  max mem: 18331\n",
      "Epoch: [0]  [2560/2891]  eta: 0:02:25  lr: 1.4181405110281606e-05  loss: 0.7790 (0.7814)  time: 0.4397  data: 0.0040  max mem: 18331\n",
      "Epoch: [0]  [2570/2891]  eta: 0:02:21  lr: 1.3794046891109155e-05  loss: 0.7230 (0.7813)  time: 0.4387  data: 0.0040  max mem: 18331\n",
      "Epoch: [0]  [2580/2891]  eta: 0:02:16  lr: 1.3405476076528968e-05  loss: 0.7060 (0.7810)  time: 0.4382  data: 0.0039  max mem: 18331\n",
      "Epoch: [0]  [2590/2891]  eta: 0:02:12  lr: 1.3015649554738896e-05  loss: 0.7060 (0.7809)  time: 0.4388  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [2600/2891]  eta: 0:02:07  lr: 1.2624521189471922e-05  loss: 0.7528 (0.7808)  time: 0.4394  data: 0.0039  max mem: 18331\n",
      "Epoch: [0]  [2610/2891]  eta: 0:02:03  lr: 1.2232041495894265e-05  loss: 0.7361 (0.7805)  time: 0.4386  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [2620/2891]  eta: 0:01:59  lr: 1.1838157268914726e-05  loss: 0.7140 (0.7805)  time: 0.4390  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [2630/2891]  eta: 0:01:54  lr: 1.144281115488887e-05  loss: 0.7431 (0.7804)  time: 0.4395  data: 0.0041  max mem: 18331\n",
      "Epoch: [0]  [2640/2891]  eta: 0:01:50  lr: 1.104594115557865e-05  loss: 0.7641 (0.7805)  time: 0.4397  data: 0.0045  max mem: 18331\n",
      "Epoch: [0]  [2650/2891]  eta: 0:01:45  lr: 1.0647480050500043e-05  loss: 0.8175 (0.7806)  time: 0.4387  data: 0.0042  max mem: 18331\n",
      "Epoch: [0]  [2660/2891]  eta: 0:01:41  lr: 1.0247354720253793e-05  loss: 0.7250 (0.7803)  time: 0.4377  data: 0.0039  max mem: 18331\n",
      "Epoch: [0]  [2670/2891]  eta: 0:01:37  lr: 9.84548534879848e-06  loss: 0.7181 (0.7805)  time: 0.4374  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [2680/2891]  eta: 0:01:32  lr: 9.441784476488069e-06  loss: 0.7399 (0.7803)  time: 0.4371  data: 0.0036  max mem: 18331\n",
      "Epoch: [0]  [2690/2891]  eta: 0:01:28  lr: 9.036155867472618e-06  loss: 0.7363 (0.7803)  time: 0.4370  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [2700/2891]  eta: 0:01:23  lr: 8.628493143902942e-06  loss: 0.7525 (0.7803)  time: 0.4368  data: 0.0037  max mem: 18331\n",
      "Epoch: [0]  [2710/2891]  eta: 0:01:19  lr: 8.218678124029545e-06  loss: 0.7582 (0.7802)  time: 0.4374  data: 0.0036  max mem: 18331\n",
      "Epoch: [0]  [2720/2891]  eta: 0:01:15  lr: 7.80657877984198e-06  loss: 0.7882 (0.7803)  time: 0.4385  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [2730/2891]  eta: 0:01:10  lr: 7.3920466994360295e-06  loss: 0.7704 (0.7802)  time: 0.4394  data: 0.0041  max mem: 18331\n",
      "Epoch: [0]  [2740/2891]  eta: 0:01:06  lr: 6.974913895220743e-06  loss: 0.7494 (0.7800)  time: 0.4392  data: 0.0039  max mem: 18331\n",
      "Epoch: [0]  [2750/2891]  eta: 0:01:01  lr: 6.554988733971114e-06  loss: 0.7631 (0.7800)  time: 0.4385  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [2760/2891]  eta: 0:00:57  lr: 6.132050666312096e-06  loss: 0.7258 (0.7797)  time: 0.4389  data: 0.0039  max mem: 18331\n",
      "Epoch: [0]  [2770/2891]  eta: 0:00:53  lr: 5.705843280505286e-06  loss: 0.7314 (0.7797)  time: 0.4384  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [2780/2891]  eta: 0:00:48  lr: 5.276064961316049e-06  loss: 0.7464 (0.7798)  time: 0.4374  data: 0.0036  max mem: 18331\n",
      "Epoch: [0]  [2790/2891]  eta: 0:00:44  lr: 4.842356031049392e-06  loss: 0.7289 (0.7799)  time: 0.4383  data: 0.0043  max mem: 18331\n",
      "Epoch: [0]  [2800/2891]  eta: 0:00:39  lr: 4.404280555231228e-06  loss: 0.7745 (0.7799)  time: 0.4396  data: 0.0044  max mem: 18331\n",
      "Epoch: [0]  [2810/2891]  eta: 0:00:35  lr: 3.9612997431422695e-06  loss: 0.7945 (0.7800)  time: 0.4405  data: 0.0039  max mem: 18331\n",
      "Epoch: [0]  [2820/2891]  eta: 0:00:31  lr: 3.5127314861473155e-06  loss: 0.7566 (0.7799)  time: 0.4419  data: 0.0041  max mem: 18331\n",
      "Epoch: [0]  [2830/2891]  eta: 0:00:26  lr: 3.0576857047182892e-06  loss: 0.7234 (0.7797)  time: 0.4412  data: 0.0042  max mem: 18331\n",
      "Epoch: [0]  [2840/2891]  eta: 0:00:22  lr: 2.594954345115654e-06  loss: 0.6919 (0.7796)  time: 0.4386  data: 0.0038  max mem: 18331\n",
      "Epoch: [0]  [2850/2891]  eta: 0:00:18  lr: 2.1228079709258664e-06  loss: 0.7256 (0.7794)  time: 0.4381  data: 0.0036  max mem: 18331\n",
      "Epoch: [0]  [2860/2891]  eta: 0:00:13  lr: 1.6385731975468226e-06  loss: 0.7256 (0.7792)  time: 0.4394  data: 0.0041  max mem: 18331\n",
      "Epoch: [0]  [2870/2891]  eta: 0:00:09  lr: 1.1375846246494289e-06  loss: 0.6575 (0.7788)  time: 0.4398  data: 0.0042  max mem: 18331\n",
      "Epoch: [0]  [2880/2891]  eta: 0:00:04  lr: 6.096165060442751e-07  loss: 0.6946 (0.7788)  time: 0.4343  data: 0.0040  max mem: 18331\n",
      "Epoch: [0]  [2890/2891]  eta: 0:00:00  lr: 0.0  loss: 0.7648 (0.7790)  time: 0.4242  data: 0.0034  max mem: 18331\n",
      "Epoch: [0] Total time: 0:21:10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'confmat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m train_one_epoch(qat_model, criterion, optimizer, data_loader, lr_scheduler, device, epoch, args\u001b[38;5;241m.\u001b[39mprint_freq, scaler)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# confmat = evaluate(qat_model, data_loader_test, device=device, num_classes=num_classes)\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mconfmat\u001b[49m)\n\u001b[1;32m     70\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: qat_model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[1;32m     76\u001b[0m }\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mamp:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'confmat' is not defined"
     ]
    }
   ],
   "source": [
    "if args.output_dir:\n",
    "    utils.mkdir(args.output_dir)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "dataset, num_classes = get_dataset(args, is_train=True)\n",
    "dataset_test, _ = get_dataset(args, is_train=False)\n",
    "\n",
    "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=args.workers,\n",
    "    collate_fn=utils.collate_fn,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, sampler=test_sampler, num_workers=args.workers, collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "params_to_optimize = [\n",
    "    {\"params\": [p for p in qat_model.backbone.parameters() if p.requires_grad]},\n",
    "    {\"params\": [p for p in qat_model.classifier.parameters() if p.requires_grad]},\n",
    "]\n",
    "if args.aux_loss:\n",
    "    params = [p for p in qat_model.aux_classifier.parameters() if p.requires_grad]\n",
    "    params_to_optimize.append({\"params\": params, \"lr\": args.lr})\n",
    "    \n",
    "optimizer = torch.optim.SGD(params_to_optimize, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler() if args.amp else None\n",
    "\n",
    "iters_per_epoch = len(data_loader)\n",
    "main_lr_scheduler = PolynomialLR(\n",
    "    optimizer, total_iters=iters_per_epoch * (args.epochs - args.lr_warmup_epochs), power=0.9\n",
    ")\n",
    "\n",
    "if args.lr_warmup_epochs > 0:\n",
    "    warmup_iters = iters_per_epoch * args.lr_warmup_epochs\n",
    "    args.lr_warmup_method = args.lr_warmup_method.lower()\n",
    "    if args.lr_warmup_method == \"linear\":\n",
    "        warmup_lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=args.lr_warmup_decay, total_iters=warmup_iters\n",
    "        )\n",
    "    elif args.lr_warmup_method == \"constant\":\n",
    "        warmup_lr_scheduler = torch.optim.lr_scheduler.ConstantLR(\n",
    "            optimizer, factor=args.lr_warmup_decay, total_iters=warmup_iters\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f\"Invalid warmup lr method '{args.lr_warmup_method}'. Only linear and constant are supported.\"\n",
    "        )\n",
    "    lr_scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "        optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[warmup_iters]\n",
    "    )\n",
    "else:\n",
    "    lr_scheduler = main_lr_scheduler\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    train_one_epoch(qat_model, criterion, optimizer, data_loader, lr_scheduler, device, epoch, args.print_freq, scaler)\n",
    "    confmat = evaluate(qat_model, data_loader_test, device=device, num_classes=num_classes)\n",
    "    print(confmat)\n",
    "    checkpoint = {\n",
    "        \"model\": qat_model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"args\": args,\n",
    "    }\n",
    "    if args.amp:\n",
    "        checkpoint[\"scaler\"] = scaler.state_dict()\n",
    "    utils.save_on_master(checkpoint, os.path.join(args.output_dir, f\"model_{epoch}.pth\"))\n",
    "    utils.save_on_master(checkpoint, os.path.join(args.output_dir, \"checkpoint.pth\"))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print(f\"Training time {total_time_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fd4b5fd-b97c-4c62-9c5b-aed6a4593c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [   0/5000]  eta: 0:43:49    time: 0.5259  data: 0.3771  max mem: 18331\n",
      "Test:  [ 100/5000]  eta: 0:02:52    time: 0.0282  data: 0.0010  max mem: 18331\n",
      "Test:  [ 200/5000]  eta: 0:02:24    time: 0.0245  data: 0.0009  max mem: 18331\n",
      "Test:  [ 300/5000]  eta: 0:02:15    time: 0.0253  data: 0.0010  max mem: 18331\n",
      "Test:  [ 400/5000]  eta: 0:02:10    time: 0.0303  data: 0.0011  max mem: 18331\n",
      "Test:  [ 500/5000]  eta: 0:02:05    time: 0.0264  data: 0.0010  max mem: 18331\n",
      "Test:  [ 600/5000]  eta: 0:02:01    time: 0.0260  data: 0.0010  max mem: 18331\n",
      "Test:  [ 700/5000]  eta: 0:01:57    time: 0.0245  data: 0.0010  max mem: 18331\n",
      "Test:  [ 800/5000]  eta: 0:01:53    time: 0.0256  data: 0.0010  max mem: 18331\n",
      "Test:  [ 900/5000]  eta: 0:01:50    time: 0.0242  data: 0.0010  max mem: 18331\n",
      "Test:  [1000/5000]  eta: 0:01:46    time: 0.0268  data: 0.0009  max mem: 18331\n",
      "Test:  [1100/5000]  eta: 0:01:44    time: 0.0284  data: 0.0010  max mem: 18331\n",
      "Test:  [1200/5000]  eta: 0:01:41    time: 0.0266  data: 0.0011  max mem: 18331\n",
      "Test:  [1300/5000]  eta: 0:01:39    time: 0.0276  data: 0.0009  max mem: 18331\n",
      "Test:  [1400/5000]  eta: 0:01:36    time: 0.0261  data: 0.0010  max mem: 18331\n",
      "Test:  [1500/5000]  eta: 0:01:33    time: 0.0259  data: 0.0010  max mem: 18331\n",
      "Test:  [1600/5000]  eta: 0:01:30    time: 0.0278  data: 0.0010  max mem: 18331\n",
      "Test:  [1700/5000]  eta: 0:01:28    time: 0.0268  data: 0.0010  max mem: 18331\n",
      "Test:  [1800/5000]  eta: 0:01:26    time: 0.0315  data: 0.0010  max mem: 18331\n",
      "Test:  [1900/5000]  eta: 0:01:23    time: 0.0267  data: 0.0010  max mem: 18331\n",
      "Test:  [2000/5000]  eta: 0:01:20    time: 0.0277  data: 0.0010  max mem: 18331\n",
      "Test:  [2100/5000]  eta: 0:01:18    time: 0.0275  data: 0.0009  max mem: 18331\n",
      "Test:  [2200/5000]  eta: 0:01:15    time: 0.0280  data: 0.0010  max mem: 18331\n",
      "Test:  [2300/5000]  eta: 0:01:13    time: 0.0277  data: 0.0010  max mem: 18331\n",
      "Test:  [2400/5000]  eta: 0:01:10    time: 0.0286  data: 0.0010  max mem: 18331\n",
      "Test:  [2500/5000]  eta: 0:01:07    time: 0.0305  data: 0.0010  max mem: 18331\n",
      "Test:  [2600/5000]  eta: 0:01:05    time: 0.0303  data: 0.0009  max mem: 18331\n",
      "Test:  [2700/5000]  eta: 0:01:02    time: 0.0283  data: 0.0009  max mem: 18331\n",
      "Test:  [2800/5000]  eta: 0:01:00    time: 0.0279  data: 0.0010  max mem: 18331\n",
      "Test:  [2900/5000]  eta: 0:00:57    time: 0.0287  data: 0.0010  max mem: 18331\n",
      "Test:  [3000/5000]  eta: 0:00:54    time: 0.0278  data: 0.0009  max mem: 18331\n",
      "Test:  [3100/5000]  eta: 0:00:52    time: 0.0284  data: 0.0009  max mem: 18331\n",
      "Test:  [3200/5000]  eta: 0:00:49    time: 0.0290  data: 0.0010  max mem: 18331\n",
      "Test:  [3300/5000]  eta: 0:00:46    time: 0.0296  data: 0.0010  max mem: 18331\n",
      "Test:  [3400/5000]  eta: 0:00:44    time: 0.0288  data: 0.0011  max mem: 18331\n",
      "Test:  [3500/5000]  eta: 0:00:41    time: 0.0289  data: 0.0010  max mem: 18331\n",
      "Test:  [3600/5000]  eta: 0:00:38    time: 0.0293  data: 0.0010  max mem: 18331\n",
      "Test:  [3700/5000]  eta: 0:00:35    time: 0.0292  data: 0.0010  max mem: 18331\n",
      "Test:  [3800/5000]  eta: 0:00:33    time: 0.0290  data: 0.0010  max mem: 18331\n",
      "Test:  [3900/5000]  eta: 0:00:30    time: 0.0292  data: 0.0010  max mem: 18331\n",
      "Test:  [4000/5000]  eta: 0:00:27    time: 0.0300  data: 0.0010  max mem: 18331\n",
      "Test:  [4100/5000]  eta: 0:00:25    time: 0.0291  data: 0.0009  max mem: 18331\n",
      "Test:  [4200/5000]  eta: 0:00:22    time: 0.0314  data: 0.0009  max mem: 18331\n",
      "Test:  [4300/5000]  eta: 0:00:19    time: 0.0294  data: 0.0009  max mem: 18331\n",
      "Test:  [4400/5000]  eta: 0:00:16    time: 0.0302  data: 0.0009  max mem: 18331\n",
      "Test:  [4500/5000]  eta: 0:00:14    time: 0.0301  data: 0.0010  max mem: 18331\n",
      "Test:  [4600/5000]  eta: 0:00:11    time: 0.0304  data: 0.0010  max mem: 18331\n",
      "Test:  [4700/5000]  eta: 0:00:08    time: 0.0309  data: 0.0009  max mem: 18331\n",
      "Test:  [4800/5000]  eta: 0:00:05    time: 0.0302  data: 0.0009  max mem: 18331\n",
      "Test:  [4900/5000]  eta: 0:00:02    time: 0.0308  data: 0.0010  max mem: 18331\n",
      "Test: Total time: 0:02:21\n",
      "global correct: 88.5\n",
      "average row correct: ['91.9', '81.2', '70.0', '65.1', '43.9', '44.9', '81.5', '53.7', '87.9', '37.3', '61.0', '58.4', '75.7', '74.4', '80.7', '84.1', '48.3', '82.5', '57.0', '71.1', '55.4']\n",
      "IoU: ['87.4', '57.4', '46.5', '37.3', '34.3', '25.8', '68.1', '42.7', '57.3', '26.4', '50.4', '29.1', '49.9', '55.1', '64.0', '70.8', '14.3', '60.3', '37.5', '64.9', '44.1']\n",
      "mean IoU: 48.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d.chudakov/wrk/deepschool/deeplab_quantization/utils.py:295: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(val)\n"
     ]
    }
   ],
   "source": [
    "confmat = evaluate(qat_model, data_loader_test, device=device, num_classes=num_classes)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bfc0539-a31a-46de-b3f4-80cb2ca2cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model.cpu()\n",
    "int_qat_model = convert_fx(deepcopy(qat_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d99794d-febc-4d9b-905b-ff95b161ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_int_qat_model = fuse_fx(deepcopy(int_qat_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dd62bb0-5e17-4a60-9d11-6b8a500dfe01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [   0/5000]  eta: 0:52:53    time: 0.6346  data: 0.4461  max mem: 18331\n",
      "Test:  [ 100/5000]  eta: 0:12:58    time: 0.1477  data: 0.0009  max mem: 18331\n",
      "Test:  [ 200/5000]  eta: 0:12:29    time: 0.1544  data: 0.0009  max mem: 18331\n",
      "Test:  [ 300/5000]  eta: 0:12:07    time: 0.1579  data: 0.0009  max mem: 18331\n",
      "Test:  [ 400/5000]  eta: 0:11:53    time: 0.1583  data: 0.0009  max mem: 18331\n",
      "Test:  [ 500/5000]  eta: 0:11:40    time: 0.1538  data: 0.0009  max mem: 18331\n",
      "Test:  [ 600/5000]  eta: 0:11:25    time: 0.1590  data: 0.0009  max mem: 18331\n",
      "Test:  [ 700/5000]  eta: 0:11:10    time: 0.1606  data: 0.0009  max mem: 18331\n",
      "Test:  [ 800/5000]  eta: 0:10:55    time: 0.1624  data: 0.0010  max mem: 18331\n",
      "Test:  [ 900/5000]  eta: 0:10:40    time: 0.1525  data: 0.0008  max mem: 18331\n",
      "Test:  [1000/5000]  eta: 0:10:26    time: 0.1667  data: 0.0009  max mem: 18331\n",
      "Test:  [1100/5000]  eta: 0:10:11    time: 0.1571  data: 0.0009  max mem: 18331\n",
      "Test:  [1200/5000]  eta: 0:09:56    time: 0.1625  data: 0.0009  max mem: 18331\n",
      "Test:  [1300/5000]  eta: 0:09:41    time: 0.1535  data: 0.0009  max mem: 18331\n",
      "Test:  [1400/5000]  eta: 0:09:25    time: 0.1531  data: 0.0010  max mem: 18331\n",
      "Test:  [1500/5000]  eta: 0:09:09    time: 0.1634  data: 0.0009  max mem: 18331\n",
      "Test:  [1600/5000]  eta: 0:08:54    time: 0.1620  data: 0.0009  max mem: 18331\n",
      "Test:  [1700/5000]  eta: 0:08:39    time: 0.1626  data: 0.0010  max mem: 18331\n",
      "Test:  [1800/5000]  eta: 0:08:24    time: 0.1749  data: 0.0008  max mem: 18331\n",
      "Test:  [1900/5000]  eta: 0:08:08    time: 0.1587  data: 0.0009  max mem: 18331\n",
      "Test:  [2000/5000]  eta: 0:07:52    time: 0.1586  data: 0.0009  max mem: 18331\n",
      "Test:  [2100/5000]  eta: 0:07:37    time: 0.1495  data: 0.0009  max mem: 18331\n",
      "Test:  [2200/5000]  eta: 0:07:21    time: 0.1600  data: 0.0008  max mem: 18331\n",
      "Test:  [2300/5000]  eta: 0:07:05    time: 0.1568  data: 0.0010  max mem: 18331\n",
      "Test:  [2400/5000]  eta: 0:06:49    time: 0.1579  data: 0.0008  max mem: 18331\n",
      "Test:  [2500/5000]  eta: 0:06:34    time: 0.1631  data: 0.0009  max mem: 18331\n",
      "Test:  [2600/5000]  eta: 0:06:18    time: 0.1733  data: 0.0010  max mem: 18331\n",
      "Test:  [2700/5000]  eta: 0:06:03    time: 0.1663  data: 0.0008  max mem: 18331\n",
      "Test:  [2800/5000]  eta: 0:05:47    time: 0.1569  data: 0.0009  max mem: 18331\n",
      "Test:  [2900/5000]  eta: 0:05:31    time: 0.1653  data: 0.0010  max mem: 18331\n",
      "Test:  [3000/5000]  eta: 0:05:16    time: 0.1553  data: 0.0009  max mem: 18331\n",
      "Test:  [3100/5000]  eta: 0:05:00    time: 0.1552  data: 0.0008  max mem: 18331\n",
      "Test:  [3200/5000]  eta: 0:04:44    time: 0.1624  data: 0.0010  max mem: 18331\n",
      "Test:  [3300/5000]  eta: 0:04:29    time: 0.1661  data: 0.0009  max mem: 18331\n",
      "Test:  [3400/5000]  eta: 0:04:13    time: 0.1554  data: 0.0009  max mem: 18331\n",
      "Test:  [3500/5000]  eta: 0:03:57    time: 0.1553  data: 0.0010  max mem: 18331\n",
      "Test:  [3600/5000]  eta: 0:03:41    time: 0.1600  data: 0.0010  max mem: 18331\n",
      "Test:  [3700/5000]  eta: 0:03:26    time: 0.1632  data: 0.0010  max mem: 18331\n",
      "Test:  [3800/5000]  eta: 0:03:10    time: 0.1641  data: 0.0009  max mem: 18331\n",
      "Test:  [3900/5000]  eta: 0:02:54    time: 0.1583  data: 0.0009  max mem: 18331\n",
      "Test:  [4000/5000]  eta: 0:02:38    time: 0.1532  data: 0.0008  max mem: 18331\n",
      "Test:  [4100/5000]  eta: 0:02:22    time: 0.1609  data: 0.0008  max mem: 18331\n",
      "Test:  [4200/5000]  eta: 0:02:06    time: 0.1649  data: 0.0008  max mem: 18331\n",
      "Test:  [4300/5000]  eta: 0:01:51    time: 0.1631  data: 0.0012  max mem: 18331\n",
      "Test:  [4400/5000]  eta: 0:01:35    time: 0.1659  data: 0.0010  max mem: 18331\n",
      "Test:  [4500/5000]  eta: 0:01:19    time: 0.1603  data: 0.0010  max mem: 18331\n",
      "Test:  [4600/5000]  eta: 0:01:03    time: 0.1654  data: 0.0009  max mem: 18331\n",
      "Test:  [4700/5000]  eta: 0:00:47    time: 0.1684  data: 0.0010  max mem: 18331\n",
      "Test:  [4800/5000]  eta: 0:00:31    time: 0.1589  data: 0.0009  max mem: 18331\n",
      "Test:  [4900/5000]  eta: 0:00:15    time: 0.1579  data: 0.0009  max mem: 18331\n",
      "Test: Total time: 0:13:15\n",
      "global correct: 88.6\n",
      "average row correct: ['92.1', '78.4', '71.3', '64.9', '39.4', '42.5', '77.8', '53.6', '88.3', '34.9', '57.0', '61.0', '78.1', '74.7', '78.2', '83.8', '53.8', '78.9', '58.5', '69.5', '55.9']\n",
      "IoU: ['87.5', '63.1', '44.3', '38.4', '32.5', '27.1', '66.3', '42.0', '59.8', '24.9', '50.0', '30.4', '49.6', '56.2', '62.0', '70.7', '15.8', '57.1', '35.4', '64.1', '45.0']\n",
      "mean IoU: 48.7\n"
     ]
    }
   ],
   "source": [
    "confmat = evaluate(int_qat_model, data_loader_test, device='cpu', num_classes=num_classes)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf0e302d-586c-4304-8d44-0a6cb5235e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d.chudakov/miniconda3/envs/road/lib/python3.8/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Делаем фейк квантование\n",
    "qat_model = fake_quantization(model, data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c16b631d-1afa-466b-be8c-1f302f70ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d483e87c-5905-4f2c-bac3-13e4564b8643",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f83e30de-5a6a-45a2-bf7e-27b6ac3c9d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_loss = torch.nn.MSELoss()\n",
    "\n",
    "def criterion_distill(inputs, target):\n",
    "    losses = {}\n",
    "    for (name, x), (name_t, x_t) in zip(inputs.items(), target.items()):\n",
    "        losses[name] = rmse_loss(x, x_t)\n",
    "\n",
    "    if len(losses) == 1:\n",
    "        return losses[\"out\"]\n",
    "\n",
    "    return losses[\"out\"] + 0.5 * losses[\"aux\"]\n",
    "\n",
    "def train_one_epoch_distill(model, criterion, optimizer, data_loader, lr_scheduler, device, epoch, print_freq, scaler=None, t_model=None):\n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "    for image, target in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        with torch.no_grad():\n",
    "            t_output = t_model(image)\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            d_loss = criterion_distill(output, t_output)\n",
    "            loss = loss + d_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=loss.item(), lr=optimizer.param_groups[0][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f73c1186-f9d4-4c26-91dd-9770811118aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr = args.lr * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a77aada-bd52-41ea-86e4-09315a8743f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=7.54s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d.chudakov/miniconda3/envs/road/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1708025829503/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/d.chudakov/miniconda3/envs/road/lib/python3.8/site-packages/torch/ao/quantization/fake_quantize.py:353: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025829503/work/aten/src/ATen/native/ReduceAllOps.cpp:72.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n",
      "/home/d.chudakov/miniconda3/envs/road/lib/python3.8/site-packages/torch/ao/quantization/fake_quantize.py:353: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025829503/work/aten/src/ATen/native/TensorCompare.cpp:677.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/2891]  eta: 4:18:57  lr: 9.996886836501132e-05  loss: 4.6854 (4.6854)  time: 5.3745  data: 2.7601  max mem: 20709\n",
      "Epoch: [0]  [  10/2891]  eta: 0:48:45  lr: 9.96574926992653e-05  loss: 4.8821 (5.1211)  time: 1.0156  data: 0.2551  max mem: 20740\n",
      "Epoch: [0]  [  20/2891]  eta: 0:38:26  lr: 9.934600889796806e-05  loss: 4.8799 (4.8753)  time: 0.5749  data: 0.0039  max mem: 20740\n",
      "Epoch: [0]  [  30/2891]  eta: 0:34:49  lr: 9.903441654658893e-05  loss: 4.4608 (4.7101)  time: 0.5732  data: 0.0035  max mem: 20740\n",
      "Epoch: [0]  [  40/2891]  eta: 0:32:55  lr: 9.87227152275529e-05  loss: 3.9931 (4.4981)  time: 0.5769  data: 0.0036  max mem: 20740\n",
      "Epoch: [0]  [  50/2891]  eta: 0:31:41  lr: 9.841090452020753e-05  loss: 3.9512 (4.4092)  time: 0.5750  data: 0.0038  max mem: 20740\n",
      "Epoch: [0]  [  60/2891]  eta: 0:30:47  lr: 9.809898400078932e-05  loss: 3.9656 (4.2992)  time: 0.5698  data: 0.0041  max mem: 20740\n",
      "Epoch: [0]  [  70/2891]  eta: 0:30:05  lr: 9.778695324238973e-05  loss: 3.6022 (4.2344)  time: 0.5653  data: 0.0040  max mem: 20740\n",
      "Epoch: [0]  [  80/2891]  eta: 0:29:32  lr: 9.747481181492043e-05  loss: 3.5059 (4.1485)  time: 0.5637  data: 0.0037  max mem: 20740\n",
      "Epoch: [0]  [  90/2891]  eta: 0:29:05  lr: 9.716255928507848e-05  loss: 3.5975 (4.1116)  time: 0.5638  data: 0.0036  max mem: 20740\n",
      "Epoch: [0]  [ 100/2891]  eta: 0:28:44  lr: 9.68501952163104e-05  loss: 3.8482 (4.0871)  time: 0.5660  data: 0.0038  max mem: 20740\n",
      "Epoch: [0]  [ 110/2891]  eta: 0:28:25  lr: 9.65377191687763e-05  loss: 3.7961 (4.0596)  time: 0.5670  data: 0.0042  max mem: 20740\n",
      "Epoch: [0]  [ 120/2891]  eta: 0:28:08  lr: 9.622513069931312e-05  loss: 3.7125 (4.0287)  time: 0.5676  data: 0.0037  max mem: 20740\n",
      "Epoch: [0]  [ 130/2891]  eta: 0:27:53  lr: 9.591242936139748e-05  loss: 3.6579 (3.9991)  time: 0.5663  data: 0.0034  max mem: 20740\n",
      "Epoch: [0]  [ 140/2891]  eta: 0:27:38  lr: 9.559961470510785e-05  loss: 3.4452 (3.9570)  time: 0.5616  data: 0.0039  max mem: 20740\n",
      "Epoch: [0]  [ 150/2891]  eta: 0:27:24  lr: 9.528668627708636e-05  loss: 3.4645 (3.9281)  time: 0.5613  data: 0.0037  max mem: 20740\n",
      "Epoch: [0]  [ 160/2891]  eta: 0:27:12  lr: 9.497364362049972e-05  loss: 3.4504 (3.9006)  time: 0.5627  data: 0.0032  max mem: 20740\n",
      "Epoch: [0]  [ 170/2891]  eta: 0:27:00  lr: 9.466048627500001e-05  loss: 3.4348 (3.8782)  time: 0.5622  data: 0.0034  max mem: 20740\n",
      "Epoch: [0]  [ 180/2891]  eta: 0:26:49  lr: 9.434721377668437e-05  loss: 3.3657 (3.8469)  time: 0.5621  data: 0.0035  max mem: 20740\n",
      "Epoch: [0]  [ 190/2891]  eta: 0:26:40  lr: 9.403382565805451e-05  loss: 3.3478 (3.8304)  time: 0.5664  data: 0.0033  max mem: 20740\n",
      "Epoch: [0]  [ 200/2891]  eta: 0:26:32  lr: 9.37203214479753e-05  loss: 3.5207 (3.8153)  time: 0.5741  data: 0.0036  max mem: 20740\n",
      "Epoch: [0]  [ 210/2891]  eta: 0:26:24  lr: 9.340670067163293e-05  loss: 3.3950 (3.7961)  time: 0.5776  data: 0.0038  max mem: 20740\n",
      "Epoch: [0]  [ 220/2891]  eta: 0:26:17  lr: 9.309296285049231e-05  loss: 3.2676 (3.7702)  time: 0.5796  data: 0.0039  max mem: 20740\n",
      "Epoch: [0]  [ 230/2891]  eta: 0:26:10  lr: 9.277910750225378e-05  loss: 3.1966 (3.7537)  time: 0.5787  data: 0.0045  max mem: 20740\n",
      "Epoch: [0]  [ 240/2891]  eta: 0:26:02  lr: 9.246513414080938e-05  loss: 3.2657 (3.7383)  time: 0.5741  data: 0.0044  max mem: 20740\n",
      "Epoch: [0]  [ 250/2891]  eta: 0:25:54  lr: 9.215104227619828e-05  loss: 3.3236 (3.7174)  time: 0.5694  data: 0.0039  max mem: 20740\n",
      "Epoch: [0]  [ 260/2891]  eta: 0:25:45  lr: 9.183683141456136e-05  loss: 3.2125 (3.7004)  time: 0.5628  data: 0.0038  max mem: 20740\n",
      "Epoch: [0]  [ 270/2891]  eta: 0:25:36  lr: 9.152250105809535e-05  loss: 3.2886 (3.6863)  time: 0.5592  data: 0.0037  max mem: 20740\n",
      "Epoch: [0]  [ 280/2891]  eta: 0:25:28  lr: 9.120805070500629e-05  loss: 3.2886 (3.6733)  time: 0.5609  data: 0.0036  max mem: 20740\n",
      "Epoch: [0]  [ 290/2891]  eta: 0:25:21  lr: 9.089347984946188e-05  loss: 3.1989 (3.6558)  time: 0.5666  data: 0.0035  max mem: 20740\n",
      "Epoch: [0]  [ 300/2891]  eta: 0:25:13  lr: 9.057878798154349e-05  loss: 3.2454 (3.6431)  time: 0.5675  data: 0.0035  max mem: 20740\n",
      "Epoch: [0]  [ 310/2891]  eta: 0:25:05  lr: 9.026397458719719e-05  loss: 3.3282 (3.6381)  time: 0.5621  data: 0.0037  max mem: 20740\n",
      "Epoch: [0]  [ 320/2891]  eta: 0:24:58  lr: 8.994903914818402e-05  loss: 3.2452 (3.6242)  time: 0.5616  data: 0.0035  max mem: 20740\n",
      "Epoch: [0]  [ 330/2891]  eta: 0:24:51  lr: 8.963398114202948e-05  loss: 3.2270 (3.6113)  time: 0.5634  data: 0.0031  max mem: 20740\n",
      "Epoch: [0]  [ 340/2891]  eta: 0:24:44  lr: 8.93188000419724e-05  loss: 3.2229 (3.6011)  time: 0.5692  data: 0.0034  max mem: 20740\n",
      "Epoch: [0]  [ 350/2891]  eta: 0:24:37  lr: 8.900349531691258e-05  loss: 3.2221 (3.5869)  time: 0.5707  data: 0.0038  max mem: 20740\n",
      "Epoch: [0]  [ 360/2891]  eta: 0:24:31  lr: 8.868806643135802e-05  loss: 3.1104 (3.5757)  time: 0.5678  data: 0.0036  max mem: 20740\n",
      "Epoch: [0]  [ 370/2891]  eta: 0:24:24  lr: 8.837251284537095e-05  loss: 3.0956 (3.5655)  time: 0.5678  data: 0.0037  max mem: 20740\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mstart_epoch, args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mtrain_one_epoch_distill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqat_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     confmat \u001b[38;5;241m=\u001b[39m evaluate(qat_model, data_loader_test, device\u001b[38;5;241m=\u001b[39mdevice, num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(confmat)\n",
      "Cell \u001b[0;32mIn[11], line 34\u001b[0m, in \u001b[0;36mtrain_one_epoch_distill\u001b[0;34m(model, criterion, optimizer, data_loader, lr_scheduler, device, epoch, print_freq, scaler, t_model)\u001b[0m\n\u001b[1;32m     32\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     37\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/road/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/road/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if args.output_dir:\n",
    "    utils.mkdir(args.output_dir)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "dataset, num_classes = get_dataset(args, is_train=True)\n",
    "dataset_test, _ = get_dataset(args, is_train=False)\n",
    "\n",
    "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=args.workers,\n",
    "    collate_fn=utils.collate_fn,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, sampler=test_sampler, num_workers=args.workers, collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "params_to_optimize = [\n",
    "    {\"params\": [p for p in qat_model.backbone.parameters() if p.requires_grad]},\n",
    "    {\"params\": [p for p in qat_model.classifier.parameters() if p.requires_grad]},\n",
    "]\n",
    "if args.aux_loss:\n",
    "    params = [p for p in qat_model.aux_classifier.parameters() if p.requires_grad]\n",
    "    params_to_optimize.append({\"params\": params, \"lr\": args.lr})\n",
    "    \n",
    "optimizer = torch.optim.SGD(params_to_optimize, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler() if args.amp else None\n",
    "\n",
    "iters_per_epoch = len(data_loader)\n",
    "main_lr_scheduler = PolynomialLR(\n",
    "    optimizer, total_iters=iters_per_epoch * (args.epochs - args.lr_warmup_epochs), power=0.9\n",
    ")\n",
    "\n",
    "if args.lr_warmup_epochs > 0:\n",
    "    warmup_iters = iters_per_epoch * args.lr_warmup_epochs\n",
    "    args.lr_warmup_method = args.lr_warmup_method.lower()\n",
    "    if args.lr_warmup_method == \"linear\":\n",
    "        warmup_lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=args.lr_warmup_decay, total_iters=warmup_iters\n",
    "        )\n",
    "    elif args.lr_warmup_method == \"constant\":\n",
    "        warmup_lr_scheduler = torch.optim.lr_scheduler.ConstantLR(\n",
    "            optimizer, factor=args.lr_warmup_decay, total_iters=warmup_iters\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f\"Invalid warmup lr method '{args.lr_warmup_method}'. Only linear and constant are supported.\"\n",
    "        )\n",
    "    lr_scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "        optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[warmup_iters]\n",
    "    )\n",
    "else:\n",
    "    lr_scheduler = main_lr_scheduler\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    train_one_epoch_distill(qat_model, criterion, optimizer, data_loader, lr_scheduler, device, epoch, args.print_freq, scaler, model)\n",
    "    confmat = evaluate(qat_model, data_loader_test, device=device, num_classes=num_classes)\n",
    "    print(confmat)\n",
    "    checkpoint = {\n",
    "        \"model\": qat_model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"args\": args,\n",
    "    }\n",
    "    if args.amp:\n",
    "        checkpoint[\"scaler\"] = scaler.state_dict()\n",
    "    utils.save_on_master(checkpoint, os.path.join(args.output_dir, f\"model_{epoch}.pth\"))\n",
    "    utils.save_on_master(checkpoint, os.path.join(args.output_dir, \"checkpoint.pth\"))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print(f\"Training time {total_time_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3425518-6eaf-413a-84f1-61a13dc915ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
