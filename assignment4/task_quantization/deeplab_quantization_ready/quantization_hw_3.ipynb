{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Балуемся с дистилляцией\n",
    "Врываемся в train.py и добавляем туда дистилляцию, просто по последнему слою (до софтмакса, на логитах) делаем стягивание по MSE\n",
    "\n",
    "Цель поднять точность и ускорить сходимость.\n",
    "\n",
    "Балуемся с весами обычного и distill лосса.\n",
    "\n",
    "Можно вообще выкинуть classification loss и смоделировать ситуацию когда вам не выдали лейблов (жиза)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvasserm/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "from torch.ao.quantization.quantize_fx import convert_fx\n",
    "from torch.ao.quantization.quantize_fx import fuse_fx\n",
    "from torch.optim.lr_scheduler import PolynomialLR\n",
    "from torchvision.models.segmentation import DeepLabV3_MobileNet_V3_Large_Weights, deeplabv3_mobilenet_v3_large\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "from quantization_utils.fake_quantization import fake_quantization\n",
    "from quantization_utils.static_quantization import quantize_static\n",
    "from train import evaluate\n",
    "from train import get_dataset\n",
    "from train import train_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(inputs, target):\n",
    "    losses = {}\n",
    "    for name, x in inputs.items():\n",
    "        losses[name] = nn.functional.cross_entropy(x, target, ignore_index=255)\n",
    "\n",
    "    if len(losses) == 1:\n",
    "        return losses[\"out\"]\n",
    "\n",
    "    return losses[\"out\"] + 0.5 * losses[\"aux\"]\n",
    "\n",
    "def train_one_epoch(student_model, teacher_model, criterion, optimizer, data_loader, lr_scheduler, device, epoch, print_freq, scaler=None):\n",
    "    base_k = 0.5\n",
    "    KD_k = 0.5\n",
    "\n",
    "    student_model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "    for image, target in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            student_out = student_model(image)\n",
    "            if teacher_model is not None:\n",
    "                teacher_out = teacher_model(image)\n",
    "                #KD with last layer logits\n",
    "                KDloss1 = nn.functional.mse_loss(student_out['out'], teacher_out['out'])\n",
    "                #Lets use also auxilary loss in KD\n",
    "                KDloss2 = nn.functional.mse_loss(student_out['aux'], teacher_out['aux'])\n",
    "                KDloss = KDloss1 + 0.5*KDloss2\n",
    "            else:\n",
    "                KDloss = 0\n",
    "            loss = base_k*criterion(student_out, target) + KD_k*KDloss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=loss.item(), lr=optimizer.param_groups[0][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(student_model, teacher_model, args):\n",
    "\n",
    "    if args.output_dir:\n",
    "        utils.mkdir(args.output_dir)\n",
    "\n",
    "    utils.init_distributed_mode(args)\n",
    "\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    #torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "    dataset, num_classes = get_dataset(args, is_train=True)\n",
    "    dataset_test, _ = get_dataset(args, is_train=False)\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=args.workers,\n",
    "        collate_fn=utils.collate_fn,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, \n",
    "        batch_size=1, \n",
    "        sampler=test_sampler, \n",
    "        num_workers=args.workers, \n",
    "        collate_fn=utils.collate_fn\n",
    "    )\n",
    "\n",
    "    student_model.to(device)\n",
    "    if teacher_model is not None:\n",
    "        teacher_model.to(device)\n",
    "        teacher_model.eval()\n",
    "\n",
    "    model_without_ddp = student_model\n",
    "\n",
    "    params_to_optimize = [\n",
    "        {\"params\": [p for p in model_without_ddp.backbone.parameters() if p.requires_grad]},\n",
    "        {\"params\": [p for p in model_without_ddp.classifier.parameters() if p.requires_grad]},\n",
    "    ]\n",
    "    if args.aux_loss:\n",
    "        params = [p for p in model_without_ddp.aux_classifier.parameters() if p.requires_grad]\n",
    "        params_to_optimize.append({\"params\": params, \"lr\": args.lr * 10})\n",
    "    \n",
    "    optimizer = torch.optim.SGD(params_to_optimize, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler() if args.amp else None\n",
    "\n",
    "    iters_per_epoch = len(data_loader)\n",
    "    main_lr_scheduler = PolynomialLR(\n",
    "        optimizer, total_iters=iters_per_epoch * (args.epochs - args.lr_warmup_epochs), power=0.9\n",
    "    )\n",
    "\n",
    "    if args.lr_warmup_epochs > 0:\n",
    "        warmup_iters = iters_per_epoch * args.lr_warmup_epochs\n",
    "        args.lr_warmup_method = args.lr_warmup_method.lower()\n",
    "        if args.lr_warmup_method == \"linear\":\n",
    "            warmup_lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "                optimizer, start_factor=args.lr_warmup_decay, total_iters=warmup_iters\n",
    "            )\n",
    "        elif args.lr_warmup_method == \"constant\":\n",
    "            warmup_lr_scheduler = torch.optim.lr_scheduler.ConstantLR(\n",
    "                optimizer, factor=args.lr_warmup_decay, total_iters=warmup_iters\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"Invalid warmup lr method '{args.lr_warmup_method}'. Only linear and constant are supported.\"\n",
    "            )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "            optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[warmup_iters]\n",
    "        )\n",
    "    else:\n",
    "        lr_scheduler = main_lr_scheduler\n",
    "\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume, map_location=\"cpu\", weights_only=True)\n",
    "        model_without_ddp.load_state_dict(checkpoint[\"model\"], strict=not args.test_only)\n",
    "        if not args.test_only:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "            lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
    "            args.start_epoch = checkpoint[\"epoch\"] + 1\n",
    "            if args.amp:\n",
    "                scaler.load_state_dict(checkpoint[\"scaler\"])\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        if args.distributed:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "        train_one_epoch(student_model, teacher_model, criterion, optimizer, data_loader, lr_scheduler, device, epoch, args.print_freq, scaler)\n",
    "        confmat = evaluate(student_model, data_loader_test, device=device, num_classes=num_classes)\n",
    "        print(confmat)\n",
    "        checkpoint = {\n",
    "            \"model\": model_without_ddp.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"args\": args,\n",
    "        }\n",
    "        if args.amp:\n",
    "            checkpoint[\"scaler\"] = scaler.state_dict()\n",
    "        utils.save_on_master(checkpoint, os.path.join(args.output_dir, f\"model_{epoch}.pth\"))\n",
    "        utils.save_on_master(checkpoint, os.path.join(args.output_dir, \"checkpoint.pth\"))\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print(f\"Training time {total_time_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 16899571712, Reserved: 0, Allocated: 0, Free: 0\n",
      "Namespace(data_path='/home/gvasserm/data/coco2017/', dataset='coco', model='deeplabv3_mobilenet_v3_large', aux_loss=False, device='cuda', batch_size=16, epochs=1, workers=8, lr=0.01, momentum=0.9, weight_decay=0.0001, lr_warmup_epochs=0, lr_warmup_method='linear', lr_warmup_decay=0.01, print_freq=10, output_dir='.', resume='', start_epoch=0, test_only=False, use_deterministic_algorithms=False, world_size=1, dist_url='env://', weights=None, weights_backbone=None, amp=False, backend='pil', use_v2=False)\n",
      "Not using distributed mode\n",
      "loading annotations into memory...\n",
      "Done (t=0.27s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=7.98s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvasserm/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "loading annotations into memory...\n",
      "Done (t=9.13s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvasserm/.local/lib/python3.10/site-packages/torch/ao/quantization/fake_quantize.py:353: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/ReduceAllOps.cpp:72.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n",
      "/home/gvasserm/.local/lib/python3.10/site-packages/torch/ao/quantization/fake_quantize.py:353: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:677.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/5782]  eta: 4:46:34  lr: 0.009998443431713478  loss: 2.6787 (2.6787)  time: 2.9739  data: 1.1832  max mem: 14888\n",
      "Epoch: [0]  [  10/5782]  eta: 2:00:54  lr: 0.009982876267081917  loss: 2.6048 (2.6146)  time: 1.2568  data: 0.1119  max mem: 14987\n",
      "Epoch: [0]  [  20/5782]  eta: 1:53:10  lr: 0.009967306404733907  loss: 2.4871 (2.5841)  time: 1.0888  data: 0.0048  max mem: 14987\n",
      "Epoch: [0]  [  30/5782]  eta: 1:50:21  lr: 0.009951733839518005  loss: 2.5164 (2.5239)  time: 1.0932  data: 0.0051  max mem: 14987\n",
      "Epoch: [0]  [  40/5782]  eta: 1:48:54  lr: 0.009936158566263944  loss: 2.2878 (2.4690)  time: 1.0954  data: 0.0053  max mem: 14987\n",
      "Epoch: [0]  [  50/5782]  eta: 1:47:56  lr: 0.00992058057978255  loss: 2.2419 (2.4135)  time: 1.0966  data: 0.0053  max mem: 14987\n",
      "Epoch: [0]  [  60/5782]  eta: 1:47:15  lr: 0.009904999874865638  loss: 2.0757 (2.3853)  time: 1.0976  data: 0.0053  max mem: 14987\n",
      "Epoch: [0]  [  70/5782]  eta: 1:46:44  lr: 0.0098894164462859  loss: 2.0396 (2.3273)  time: 1.0993  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [  80/5782]  eta: 1:46:18  lr: 0.0098738302887968  loss: 1.8776 (2.2873)  time: 1.0999  data: 0.0053  max mem: 14987\n",
      "Epoch: [0]  [  90/5782]  eta: 1:45:57  lr: 0.009858241397132478  loss: 1.8984 (2.2590)  time: 1.1013  data: 0.0054  max mem: 14987\n",
      "Epoch: [0]  [ 100/5782]  eta: 1:45:38  lr: 0.009842649766007645  loss: 2.0156 (2.2376)  time: 1.1030  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [ 110/5782]  eta: 1:45:22  lr: 0.009827055390117464  loss: 2.0389 (2.2227)  time: 1.1045  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 120/5782]  eta: 1:45:07  lr: 0.009811458264137459  loss: 1.9836 (2.1962)  time: 1.1063  data: 0.0051  max mem: 14987\n",
      "Epoch: [0]  [ 130/5782]  eta: 1:44:53  lr: 0.009795858382723403  loss: 1.8284 (2.1704)  time: 1.1075  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [ 140/5782]  eta: 1:44:41  lr: 0.009780255740511199  loss: 1.8371 (2.1568)  time: 1.1092  data: 0.0053  max mem: 14987\n",
      "Epoch: [0]  [ 150/5782]  eta: 1:44:29  lr: 0.009764650332116789  loss: 1.9413 (2.1493)  time: 1.1110  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [ 160/5782]  eta: 1:44:17  lr: 0.009749042152136027  loss: 1.9413 (2.1344)  time: 1.1117  data: 0.0051  max mem: 14987\n",
      "Epoch: [0]  [ 170/5782]  eta: 1:44:06  lr: 0.009733431195144573  loss: 1.8483 (2.1250)  time: 1.1118  data: 0.0051  max mem: 14987\n",
      "Epoch: [0]  [ 180/5782]  eta: 1:43:55  lr: 0.009717817455697792  loss: 1.7891 (2.1108)  time: 1.1122  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [ 190/5782]  eta: 1:43:44  lr: 0.009702200928330627  loss: 1.8052 (2.0980)  time: 1.1131  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [ 200/5782]  eta: 1:43:33  lr: 0.009686581607557488  loss: 1.8221 (2.0848)  time: 1.1143  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [ 210/5782]  eta: 1:43:23  lr: 0.009670959487872154  loss: 1.9352 (2.0782)  time: 1.1153  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [ 220/5782]  eta: 1:43:12  lr: 0.009655334563747642  loss: 1.9413 (2.0703)  time: 1.1155  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 230/5782]  eta: 1:43:01  lr: 0.009639706829636096  loss: 1.9337 (2.0656)  time: 1.1146  data: 0.0051  max mem: 14987\n",
      "Epoch: [0]  [ 240/5782]  eta: 1:42:50  lr: 0.009624076279968656  loss: 1.8818 (2.0582)  time: 1.1151  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [ 250/5782]  eta: 1:42:40  lr: 0.009608442909155378  loss: 1.7193 (2.0493)  time: 1.1154  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [ 260/5782]  eta: 1:42:29  lr: 0.009592806711585078  loss: 1.7514 (2.0423)  time: 1.1155  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [ 270/5782]  eta: 1:42:18  lr: 0.00957716768162523  loss: 1.8775 (2.0369)  time: 1.1164  data: 0.0053  max mem: 14987\n",
      "Epoch: [0]  [ 280/5782]  eta: 1:42:08  lr: 0.00956152581362184  loss: 1.8519 (2.0295)  time: 1.1166  data: 0.0053  max mem: 14987\n",
      "Epoch: [0]  [ 290/5782]  eta: 1:41:57  lr: 0.00954588110189933  loss: 1.7450 (2.0185)  time: 1.1169  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [ 300/5782]  eta: 1:41:47  lr: 0.009530233540760416  loss: 1.7303 (2.0097)  time: 1.1182  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [ 310/5782]  eta: 1:41:32  lr: 0.00951458312448599  loss: 1.7518 (2.0039)  time: 1.1042  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [ 320/5782]  eta: 1:41:16  lr: 0.009498929847334968  loss: 1.7764 (1.9962)  time: 1.0867  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 330/5782]  eta: 1:41:00  lr: 0.009483273703544211  loss: 1.7679 (1.9902)  time: 1.0849  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 340/5782]  eta: 1:40:45  lr: 0.009467614687328358  loss: 1.8645 (1.9898)  time: 1.0851  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 350/5782]  eta: 1:40:29  lr: 0.009451952792879727  loss: 1.8645 (1.9860)  time: 1.0838  data: 0.0048  max mem: 14987\n",
      "Epoch: [0]  [ 360/5782]  eta: 1:40:14  lr: 0.009436288014368178  loss: 1.8507 (1.9841)  time: 1.0830  data: 0.0048  max mem: 14987\n",
      "Epoch: [0]  [ 370/5782]  eta: 1:39:59  lr: 0.009420620345940975  loss: 1.8507 (1.9805)  time: 1.0822  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 380/5782]  eta: 1:39:44  lr: 0.009404949781722674  loss: 1.7250 (1.9770)  time: 1.0804  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 390/5782]  eta: 1:39:29  lr: 0.009389276315814973  loss: 1.7351 (1.9732)  time: 1.0796  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 400/5782]  eta: 1:39:14  lr: 0.009373599942296609  loss: 1.7753 (1.9684)  time: 1.0795  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 410/5782]  eta: 1:38:59  lr: 0.009357920655223196  loss: 1.7856 (1.9675)  time: 1.0795  data: 0.0048  max mem: 14987\n",
      "Epoch: [0]  [ 420/5782]  eta: 1:38:45  lr: 0.009342238448627102  loss: 1.6523 (1.9600)  time: 1.0796  data: 0.0048  max mem: 14987\n",
      "Epoch: [0]  [ 430/5782]  eta: 1:38:31  lr: 0.009326553316517324  loss: 1.6941 (1.9574)  time: 1.0794  data: 0.0048  max mem: 14987\n",
      "Epoch: [0]  [ 440/5782]  eta: 1:38:17  lr: 0.009310865252879342  loss: 1.8009 (1.9559)  time: 1.0794  data: 0.0048  max mem: 14987\n",
      "Epoch: [0]  [ 450/5782]  eta: 1:38:03  lr: 0.00929517425167499  loss: 1.6481 (1.9489)  time: 1.0795  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 460/5782]  eta: 1:37:49  lr: 0.009279480306842305  loss: 1.6362 (1.9442)  time: 1.0796  data: 0.0048  max mem: 14987\n",
      "Epoch: [0]  [ 470/5782]  eta: 1:37:35  lr: 0.0092637834122954  loss: 1.7055 (1.9404)  time: 1.0797  data: 0.0048  max mem: 14987\n",
      "Epoch: [0]  [ 480/5782]  eta: 1:37:22  lr: 0.00924808356192433  loss: 1.6598 (1.9354)  time: 1.0791  data: 0.0048  max mem: 14987\n",
      "Epoch: [0]  [ 490/5782]  eta: 1:37:08  lr: 0.009232380749594933  loss: 1.7634 (1.9331)  time: 1.0787  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 500/5782]  eta: 1:36:55  lr: 0.009216674969148707  loss: 1.6533 (1.9275)  time: 1.0787  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 510/5782]  eta: 1:36:41  lr: 0.00920096621440265  loss: 1.6141 (1.9233)  time: 1.0789  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 520/5782]  eta: 1:36:29  lr: 0.009185254479149127  loss: 1.6970 (1.9200)  time: 1.0832  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 530/5782]  eta: 1:36:17  lr: 0.009169539757155723  loss: 1.6541 (1.9145)  time: 1.0899  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 540/5782]  eta: 1:36:07  lr: 0.009153822042165093  loss: 1.5790 (1.9092)  time: 1.0992  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 550/5782]  eta: 1:35:57  lr: 0.009138101327894823  loss: 1.6121 (1.9061)  time: 1.1091  data: 0.0051  max mem: 14987\n",
      "Epoch: [0]  [ 560/5782]  eta: 1:35:48  lr: 0.009122377608037273  loss: 1.7908 (1.9048)  time: 1.1146  data: 0.0051  max mem: 14987\n",
      "Epoch: [0]  [ 570/5782]  eta: 1:35:38  lr: 0.009106650876259429  loss: 1.7908 (1.9024)  time: 1.1184  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 580/5782]  eta: 1:35:29  lr: 0.00909092112620275  loss: 1.7390 (1.8997)  time: 1.1194  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 590/5782]  eta: 1:35:19  lr: 0.009075188351483023  loss: 1.7321 (1.8976)  time: 1.1183  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 600/5782]  eta: 1:35:10  lr: 0.0090594525456902  loss: 1.6431 (1.8938)  time: 1.1173  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 610/5782]  eta: 1:35:00  lr: 0.009043713702388257  loss: 1.6431 (1.8908)  time: 1.1162  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 620/5782]  eta: 1:34:50  lr: 0.009027971815115025  loss: 1.5957 (1.8868)  time: 1.1145  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 630/5782]  eta: 1:34:39  lr: 0.00901222687738204  loss: 1.6211 (1.8843)  time: 1.1126  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 640/5782]  eta: 1:34:29  lr: 0.008996478882674375  loss: 1.6067 (1.8792)  time: 1.1118  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 650/5782]  eta: 1:34:19  lr: 0.0089807278244505  loss: 1.5763 (1.8769)  time: 1.1121  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 660/5782]  eta: 1:34:08  lr: 0.008964973696142102  loss: 1.6517 (1.8742)  time: 1.1105  data: 0.0051  max mem: 14987\n",
      "Epoch: [0]  [ 670/5782]  eta: 1:33:58  lr: 0.008949216491153933  loss: 1.6339 (1.8727)  time: 1.1092  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 680/5782]  eta: 1:33:47  lr: 0.008933456202863642  loss: 1.6797 (1.8707)  time: 1.1091  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 690/5782]  eta: 1:33:37  lr: 0.008917692824621614  loss: 1.6831 (1.8677)  time: 1.1087  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 700/5782]  eta: 1:33:26  lr: 0.008901926349750797  loss: 1.6831 (1.8665)  time: 1.1082  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 710/5782]  eta: 1:33:15  lr: 0.008886156771546548  loss: 1.7127 (1.8644)  time: 1.1080  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 720/5782]  eta: 1:33:05  lr: 0.008870384083276452  loss: 1.7127 (1.8631)  time: 1.1085  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 730/5782]  eta: 1:32:54  lr: 0.00885460827818015  loss: 1.6641 (1.8601)  time: 1.1096  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 740/5782]  eta: 1:32:44  lr: 0.008838829349469184  loss: 1.5586 (1.8578)  time: 1.1109  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 750/5782]  eta: 1:32:33  lr: 0.008823047290326805  loss: 1.6842 (1.8569)  time: 1.1126  data: 0.0049  max mem: 14987\n",
      "Epoch: [0]  [ 760/5782]  eta: 1:32:23  lr: 0.008807262093907817  loss: 1.6686 (1.8537)  time: 1.1137  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 770/5782]  eta: 1:32:13  lr: 0.00879147375333838  loss: 1.5982 (1.8511)  time: 1.1151  data: 0.0050  max mem: 14987\n",
      "Epoch: [0]  [ 780/5782]  eta: 1:32:03  lr: 0.008775682261715849  loss: 1.6361 (1.8491)  time: 1.1174  data: 0.0051  max mem: 14987\n",
      "Epoch: [0]  [ 790/5782]  eta: 1:31:53  lr: 0.008759887612108594  loss: 1.6283 (1.8488)  time: 1.1200  data: 0.0054  max mem: 14987\n",
      "Epoch: [0]  [ 800/5782]  eta: 1:31:44  lr: 0.008744089797555808  loss: 1.8175 (1.8490)  time: 1.1352  data: 0.0053  max mem: 14987\n",
      "Epoch: [0]  [ 810/5782]  eta: 1:31:34  lr: 0.008728288811067344  loss: 1.8904 (1.8499)  time: 1.1355  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [ 820/5782]  eta: 1:31:24  lr: 0.00871248464562352  loss: 1.7698 (1.8486)  time: 1.1218  data: 0.0051  max mem: 14987\n",
      "Epoch: [0]  [ 830/5782]  eta: 1:31:14  lr: 0.008696677294174936  loss: 1.7607 (1.8473)  time: 1.1220  data: 0.0053  max mem: 14987\n",
      "Epoch: [0]  [ 840/5782]  eta: 1:31:04  lr: 0.008680866749642276  loss: 1.7420 (1.8473)  time: 1.1226  data: 0.0057  max mem: 14987\n",
      "Epoch: [0]  [ 850/5782]  eta: 1:30:53  lr: 0.008665053004916157  loss: 1.7082 (1.8460)  time: 1.1154  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [ 860/5782]  eta: 1:30:42  lr: 0.008649236052856897  loss: 1.7082 (1.8447)  time: 1.1056  data: 0.0055  max mem: 14987\n",
      "Epoch: [0]  [ 870/5782]  eta: 1:30:31  lr: 0.008633415886294345  loss: 1.6892 (1.8443)  time: 1.1059  data: 0.0056  max mem: 14987\n",
      "Epoch: [0]  [ 880/5782]  eta: 1:30:21  lr: 0.008617592498027694  loss: 1.6283 (1.8427)  time: 1.1118  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [ 890/5782]  eta: 1:30:10  lr: 0.008601765880825271  loss: 1.6140 (1.8400)  time: 1.1165  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [ 900/5782]  eta: 1:29:59  lr: 0.008585936027424353  loss: 1.6446 (1.8394)  time: 1.1141  data: 0.0057  max mem: 14987\n",
      "Epoch: [0]  [ 910/5782]  eta: 1:29:48  lr: 0.008570102930530977  loss: 1.6406 (1.8370)  time: 1.1041  data: 0.0056  max mem: 14987\n",
      "Epoch: [0]  [ 920/5782]  eta: 1:29:36  lr: 0.008554266582819709  loss: 1.5278 (1.8351)  time: 1.0950  data: 0.0054  max mem: 14987\n",
      "Epoch: [0]  [ 930/5782]  eta: 1:29:24  lr: 0.008538426976933485  loss: 1.6157 (1.8335)  time: 1.0925  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [ 940/5782]  eta: 1:29:12  lr: 0.008522584105483379  loss: 1.6309 (1.8310)  time: 1.0917  data: 0.0053  max mem: 14987\n",
      "Epoch: [0]  [ 950/5782]  eta: 1:29:01  lr: 0.008506737961048411  loss: 1.6454 (1.8292)  time: 1.0905  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [ 960/5782]  eta: 1:28:49  lr: 0.008490888536175337  loss: 1.6454 (1.8268)  time: 1.0902  data: 0.0054  max mem: 14987\n",
      "Epoch: [0]  [ 970/5782]  eta: 1:28:37  lr: 0.00847503582337845  loss: 1.5875 (1.8256)  time: 1.0889  data: 0.0056  max mem: 14987\n",
      "Epoch: [0]  [ 980/5782]  eta: 1:28:25  lr: 0.008459179815139354  loss: 1.6316 (1.8249)  time: 1.0877  data: 0.0055  max mem: 14987\n",
      "Epoch: [0]  [ 990/5782]  eta: 1:28:13  lr: 0.008443320503906776  loss: 1.6301 (1.8232)  time: 1.0878  data: 0.0056  max mem: 14987\n",
      "Epoch: [0]  [1000/5782]  eta: 1:28:01  lr: 0.008427457882096331  loss: 1.6347 (1.8226)  time: 1.0869  data: 0.0054  max mem: 14987\n",
      "Epoch: [0]  [1010/5782]  eta: 1:27:49  lr: 0.008411591942090318  loss: 1.6813 (1.8219)  time: 1.0860  data: 0.0053  max mem: 14987\n",
      "Epoch: [0]  [1020/5782]  eta: 1:27:37  lr: 0.008395722676237513  loss: 1.6670 (1.8202)  time: 1.0865  data: 0.0053  max mem: 14987\n",
      "Epoch: [0]  [1030/5782]  eta: 1:27:26  lr: 0.008379850076852926  loss: 1.6490 (1.8190)  time: 1.0860  data: 0.0054  max mem: 14987\n",
      "Epoch: [0]  [1040/5782]  eta: 1:27:14  lr: 0.0083639741362176  loss: 1.5450 (1.8169)  time: 1.0856  data: 0.0054  max mem: 14987\n",
      "Epoch: [0]  [1050/5782]  eta: 1:27:02  lr: 0.008348094846578386  loss: 1.5041 (1.8148)  time: 1.0858  data: 0.0054  max mem: 14987\n",
      "Epoch: [0]  [1060/5782]  eta: 1:26:50  lr: 0.00833221220014771  loss: 1.5268 (1.8134)  time: 1.0856  data: 0.0054  max mem: 14987\n",
      "Epoch: [0]  [1070/5782]  eta: 1:26:38  lr: 0.00831632618910336  loss: 1.5691 (1.8108)  time: 1.0871  data: 0.0055  max mem: 14987\n",
      "Epoch: [0]  [1080/5782]  eta: 1:26:26  lr: 0.008300436805588254  loss: 1.5691 (1.8094)  time: 1.0870  data: 0.0056  max mem: 14987\n",
      "Epoch: [0]  [1090/5782]  eta: 1:26:15  lr: 0.008284544041710199  loss: 1.5208 (1.8076)  time: 1.0919  data: 0.0057  max mem: 14987\n",
      "Epoch: [0]  [1100/5782]  eta: 1:26:05  lr: 0.008268647889541673  loss: 1.6117 (1.8065)  time: 1.1052  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [1110/5782]  eta: 1:25:54  lr: 0.008252748341119581  loss: 1.6139 (1.8050)  time: 1.1129  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [1120/5782]  eta: 1:25:44  lr: 0.00823684538844503  loss: 1.5406 (1.8032)  time: 1.1159  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [1130/5782]  eta: 1:25:33  lr: 0.008220939023483078  loss: 1.5096 (1.8014)  time: 1.1172  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [1140/5782]  eta: 1:25:22  lr: 0.008205029238162503  loss: 1.5148 (1.7992)  time: 1.1151  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [1150/5782]  eta: 1:25:12  lr: 0.008189116024375562  loss: 1.4975 (1.7959)  time: 1.1146  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [1160/5782]  eta: 1:25:01  lr: 0.008173199373977735  loss: 1.4776 (1.7942)  time: 1.1152  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [1170/5782]  eta: 1:24:51  lr: 0.008157279278787484  loss: 1.5410 (1.7923)  time: 1.1143  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [1180/5782]  eta: 1:24:40  lr: 0.008141355730586015  loss: 1.4954 (1.7901)  time: 1.1138  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [1190/5782]  eta: 1:24:29  lr: 0.008125428721117003  loss: 1.6050 (1.7906)  time: 1.1133  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [1200/5782]  eta: 1:24:19  lr: 0.008109498242086373  loss: 1.6791 (1.7888)  time: 1.1123  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [1210/5782]  eta: 1:24:08  lr: 0.008093564285162006  loss: 1.4841 (1.7863)  time: 1.1120  data: 0.0057  max mem: 14987\n",
      "Epoch: [0]  [1220/5782]  eta: 1:23:57  lr: 0.008077626841973507  loss: 1.5387 (1.7851)  time: 1.1117  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [1230/5782]  eta: 1:23:46  lr: 0.00806168590411194  loss: 1.5387 (1.7837)  time: 1.1110  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [1240/5782]  eta: 1:23:36  lr: 0.008045741463129558  loss: 1.5234 (1.7817)  time: 1.1117  data: 0.0056  max mem: 14987\n",
      "Epoch: [0]  [1250/5782]  eta: 1:23:25  lr: 0.008029793510539548  loss: 1.6024 (1.7804)  time: 1.1127  data: 0.0055  max mem: 14987\n",
      "Epoch: [0]  [1260/5782]  eta: 1:23:14  lr: 0.008013842037815747  loss: 1.6024 (1.7784)  time: 1.1132  data: 0.0055  max mem: 14987\n",
      "Epoch: [0]  [1270/5782]  eta: 1:23:03  lr: 0.0079978870363924  loss: 1.4777 (1.7760)  time: 1.1131  data: 0.0055  max mem: 14987\n",
      "Epoch: [0]  [1280/5782]  eta: 1:22:53  lr: 0.007981928497663857  loss: 1.5934 (1.7754)  time: 1.1129  data: 0.0055  max mem: 14987\n",
      "Epoch: [0]  [1290/5782]  eta: 1:22:42  lr: 0.007965966412984322  loss: 1.6384 (1.7743)  time: 1.1140  data: 0.0055  max mem: 14987\n",
      "Epoch: [0]  [1300/5782]  eta: 1:22:31  lr: 0.007950000773667556  loss: 1.5654 (1.7732)  time: 1.1142  data: 0.0055  max mem: 14987\n",
      "Epoch: [0]  [1310/5782]  eta: 1:22:20  lr: 0.007934031570986624  loss: 1.5376 (1.7714)  time: 1.1129  data: 0.0055  max mem: 14987\n",
      "Epoch: [0]  [1320/5782]  eta: 1:22:10  lr: 0.007918058796173577  loss: 1.5376 (1.7694)  time: 1.1141  data: 0.0057  max mem: 14987\n",
      "Epoch: [0]  [1330/5782]  eta: 1:21:59  lr: 0.007902082440419204  loss: 1.6373 (1.7696)  time: 1.1170  data: 0.0057  max mem: 14987\n",
      "Epoch: [0]  [1340/5782]  eta: 1:21:48  lr: 0.007886102494872718  loss: 1.6332 (1.7680)  time: 1.1159  data: 0.0055  max mem: 14987\n",
      "Epoch: [0]  [1350/5782]  eta: 1:21:37  lr: 0.007870118950641474  loss: 1.4878 (1.7661)  time: 1.1131  data: 0.0055  max mem: 14987\n",
      "Epoch: [0]  [1360/5782]  eta: 1:21:27  lr: 0.007854131798790683  loss: 1.5243 (1.7654)  time: 1.1126  data: 0.0054  max mem: 14987\n",
      "Epoch: [0]  [1370/5782]  eta: 1:21:16  lr: 0.007838141030343103  loss: 1.6492 (1.7649)  time: 1.1122  data: 0.0053  max mem: 14987\n",
      "Epoch: [0]  [1380/5782]  eta: 1:21:05  lr: 0.007822146636278754  loss: 1.6488 (1.7638)  time: 1.1108  data: 0.0054  max mem: 14987\n",
      "Epoch: [0]  [1390/5782]  eta: 1:20:54  lr: 0.0078061486075346066  loss: 1.6040 (1.7628)  time: 1.1105  data: 0.0057  max mem: 14987\n",
      "Epoch: [0]  [1400/5782]  eta: 1:20:43  lr: 0.007790146935004286  loss: 1.6040 (1.7616)  time: 1.1108  data: 0.0056  max mem: 14987\n",
      "Epoch: [0]  [1410/5782]  eta: 1:20:32  lr: 0.007774141609537757  loss: 1.5482 (1.7604)  time: 1.1106  data: 0.0052  max mem: 14987\n",
      "Epoch: [0]  [1420/5782]  eta: 1:20:21  lr: 0.007758132621941025  loss: 1.5179 (1.7591)  time: 1.1119  data: 0.0053  max mem: 14987\n",
      "Epoch: [0]  [1430/5782]  eta: 1:20:11  lr: 0.007742119962975813  loss: 1.4585 (1.7568)  time: 1.1154  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [1440/5782]  eta: 1:20:00  lr: 0.0077261036233592486  loss: 1.4583 (1.7557)  time: 1.1186  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [1450/5782]  eta: 1:19:50  lr: 0.007710083593763557  loss: 1.5027 (1.7545)  time: 1.1196  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [1460/5782]  eta: 1:19:39  lr: 0.007694059864815719  loss: 1.4937 (1.7532)  time: 1.1196  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [1470/5782]  eta: 1:19:28  lr: 0.007678032427097158  loss: 1.4937 (1.7516)  time: 1.1195  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [1480/5782]  eta: 1:19:17  lr: 0.007662001271143421  loss: 1.3981 (1.7499)  time: 1.1189  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [1490/5782]  eta: 1:19:07  lr: 0.007645966387443824  loss: 1.3923 (1.7485)  time: 1.1190  data: 0.0056  max mem: 14987\n",
      "Epoch: [0]  [1500/5782]  eta: 1:18:56  lr: 0.007629927766441149  loss: 1.5047 (1.7474)  time: 1.1197  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [1510/5782]  eta: 1:18:46  lr: 0.007613885398531278  loss: 1.5212 (1.7457)  time: 1.1201  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [1520/5782]  eta: 1:18:35  lr: 0.0075978392740628726  loss: 1.5253 (1.7449)  time: 1.1224  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [1530/5782]  eta: 1:18:24  lr: 0.007581789383337015  loss: 1.5278 (1.7436)  time: 1.1241  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [1540/5782]  eta: 1:18:14  lr: 0.007565735716606881  loss: 1.5095 (1.7426)  time: 1.1239  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [1550/5782]  eta: 1:18:05  lr: 0.007549678264077363  loss: 1.5204 (1.7418)  time: 1.1511  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [1560/5782]  eta: 1:17:55  lr: 0.007533617015904739  loss: 1.5339 (1.7408)  time: 1.1745  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [1570/5782]  eta: 1:17:46  lr: 0.0075175519621963095  loss: 1.4615 (1.7391)  time: 1.1754  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [1580/5782]  eta: 1:17:37  lr: 0.007501483093010029  loss: 1.4623 (1.7382)  time: 1.1837  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [1590/5782]  eta: 1:17:28  lr: 0.007485410398354152  loss: 1.4950 (1.7378)  time: 1.1778  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [1600/5782]  eta: 1:17:18  lr: 0.0074693338681868565  loss: 1.5611 (1.7364)  time: 1.1715  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [1610/5782]  eta: 1:17:09  lr: 0.007453253492415878  loss: 1.5125 (1.7351)  time: 1.1758  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [1620/5782]  eta: 1:16:59  lr: 0.007437169260898139  loss: 1.4755 (1.7342)  time: 1.1698  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [1630/5782]  eta: 1:16:50  lr: 0.007421081163439362  loss: 1.4209 (1.7327)  time: 1.1814  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [1640/5782]  eta: 1:16:42  lr: 0.007404989189793683  loss: 1.5477 (1.7320)  time: 1.1989  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [1650/5782]  eta: 1:16:33  lr: 0.007388893329663282  loss: 1.6233 (1.7309)  time: 1.2091  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [1660/5782]  eta: 1:16:25  lr: 0.007372793572697981  loss: 1.5478 (1.7302)  time: 1.2157  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [1670/5782]  eta: 1:16:16  lr: 0.0073566899084948475  loss: 1.5187 (1.7289)  time: 1.2166  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [1680/5782]  eta: 1:16:07  lr: 0.0073405823265978104  loss: 1.5187 (1.7277)  time: 1.2176  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [1690/5782]  eta: 1:15:59  lr: 0.0073244708164972435  loss: 1.5519 (1.7269)  time: 1.2162  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [1700/5782]  eta: 1:15:50  lr: 0.007308355367629564  loss: 1.4695 (1.7257)  time: 1.2089  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [1710/5782]  eta: 1:15:41  lr: 0.007292235969376828  loss: 1.4317 (1.7250)  time: 1.1991  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [1720/5782]  eta: 1:15:31  lr: 0.007276112611066311  loss: 1.5026 (1.7239)  time: 1.1971  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [1730/5782]  eta: 1:15:21  lr: 0.00725998528197009  loss: 1.5431 (1.7231)  time: 1.1706  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [1740/5782]  eta: 1:15:10  lr: 0.007243853971304632  loss: 1.4526 (1.7212)  time: 1.1450  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [1750/5782]  eta: 1:14:59  lr: 0.007227718668230342  loss: 1.4526 (1.7208)  time: 1.1336  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [1760/5782]  eta: 1:14:48  lr: 0.0072115793618511585  loss: 1.5946 (1.7201)  time: 1.1247  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [1770/5782]  eta: 1:14:37  lr: 0.007195436041214106  loss: 1.4878 (1.7186)  time: 1.1238  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [1780/5782]  eta: 1:14:26  lr: 0.007179288695308855  loss: 1.4238 (1.7173)  time: 1.1232  data: 0.0057  max mem: 14987\n",
      "Epoch: [0]  [1790/5782]  eta: 1:14:15  lr: 0.007163137313067274  loss: 1.5435 (1.7171)  time: 1.1241  data: 0.0057  max mem: 14987\n",
      "Epoch: [0]  [1800/5782]  eta: 1:14:04  lr: 0.007146981883362999  loss: 1.4951 (1.7154)  time: 1.1241  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [1810/5782]  eta: 1:13:53  lr: 0.007130822395010948  loss: 1.4223 (1.7144)  time: 1.1231  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [1820/5782]  eta: 1:13:42  lr: 0.007114658836766892  loss: 1.4976 (1.7134)  time: 1.1228  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [1830/5782]  eta: 1:13:31  lr: 0.007098491197326981  loss: 1.5586 (1.7129)  time: 1.1225  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [1840/5782]  eta: 1:13:20  lr: 0.007082319465327271  loss: 1.5095 (1.7116)  time: 1.1226  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [1850/5782]  eta: 1:13:09  lr: 0.007066143629343258  loss: 1.5131 (1.7112)  time: 1.1224  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [1860/5782]  eta: 1:12:58  lr: 0.007049963677889405  loss: 1.5226 (1.7102)  time: 1.1227  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [1870/5782]  eta: 1:12:47  lr: 0.007033779599418644  loss: 1.4884 (1.7094)  time: 1.1219  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [1880/5782]  eta: 1:12:36  lr: 0.007017591382321899  loss: 1.5289 (1.7090)  time: 1.1206  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [1890/5782]  eta: 1:12:25  lr: 0.007001399014927598  loss: 1.4759 (1.7085)  time: 1.1224  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [1900/5782]  eta: 1:12:15  lr: 0.006985202485501162  loss: 1.4571 (1.7082)  time: 1.1392  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [1910/5782]  eta: 1:12:04  lr: 0.006969001782244507  loss: 1.4653 (1.7068)  time: 1.1407  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [1920/5782]  eta: 1:11:53  lr: 0.006952796893295542  loss: 1.4622 (1.7059)  time: 1.1256  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [1930/5782]  eta: 1:11:42  lr: 0.006936587806727644  loss: 1.5249 (1.7051)  time: 1.1243  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [1940/5782]  eta: 1:11:31  lr: 0.006920374510549149  loss: 1.4287 (1.7036)  time: 1.1217  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [1950/5782]  eta: 1:11:20  lr: 0.00690415699270282  loss: 1.4287 (1.7026)  time: 1.1216  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [1960/5782]  eta: 1:11:09  lr: 0.006887935241065315  loss: 1.5666 (1.7018)  time: 1.1231  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [1970/5782]  eta: 1:10:58  lr: 0.00687170924344666  loss: 1.4803 (1.7003)  time: 1.1238  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [1980/5782]  eta: 1:10:46  lr: 0.006855478987589692  loss: 1.4511 (1.6992)  time: 1.1229  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [1990/5782]  eta: 1:10:35  lr: 0.00683924446116953  loss: 1.4711 (1.6981)  time: 1.1203  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [2000/5782]  eta: 1:10:24  lr: 0.006823005651793006  loss: 1.4922 (1.6974)  time: 1.1197  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [2010/5782]  eta: 1:10:13  lr: 0.006806762546998107  loss: 1.4916 (1.6964)  time: 1.1219  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [2020/5782]  eta: 1:10:02  lr: 0.006790515134253416  loss: 1.4557 (1.6954)  time: 1.1238  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [2030/5782]  eta: 1:09:52  lr: 0.006774263400957534  loss: 1.3654 (1.6939)  time: 1.1564  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [2040/5782]  eta: 1:09:43  lr: 0.006758007334438501  loss: 1.5136 (1.6938)  time: 1.1931  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [2050/5782]  eta: 1:09:34  lr: 0.006741746921953221  loss: 1.5576 (1.6930)  time: 1.2236  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [2060/5782]  eta: 1:09:24  lr: 0.00672548215068686  loss: 1.4005 (1.6915)  time: 1.2361  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [2070/5782]  eta: 1:09:14  lr: 0.006709213007752246  loss: 1.3394 (1.6901)  time: 1.1782  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [2080/5782]  eta: 1:09:02  lr: 0.0066929394801892765  loss: 1.4024 (1.6899)  time: 1.1305  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [2090/5782]  eta: 1:08:52  lr: 0.0066766615549643  loss: 1.5142 (1.6891)  time: 1.1380  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [2100/5782]  eta: 1:08:41  lr: 0.0066603792189695  loss: 1.5694 (1.6892)  time: 1.1363  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [2110/5782]  eta: 1:08:30  lr: 0.006644092459022267  loss: 1.5730 (1.6884)  time: 1.1232  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [2120/5782]  eta: 1:08:20  lr: 0.006627801261864574  loss: 1.5151 (1.6880)  time: 1.1552  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [2130/5782]  eta: 1:08:09  lr: 0.006611505614162326  loss: 1.5378 (1.6874)  time: 1.1767  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [2140/5782]  eta: 1:08:00  lr: 0.00659520550250473  loss: 1.6198 (1.6879)  time: 1.1929  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2150/5782]  eta: 1:07:49  lr: 0.006578900913403633  loss: 1.4698 (1.6862)  time: 1.1733  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [2160/5782]  eta: 1:07:37  lr: 0.006562591833292855  loss: 1.3951 (1.6855)  time: 1.1253  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [2170/5782]  eta: 1:07:26  lr: 0.006546278248527535  loss: 1.4034 (1.6842)  time: 1.1261  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2180/5782]  eta: 1:07:15  lr: 0.006529960145383442  loss: 1.3647 (1.6830)  time: 1.1266  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [2190/5782]  eta: 1:07:04  lr: 0.006513637510056299  loss: 1.4141 (1.6818)  time: 1.1224  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2200/5782]  eta: 1:06:53  lr: 0.006497310328661094  loss: 1.4141 (1.6808)  time: 1.1220  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2210/5782]  eta: 1:06:42  lr: 0.006480978587231377  loss: 1.4459 (1.6801)  time: 1.1440  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2220/5782]  eta: 1:06:32  lr: 0.0064646422717185535  loss: 1.4260 (1.6791)  time: 1.1548  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2230/5782]  eta: 1:06:20  lr: 0.006448301367991165  loss: 1.4399 (1.6787)  time: 1.1351  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [2240/5782]  eta: 1:06:09  lr: 0.006431955861834174  loss: 1.4859 (1.6780)  time: 1.1248  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2250/5782]  eta: 1:05:58  lr: 0.006415605738948225  loss: 1.5902 (1.6777)  time: 1.1249  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [2260/5782]  eta: 1:05:47  lr: 0.006399250984948917  loss: 1.5708 (1.6774)  time: 1.1245  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [2270/5782]  eta: 1:05:36  lr: 0.006382891585366032  loss: 1.5156 (1.6765)  time: 1.1232  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2280/5782]  eta: 1:05:25  lr: 0.006366527525642794  loss: 1.4385 (1.6756)  time: 1.1209  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2290/5782]  eta: 1:05:13  lr: 0.006350158791135108  loss: 1.4450 (1.6748)  time: 1.1196  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2300/5782]  eta: 1:05:02  lr: 0.006333785367110758  loss: 1.4390 (1.6738)  time: 1.1229  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2310/5782]  eta: 1:04:51  lr: 0.0063174072387486525  loss: 1.4814 (1.6733)  time: 1.1258  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2320/5782]  eta: 1:04:40  lr: 0.00630102439113801  loss: 1.4905 (1.6730)  time: 1.1251  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2330/5782]  eta: 1:04:29  lr: 0.006284636809277568  loss: 1.5895 (1.6726)  time: 1.1240  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [2340/5782]  eta: 1:04:18  lr: 0.006268244478074763  loss: 1.4951 (1.6715)  time: 1.1237  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [2350/5782]  eta: 1:04:06  lr: 0.006251847382344907  loss: 1.4556 (1.6706)  time: 1.1235  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [2360/5782]  eta: 1:03:55  lr: 0.006235445506810358  loss: 1.4177 (1.6697)  time: 1.1232  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [2370/5782]  eta: 1:03:44  lr: 0.00621903883609968  loss: 1.4177 (1.6689)  time: 1.1236  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2380/5782]  eta: 1:03:33  lr: 0.006202627354746789  loss: 1.4864 (1.6686)  time: 1.1244  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2390/5782]  eta: 1:03:22  lr: 0.006186211047190081  loss: 1.4728 (1.6679)  time: 1.1238  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [2400/5782]  eta: 1:03:11  lr: 0.006169789897771578  loss: 1.4676 (1.6672)  time: 1.1238  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [2410/5782]  eta: 1:02:59  lr: 0.006153363890736024  loss: 1.5415 (1.6669)  time: 1.1256  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [2420/5782]  eta: 1:02:48  lr: 0.00613693301023  loss: 1.5719 (1.6665)  time: 1.1249  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [2430/5782]  eta: 1:02:37  lr: 0.006120497240301023  loss: 1.5337 (1.6658)  time: 1.1234  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [2440/5782]  eta: 1:02:26  lr: 0.006104056564896614  loss: 1.5451 (1.6654)  time: 1.1244  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [2450/5782]  eta: 1:02:15  lr: 0.006087610967863388  loss: 1.4213 (1.6643)  time: 1.1247  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [2460/5782]  eta: 1:02:04  lr: 0.006071160432946104  loss: 1.3624 (1.6633)  time: 1.1246  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [2470/5782]  eta: 1:01:52  lr: 0.006054704943786721  loss: 1.4337 (1.6622)  time: 1.1251  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2480/5782]  eta: 1:01:41  lr: 0.006038244483923422  loss: 1.4418 (1.6618)  time: 1.1247  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2490/5782]  eta: 1:01:31  lr: 0.006021779036789658  loss: 1.4165 (1.6609)  time: 1.1422  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2500/5782]  eta: 1:01:20  lr: 0.006005308585713145  loss: 1.3774 (1.6598)  time: 1.1634  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [2510/5782]  eta: 1:01:09  lr: 0.005988833113914871  loss: 1.3840 (1.6589)  time: 1.1642  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [2520/5782]  eta: 1:00:58  lr: 0.005972352604508093  loss: 1.3921 (1.6582)  time: 1.1431  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2530/5782]  eta: 1:00:47  lr: 0.005955867040497295  loss: 1.4484 (1.6577)  time: 1.1245  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [2540/5782]  eta: 1:00:36  lr: 0.0059393764047771655  loss: 1.4642 (1.6570)  time: 1.1234  data: 0.0068  max mem: 14987\n",
      "Epoch: [0]  [2550/5782]  eta: 1:00:25  lr: 0.005922880680131542  loss: 1.4453 (1.6559)  time: 1.1232  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [2560/5782]  eta: 1:00:14  lr: 0.005906379849232344  loss: 1.4205 (1.6551)  time: 1.1370  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [2570/5782]  eta: 1:00:02  lr: 0.005889873894638501  loss: 1.4438 (1.6544)  time: 1.1372  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2580/5782]  eta: 0:59:51  lr: 0.005873362798794859  loss: 1.3655 (1.6533)  time: 1.1238  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [2590/5782]  eta: 0:59:41  lr: 0.005856846544031073  loss: 1.3402 (1.6524)  time: 1.1409  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2600/5782]  eta: 0:59:30  lr: 0.005840325112560497  loss: 1.3571 (1.6514)  time: 1.1557  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [2610/5782]  eta: 0:59:18  lr: 0.005823798486479037  loss: 1.3724 (1.6505)  time: 1.1384  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [2620/5782]  eta: 0:59:08  lr: 0.005807266647764015  loss: 1.4130 (1.6499)  time: 1.1376  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [2630/5782]  eta: 0:58:56  lr: 0.005790729578272999  loss: 1.4893 (1.6494)  time: 1.1367  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [2640/5782]  eta: 0:58:45  lr: 0.005774187259742622  loss: 1.4367 (1.6487)  time: 1.1220  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [2650/5782]  eta: 0:58:34  lr: 0.005757639673787394  loss: 1.4072 (1.6480)  time: 1.1225  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [2660/5782]  eta: 0:58:23  lr: 0.005741086801898497  loss: 1.4199 (1.6474)  time: 1.1234  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [2670/5782]  eta: 0:58:12  lr: 0.0057245286254425445  loss: 1.3828 (1.6467)  time: 1.1212  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [2680/5782]  eta: 0:58:00  lr: 0.0057079651256603525  loss: 1.3995 (1.6459)  time: 1.1202  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [2690/5782]  eta: 0:57:49  lr: 0.005691396283665687  loss: 1.4319 (1.6457)  time: 1.1231  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [2700/5782]  eta: 0:57:38  lr: 0.005674822080443969  loss: 1.5131 (1.6450)  time: 1.1333  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2710/5782]  eta: 0:57:27  lr: 0.0056582424968509995  loss: 1.3609 (1.6440)  time: 1.1429  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2720/5782]  eta: 0:57:16  lr: 0.005641657513611649  loss: 1.3478 (1.6429)  time: 1.1348  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2730/5782]  eta: 0:57:05  lr: 0.005625067111318526  loss: 1.3391 (1.6421)  time: 1.1387  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2740/5782]  eta: 0:56:55  lr: 0.005608471270430633  loss: 1.3301 (1.6413)  time: 1.1863  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2750/5782]  eta: 0:56:44  lr: 0.005591869971272015  loss: 1.4140 (1.6407)  time: 1.1908  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2760/5782]  eta: 0:56:33  lr: 0.005575263194030362  loss: 1.4413 (1.6399)  time: 1.1612  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2770/5782]  eta: 0:56:23  lr: 0.005558650918755619  loss: 1.4616 (1.6395)  time: 1.1742  data: 0.0068  max mem: 14987\n",
      "Epoch: [0]  [2780/5782]  eta: 0:56:12  lr: 0.005542033125358578  loss: 1.4865 (1.6388)  time: 1.1704  data: 0.0070  max mem: 14987\n",
      "Epoch: [0]  [2790/5782]  eta: 0:56:01  lr: 0.005525409793609411  loss: 1.3306 (1.6377)  time: 1.1396  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [2800/5782]  eta: 0:55:50  lr: 0.005508780903136249  loss: 1.3306 (1.6372)  time: 1.1410  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2810/5782]  eta: 0:55:39  lr: 0.005492146433423674  loss: 1.4951 (1.6370)  time: 1.1756  data: 0.0068  max mem: 14987\n",
      "Epoch: [0]  [2820/5782]  eta: 0:55:29  lr: 0.005475506363811235  loss: 1.4964 (1.6365)  time: 1.1988  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2830/5782]  eta: 0:55:19  lr: 0.005458860673491932  loss: 1.4866 (1.6361)  time: 1.2313  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [2840/5782]  eta: 0:55:09  lr: 0.005442209341510666  loss: 1.4775 (1.6357)  time: 1.2294  data: 0.0068  max mem: 14987\n",
      "Epoch: [0]  [2850/5782]  eta: 0:54:58  lr: 0.005425552346762686  loss: 1.4067 (1.6347)  time: 1.1779  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [2860/5782]  eta: 0:54:47  lr: 0.005408889667992008  loss: 1.3632 (1.6345)  time: 1.1753  data: 0.0068  max mem: 14987\n",
      "Epoch: [0]  [2870/5782]  eta: 0:54:36  lr: 0.0053922212837898  loss: 1.5388 (1.6342)  time: 1.1805  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2880/5782]  eta: 0:54:25  lr: 0.005375547172592753  loss: 1.5183 (1.6336)  time: 1.1622  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2890/5782]  eta: 0:54:14  lr: 0.005358867312681447  loss: 1.4323 (1.6330)  time: 1.1573  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2900/5782]  eta: 0:54:04  lr: 0.00534218168217866  loss: 1.5432 (1.6332)  time: 1.1588  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2910/5782]  eta: 0:53:53  lr: 0.005325490259047677  loss: 1.5394 (1.6329)  time: 1.1582  data: 0.0068  max mem: 14987\n",
      "Epoch: [0]  [2920/5782]  eta: 0:53:42  lr: 0.005308793021090563  loss: 1.5394 (1.6326)  time: 1.1645  data: 0.0069  max mem: 14987\n",
      "Epoch: [0]  [2930/5782]  eta: 0:53:31  lr: 0.0052920899459464234  loss: 1.5836 (1.6321)  time: 1.1782  data: 0.0069  max mem: 14987\n",
      "Epoch: [0]  [2940/5782]  eta: 0:53:20  lr: 0.005275381011089623  loss: 1.4279 (1.6314)  time: 1.1727  data: 0.0069  max mem: 14987\n",
      "Epoch: [0]  [2950/5782]  eta: 0:53:09  lr: 0.005258666193827993  loss: 1.4201 (1.6308)  time: 1.1444  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [2960/5782]  eta: 0:52:58  lr: 0.005241945471301006  loss: 1.5066 (1.6306)  time: 1.1253  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [2970/5782]  eta: 0:52:46  lr: 0.0052252188204779245  loss: 1.5211 (1.6303)  time: 1.1234  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [2980/5782]  eta: 0:52:35  lr: 0.005208486218155932  loss: 1.5327 (1.6301)  time: 1.1242  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [2990/5782]  eta: 0:52:24  lr: 0.00519174764095821  loss: 1.4850 (1.6293)  time: 1.1240  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [3000/5782]  eta: 0:52:12  lr: 0.0051750030653320254  loss: 1.3465 (1.6283)  time: 1.1241  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3010/5782]  eta: 0:52:01  lr: 0.005158252467546754  loss: 1.3776 (1.6278)  time: 1.1250  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [3020/5782]  eta: 0:51:50  lr: 0.005141495823691896  loss: 1.4214 (1.6269)  time: 1.1252  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [3030/5782]  eta: 0:51:39  lr: 0.005124733109675059  loss: 1.3782 (1.6264)  time: 1.1255  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [3040/5782]  eta: 0:51:27  lr: 0.005107964301219902  loss: 1.3941 (1.6256)  time: 1.1241  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [3050/5782]  eta: 0:51:17  lr: 0.0050911893738640555  loss: 1.4067 (1.6251)  time: 1.1551  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3060/5782]  eta: 0:51:06  lr: 0.0050744083029570135  loss: 1.4067 (1.6244)  time: 1.1751  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [3070/5782]  eta: 0:50:55  lr: 0.00505762106365799  loss: 1.2767 (1.6237)  time: 1.1526  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [3080/5782]  eta: 0:50:44  lr: 0.005040827630933735  loss: 1.2712 (1.6227)  time: 1.1656  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [3090/5782]  eta: 0:50:33  lr: 0.0050240279795563305  loss: 1.3709 (1.6223)  time: 1.1712  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [3100/5782]  eta: 0:50:22  lr: 0.005007222084100952  loss: 1.3709 (1.6215)  time: 1.1399  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [3110/5782]  eta: 0:50:11  lr: 0.004990409918943586  loss: 1.2938 (1.6207)  time: 1.1474  data: 0.0069  max mem: 14987\n",
      "Epoch: [0]  [3120/5782]  eta: 0:50:00  lr: 0.00497359145825871  loss: 1.3617 (1.6202)  time: 1.1679  data: 0.0069  max mem: 14987\n",
      "Epoch: [0]  [3130/5782]  eta: 0:49:49  lr: 0.004956766676016962  loss: 1.4049 (1.6196)  time: 1.1591  data: 0.0068  max mem: 14987\n",
      "Epoch: [0]  [3140/5782]  eta: 0:49:38  lr: 0.004939935545982743  loss: 1.4050 (1.6193)  time: 1.1508  data: 0.0074  max mem: 14987\n",
      "Epoch: [0]  [3150/5782]  eta: 0:49:27  lr: 0.0049230980417118  loss: 1.4695 (1.6194)  time: 1.1918  data: 0.0071  max mem: 14987\n",
      "Epoch: [0]  [3160/5782]  eta: 0:49:17  lr: 0.004906254136548764  loss: 1.5583 (1.6191)  time: 1.2418  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [3170/5782]  eta: 0:49:06  lr: 0.004889403803624664  loss: 1.4033 (1.6185)  time: 1.2182  data: 0.0069  max mem: 14987\n",
      "Epoch: [0]  [3180/5782]  eta: 0:48:55  lr: 0.004872547015854373  loss: 1.4090 (1.6182)  time: 1.1823  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3190/5782]  eta: 0:48:44  lr: 0.0048556837459340415  loss: 1.4683 (1.6180)  time: 1.1726  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [3200/5782]  eta: 0:48:33  lr: 0.004838813966338483  loss: 1.3578 (1.6173)  time: 1.1609  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [3210/5782]  eta: 0:48:22  lr: 0.004821937649318508  loss: 1.3494 (1.6167)  time: 1.1605  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3220/5782]  eta: 0:48:11  lr: 0.004805054766898221  loss: 1.3472 (1.6158)  time: 1.1663  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3230/5782]  eta: 0:48:00  lr: 0.004788165290872287  loss: 1.3837 (1.6153)  time: 1.1473  data: 0.0068  max mem: 14987\n",
      "Epoch: [0]  [3240/5782]  eta: 0:47:49  lr: 0.004771269192803132  loss: 1.3837 (1.6149)  time: 1.1270  data: 0.0072  max mem: 14987\n",
      "Epoch: [0]  [3250/5782]  eta: 0:47:38  lr: 0.004754366444018111  loss: 1.3540 (1.6140)  time: 1.1534  data: 0.0070  max mem: 14987\n",
      "Epoch: [0]  [3260/5782]  eta: 0:47:26  lr: 0.004737457015606632  loss: 1.3540 (1.6136)  time: 1.1530  data: 0.0069  max mem: 14987\n",
      "Epoch: [0]  [3270/5782]  eta: 0:47:15  lr: 0.004720540878417231  loss: 1.2995 (1.6130)  time: 1.1254  data: 0.0070  max mem: 14987\n",
      "Epoch: [0]  [3280/5782]  eta: 0:47:04  lr: 0.004703618003054583  loss: 1.2995 (1.6125)  time: 1.1356  data: 0.0070  max mem: 14987\n",
      "Epoch: [0]  [3290/5782]  eta: 0:46:53  lr: 0.004686688359876502  loss: 1.3589 (1.6118)  time: 1.1456  data: 0.0070  max mem: 14987\n",
      "Epoch: [0]  [3300/5782]  eta: 0:46:42  lr: 0.004669751918990852  loss: 1.4355 (1.6112)  time: 1.1445  data: 0.0071  max mem: 14987\n",
      "Epoch: [0]  [3310/5782]  eta: 0:46:30  lr: 0.004652808650252422  loss: 1.4899 (1.6110)  time: 1.1348  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [3320/5782]  eta: 0:46:19  lr: 0.00463585852325976  loss: 1.4453 (1.6105)  time: 1.1237  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [3330/5782]  eta: 0:46:08  lr: 0.004618901507351941  loss: 1.4267 (1.6100)  time: 1.1221  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3340/5782]  eta: 0:45:56  lr: 0.004601937571605281  loss: 1.3410 (1.6093)  time: 1.1226  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [3350/5782]  eta: 0:45:45  lr: 0.004584966684830002  loss: 1.3253 (1.6087)  time: 1.1226  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [3360/5782]  eta: 0:45:34  lr: 0.00456798881556684  loss: 1.3627 (1.6081)  time: 1.1218  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [3370/5782]  eta: 0:45:22  lr: 0.004551003932083589  loss: 1.4387 (1.6077)  time: 1.1223  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [3380/5782]  eta: 0:45:11  lr: 0.004534012002371595  loss: 1.4119 (1.6069)  time: 1.1229  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [3390/5782]  eta: 0:45:00  lr: 0.004517012994142184  loss: 1.3933 (1.6065)  time: 1.1221  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [3400/5782]  eta: 0:44:48  lr: 0.0045000068748230326  loss: 1.3884 (1.6057)  time: 1.1213  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [3410/5782]  eta: 0:44:37  lr: 0.004482993611554467  loss: 1.3640 (1.6051)  time: 1.1219  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [3420/5782]  eta: 0:44:26  lr: 0.004465973171185716  loss: 1.3497 (1.6048)  time: 1.1225  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3430/5782]  eta: 0:44:14  lr: 0.00444894552027108  loss: 1.3974 (1.6043)  time: 1.1219  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3440/5782]  eta: 0:44:03  lr: 0.0044319106250660465  loss: 1.3709 (1.6036)  time: 1.1208  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [3450/5782]  eta: 0:43:52  lr: 0.004414868451523325  loss: 1.3331 (1.6028)  time: 1.1198  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [3460/5782]  eta: 0:43:40  lr: 0.00439781896528883  loss: 1.3333 (1.6020)  time: 1.1197  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [3470/5782]  eta: 0:43:29  lr: 0.004380762131697573  loss: 1.3926 (1.6017)  time: 1.1197  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [3480/5782]  eta: 0:43:18  lr: 0.004363697915769491  loss: 1.3926 (1.6012)  time: 1.1191  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [3490/5782]  eta: 0:43:06  lr: 0.0043466262822052125  loss: 1.3628 (1.6006)  time: 1.1195  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [3500/5782]  eta: 0:42:55  lr: 0.004329547195381722  loss: 1.3804 (1.6001)  time: 1.1198  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [3510/5782]  eta: 0:42:44  lr: 0.004312460619347966  loss: 1.3660 (1.5995)  time: 1.1196  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [3520/5782]  eta: 0:42:32  lr: 0.004295366517820378  loss: 1.3575 (1.5991)  time: 1.1200  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [3530/5782]  eta: 0:42:21  lr: 0.004278264854178312  loss: 1.3707 (1.5985)  time: 1.1196  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [3540/5782]  eta: 0:42:10  lr: 0.0042611555914594005  loss: 1.3595 (1.5977)  time: 1.1194  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [3550/5782]  eta: 0:41:58  lr: 0.004244038692354829  loss: 1.3083 (1.5970)  time: 1.1197  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [3560/5782]  eta: 0:41:47  lr: 0.004226914119204523  loss: 1.3550 (1.5962)  time: 1.1184  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [3570/5782]  eta: 0:41:35  lr: 0.0042097818339922365  loss: 1.3550 (1.5957)  time: 1.1176  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [3580/5782]  eta: 0:41:24  lr: 0.004192641798340562  loss: 1.4112 (1.5953)  time: 1.1039  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [3590/5782]  eta: 0:41:12  lr: 0.004175493973505844  loss: 1.3667 (1.5947)  time: 1.0890  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [3600/5782]  eta: 0:41:01  lr: 0.004158338320372992  loss: 1.3667 (1.5940)  time: 1.0866  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [3610/5782]  eta: 0:40:49  lr: 0.004141174799450199  loss: 1.3770 (1.5937)  time: 1.0839  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [3620/5782]  eta: 0:40:38  lr: 0.004124003370863568  loss: 1.3644 (1.5930)  time: 1.0829  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [3630/5782]  eta: 0:40:26  lr: 0.004106823994351624  loss: 1.3347 (1.5923)  time: 1.0821  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [3640/5782]  eta: 0:40:15  lr: 0.004089636629259723  loss: 1.3950 (1.5919)  time: 1.0824  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [3650/5782]  eta: 0:40:03  lr: 0.004072441234534366  loss: 1.4315 (1.5917)  time: 1.0830  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [3660/5782]  eta: 0:39:52  lr: 0.004055237768717392  loss: 1.4310 (1.5914)  time: 1.0818  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [3670/5782]  eta: 0:39:40  lr: 0.00403802618994006  loss: 1.4828 (1.5910)  time: 1.0813  data: 0.0057  max mem: 14987\n",
      "Epoch: [0]  [3680/5782]  eta: 0:39:29  lr: 0.004020806455917023  loss: 1.4752 (1.5907)  time: 1.0807  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [3690/5782]  eta: 0:39:17  lr: 0.004003578523940175  loss: 1.3797 (1.5900)  time: 1.0807  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [3700/5782]  eta: 0:39:05  lr: 0.003986342350872383  loss: 1.3833 (1.5897)  time: 1.0815  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [3710/5782]  eta: 0:38:54  lr: 0.003969097893141092  loss: 1.3623 (1.5891)  time: 1.0818  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [3720/5782]  eta: 0:38:42  lr: 0.003951845106731813  loss: 1.3337 (1.5886)  time: 1.0813  data: 0.0057  max mem: 14987\n",
      "Epoch: [0]  [3730/5782]  eta: 0:38:31  lr: 0.00393458394718147  loss: 1.4110 (1.5882)  time: 1.0805  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [3740/5782]  eta: 0:38:19  lr: 0.003917314369571611  loss: 1.4725 (1.5879)  time: 1.0801  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [3750/5782]  eta: 0:38:08  lr: 0.0039000363285214986  loss: 1.3892 (1.5874)  time: 1.0798  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [3760/5782]  eta: 0:37:56  lr: 0.003882749778181056  loss: 1.3892 (1.5873)  time: 1.0805  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [3770/5782]  eta: 0:37:45  lr: 0.0038654546722236504  loss: 1.4113 (1.5868)  time: 1.0857  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [3780/5782]  eta: 0:37:33  lr: 0.003848150963838756  loss: 1.3350 (1.5860)  time: 1.0899  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [3790/5782]  eta: 0:37:22  lr: 0.0038308386057244585  loss: 1.3532 (1.5857)  time: 1.0935  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [3800/5782]  eta: 0:37:11  lr: 0.0038135175500797944  loss: 1.3122 (1.5851)  time: 1.1037  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [3810/5782]  eta: 0:36:59  lr: 0.0037961877485969387  loss: 1.3159 (1.5846)  time: 1.1121  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [3820/5782]  eta: 0:36:48  lr: 0.003778849152453237  loss: 1.3620 (1.5841)  time: 1.1144  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [3830/5782]  eta: 0:36:37  lr: 0.003761501712303061  loss: 1.3395 (1.5836)  time: 1.1152  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [3840/5782]  eta: 0:36:25  lr: 0.0037441453782694973  loss: 1.2831 (1.5828)  time: 1.1168  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [3850/5782]  eta: 0:36:14  lr: 0.003726780099935862  loss: 1.3013 (1.5822)  time: 1.1179  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [3860/5782]  eta: 0:36:03  lr: 0.0037094058263370277  loss: 1.3226 (1.5816)  time: 1.1188  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3870/5782]  eta: 0:35:52  lr: 0.0036920225059505736  loss: 1.3547 (1.5811)  time: 1.1202  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [3880/5782]  eta: 0:35:40  lr: 0.0036746300866877454  loss: 1.3219 (1.5803)  time: 1.1205  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3890/5782]  eta: 0:35:29  lr: 0.0036572285158842136  loss: 1.3414 (1.5799)  time: 1.1206  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3900/5782]  eta: 0:35:18  lr: 0.003639817740290634  loss: 1.3621 (1.5793)  time: 1.1237  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3910/5782]  eta: 0:35:07  lr: 0.0036223977060630096  loss: 1.2523 (1.5785)  time: 1.1307  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3920/5782]  eta: 0:34:55  lr: 0.0036049683587528287  loss: 1.3227 (1.5783)  time: 1.1355  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3930/5782]  eta: 0:34:44  lr: 0.0035875296432969967  loss: 1.3044 (1.5778)  time: 1.1304  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3940/5782]  eta: 0:34:33  lr: 0.003570081504007535  loss: 1.3044 (1.5773)  time: 1.1411  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3950/5782]  eta: 0:34:22  lr: 0.0035526238845610646  loss: 1.3865 (1.5768)  time: 1.1506  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [3960/5782]  eta: 0:34:11  lr: 0.003535156727988037  loss: 1.2748 (1.5761)  time: 1.1330  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [3970/5782]  eta: 0:33:59  lr: 0.0035176799766617295  loss: 1.2396 (1.5756)  time: 1.1217  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [3980/5782]  eta: 0:33:48  lr: 0.0035001935722870046  loss: 1.3052 (1.5752)  time: 1.1201  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [3990/5782]  eta: 0:33:37  lr: 0.00348269745588879  loss: 1.3231 (1.5747)  time: 1.1188  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [4000/5782]  eta: 0:33:25  lr: 0.0034651915678003066  loss: 1.3639 (1.5743)  time: 1.1191  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [4010/5782]  eta: 0:33:14  lr: 0.0034476758476510317  loss: 1.3832 (1.5737)  time: 1.1193  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [4020/5782]  eta: 0:33:03  lr: 0.0034301502343543747  loss: 1.3342 (1.5730)  time: 1.1191  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [4030/5782]  eta: 0:32:52  lr: 0.0034126146660950615  loss: 1.2456 (1.5724)  time: 1.1180  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [4040/5782]  eta: 0:32:40  lr: 0.003395069080316239  loss: 1.3239 (1.5723)  time: 1.1177  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [4050/5782]  eta: 0:32:29  lr: 0.003377513413706261  loss: 1.3266 (1.5717)  time: 1.1177  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [4060/5782]  eta: 0:32:18  lr: 0.0033599476021851688  loss: 1.3104 (1.5712)  time: 1.1174  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [4070/5782]  eta: 0:32:06  lr: 0.0033423715808908417  loss: 1.3868 (1.5712)  time: 1.1168  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [4080/5782]  eta: 0:31:55  lr: 0.0033247852841648343  loss: 1.3353 (1.5707)  time: 1.1159  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [4090/5782]  eta: 0:31:44  lr: 0.0033071886455378462  loss: 1.3356 (1.5704)  time: 1.1161  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [4100/5782]  eta: 0:31:33  lr: 0.003289581597714857  loss: 1.3543 (1.5699)  time: 1.1150  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [4110/5782]  eta: 0:31:21  lr: 0.0032719640725598956  loss: 1.3264 (1.5692)  time: 1.1134  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [4120/5782]  eta: 0:31:10  lr: 0.003254336001080435  loss: 1.3326 (1.5688)  time: 1.1135  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [4130/5782]  eta: 0:30:59  lr: 0.0032366973134113925  loss: 1.3675 (1.5684)  time: 1.1150  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [4140/5782]  eta: 0:30:47  lr: 0.0032190479387987523  loss: 1.3403 (1.5679)  time: 1.1154  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [4150/5782]  eta: 0:30:36  lr: 0.0032013878055827598  loss: 1.3407 (1.5675)  time: 1.1143  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [4160/5782]  eta: 0:30:25  lr: 0.003183716841180704  loss: 1.3311 (1.5669)  time: 1.1136  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [4170/5782]  eta: 0:30:13  lr: 0.003166034972069256  loss: 1.3403 (1.5666)  time: 1.1135  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [4180/5782]  eta: 0:30:02  lr: 0.0031483421237663714  loss: 1.3639 (1.5660)  time: 1.1136  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [4190/5782]  eta: 0:29:51  lr: 0.0031306382208127143  loss: 1.3249 (1.5654)  time: 1.1148  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [4200/5782]  eta: 0:29:40  lr: 0.003112923186752606  loss: 1.2667 (1.5648)  time: 1.1163  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [4210/5782]  eta: 0:29:28  lr: 0.003095196944114493  loss: 1.2424 (1.5641)  time: 1.1152  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [4220/5782]  eta: 0:29:17  lr: 0.003077459414390885  loss: 1.3079 (1.5635)  time: 1.1145  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [4230/5782]  eta: 0:29:06  lr: 0.0030597105180177756  loss: 1.3626 (1.5632)  time: 1.1007  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [4240/5782]  eta: 0:28:54  lr: 0.0030419501743535198  loss: 1.3533 (1.5626)  time: 1.0858  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [4250/5782]  eta: 0:28:43  lr: 0.0030241783016571534  loss: 1.3350 (1.5621)  time: 1.0838  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [4260/5782]  eta: 0:28:31  lr: 0.003006394817066115  loss: 1.2743 (1.5615)  time: 1.0821  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4270/5782]  eta: 0:28:20  lr: 0.0029885996365733867  loss: 1.2677 (1.5610)  time: 1.0817  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4280/5782]  eta: 0:28:09  lr: 0.0029707926750040005  loss: 1.3392 (1.5606)  time: 1.0806  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4290/5782]  eta: 0:27:57  lr: 0.0029529738459909147  loss: 1.3290 (1.5600)  time: 1.0796  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4300/5782]  eta: 0:27:46  lr: 0.002935143061950212  loss: 1.2730 (1.5594)  time: 1.0796  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4310/5782]  eta: 0:27:34  lr: 0.0029173002340556215  loss: 1.2276 (1.5587)  time: 1.0795  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4320/5782]  eta: 0:27:23  lr: 0.002899445272212332  loss: 1.2276 (1.5581)  time: 1.0794  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4330/5782]  eta: 0:27:12  lr: 0.0028815780850300603  loss: 1.2664 (1.5575)  time: 1.0784  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4340/5782]  eta: 0:27:00  lr: 0.002863698579795365  loss: 1.2688 (1.5569)  time: 1.0777  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4350/5782]  eta: 0:26:49  lr: 0.0028458066624431726  loss: 1.3068 (1.5563)  time: 1.0789  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4360/5782]  eta: 0:26:37  lr: 0.0028279022375274823  loss: 1.2260 (1.5558)  time: 1.0797  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4370/5782]  eta: 0:26:26  lr: 0.0028099852081912317  loss: 1.2608 (1.5554)  time: 1.0796  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4380/5782]  eta: 0:26:15  lr: 0.002792055476135284  loss: 1.2861 (1.5548)  time: 1.0797  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4390/5782]  eta: 0:26:03  lr: 0.002774112941586503  loss: 1.3426 (1.5544)  time: 1.0795  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [4400/5782]  eta: 0:25:52  lr: 0.002756157503264892  loss: 1.3185 (1.5538)  time: 1.0794  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4410/5782]  eta: 0:25:41  lr: 0.002738189058349759  loss: 1.2638 (1.5533)  time: 1.0789  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4420/5782]  eta: 0:25:29  lr: 0.002720207502444861  loss: 1.2638 (1.5528)  time: 1.0776  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4430/5782]  eta: 0:25:18  lr: 0.002702212729542501  loss: 1.2787 (1.5525)  time: 1.0766  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4440/5782]  eta: 0:25:06  lr: 0.0026842046319865446  loss: 1.3347 (1.5522)  time: 1.0768  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4450/5782]  eta: 0:24:55  lr: 0.0026661831004342914  loss: 1.3293 (1.5516)  time: 1.0767  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [4460/5782]  eta: 0:24:44  lr: 0.0026481480238171777  loss: 1.3001 (1.5510)  time: 1.0766  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4470/5782]  eta: 0:24:32  lr: 0.002630099289300258  loss: 1.3906 (1.5508)  time: 1.0766  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4480/5782]  eta: 0:24:21  lr: 0.0026120367822404273  loss: 1.4834 (1.5506)  time: 1.0766  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4490/5782]  eta: 0:24:10  lr: 0.002593960386143305  loss: 1.3567 (1.5503)  time: 1.0766  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [4500/5782]  eta: 0:23:58  lr: 0.002575869982618772  loss: 1.3567 (1.5498)  time: 1.0765  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [4510/5782]  eta: 0:23:47  lr: 0.0025577654513350744  loss: 1.3176 (1.5493)  time: 1.0771  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4520/5782]  eta: 0:23:36  lr: 0.0025396466699714433  loss: 1.3382 (1.5489)  time: 1.0769  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4530/5782]  eta: 0:23:24  lr: 0.002521513514169174  loss: 1.3382 (1.5484)  time: 1.0770  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4540/5782]  eta: 0:23:13  lr: 0.002503365857481105  loss: 1.3740 (1.5483)  time: 1.0774  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4550/5782]  eta: 0:23:02  lr: 0.002485203571319434  loss: 1.3740 (1.5480)  time: 1.0777  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4560/5782]  eta: 0:22:50  lr: 0.002467026524901769  loss: 1.3398 (1.5477)  time: 1.0777  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4570/5782]  eta: 0:22:39  lr: 0.0024488345851954004  loss: 1.2306 (1.5470)  time: 1.0776  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4580/5782]  eta: 0:22:28  lr: 0.0024306276168596613  loss: 1.2508 (1.5466)  time: 1.0767  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4590/5782]  eta: 0:22:16  lr: 0.00241240548218633  loss: 1.3171 (1.5463)  time: 1.0766  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4600/5782]  eta: 0:22:05  lr: 0.002394168041037979  loss: 1.4003 (1.5459)  time: 1.0775  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [4610/5782]  eta: 0:21:54  lr: 0.002375915150784178  loss: 1.3033 (1.5454)  time: 1.0770  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [4620/5782]  eta: 0:21:42  lr: 0.00235764666623546  loss: 1.3162 (1.5450)  time: 1.0769  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4630/5782]  eta: 0:21:31  lr: 0.002339362439574974  loss: 1.3143 (1.5443)  time: 1.0767  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4640/5782]  eta: 0:21:20  lr: 0.0023210623202876758  loss: 1.2690 (1.5439)  time: 1.0764  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [4650/5782]  eta: 0:21:08  lr: 0.002302746155086986  loss: 1.2958 (1.5433)  time: 1.0757  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [4660/5782]  eta: 0:20:57  lr: 0.002284413787838789  loss: 1.3014 (1.5430)  time: 1.0749  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4670/5782]  eta: 0:20:46  lr: 0.0022660650594826474  loss: 1.2382 (1.5423)  time: 1.0761  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [4680/5782]  eta: 0:20:34  lr: 0.002247699807950111  loss: 1.2382 (1.5419)  time: 1.0757  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4690/5782]  eta: 0:20:23  lr: 0.002229317868079973  loss: 1.2764 (1.5416)  time: 1.0740  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4700/5782]  eta: 0:20:12  lr: 0.002210919071530354  loss: 1.2605 (1.5409)  time: 1.0749  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4710/5782]  eta: 0:20:00  lr: 0.0021925032466874363  loss: 1.2930 (1.5407)  time: 1.0754  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4720/5782]  eta: 0:19:49  lr: 0.002174070218570712  loss: 1.2446 (1.5399)  time: 1.0746  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [4730/5782]  eta: 0:19:38  lr: 0.0021556198087345642  loss: 1.1819 (1.5393)  time: 1.0745  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [4740/5782]  eta: 0:19:26  lr: 0.0021371518351660035  loss: 1.2372 (1.5388)  time: 1.0758  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [4750/5782]  eta: 0:19:15  lr: 0.0021186661121783785  loss: 1.3335 (1.5382)  time: 1.0767  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [4760/5782]  eta: 0:19:04  lr: 0.0021001624503008596  loss: 1.2621 (1.5376)  time: 1.0763  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [4770/5782]  eta: 0:18:53  lr: 0.002081640656163474  loss: 1.2389 (1.5371)  time: 1.0766  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [4780/5782]  eta: 0:18:41  lr: 0.0020631005323774876  loss: 1.3262 (1.5366)  time: 1.0765  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [4790/5782]  eta: 0:18:30  lr: 0.0020445418774108683  loss: 1.3337 (1.5363)  time: 1.0755  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [4800/5782]  eta: 0:18:19  lr: 0.0020259644854586107  loss: 1.2013 (1.5356)  time: 1.0756  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4810/5782]  eta: 0:18:07  lr: 0.00200736814630763  loss: 1.2064 (1.5352)  time: 1.0755  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [4820/5782]  eta: 0:17:56  lr: 0.001988752645195943  loss: 1.3389 (1.5349)  time: 1.0747  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [4830/5782]  eta: 0:17:45  lr: 0.0019701177626658484  loss: 1.2985 (1.5344)  time: 1.0748  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4840/5782]  eta: 0:17:34  lr: 0.0019514632744107683  loss: 1.1785 (1.5337)  time: 1.0755  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [4850/5782]  eta: 0:17:22  lr: 0.001932788951115405  loss: 1.2339 (1.5333)  time: 1.0760  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [4860/5782]  eta: 0:17:11  lr: 0.0019140945582888694  loss: 1.2586 (1.5327)  time: 1.0756  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4870/5782]  eta: 0:17:00  lr: 0.0018953798560903616  loss: 1.2275 (1.5322)  time: 1.0753  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4880/5782]  eta: 0:16:49  lr: 0.0018766445991470122  loss: 1.2447 (1.5316)  time: 1.0758  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4890/5782]  eta: 0:16:37  lr: 0.0018578885363634177  loss: 1.2694 (1.5312)  time: 1.0753  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4900/5782]  eta: 0:16:26  lr: 0.0018391114107224181  loss: 1.3282 (1.5308)  time: 1.0748  data: 0.0057  max mem: 14987\n",
      "Epoch: [0]  [4910/5782]  eta: 0:16:15  lr: 0.0018203129590765906  loss: 1.3003 (1.5304)  time: 1.0756  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [4920/5782]  eta: 0:16:04  lr: 0.0018014929119299153  loss: 1.2542 (1.5299)  time: 1.0756  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4930/5782]  eta: 0:15:52  lr: 0.0017826509932090525  loss: 1.2045 (1.5293)  time: 1.0750  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4940/5782]  eta: 0:15:41  lr: 0.00176378692002358  loss: 1.2179 (1.5287)  time: 1.0749  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4950/5782]  eta: 0:15:30  lr: 0.0017449004024145324  loss: 1.2362 (1.5280)  time: 1.0748  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [4960/5782]  eta: 0:15:18  lr: 0.0017259911430905393  loss: 1.2499 (1.5276)  time: 1.0745  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [4970/5782]  eta: 0:15:07  lr: 0.0017070588371507624  loss: 1.3319 (1.5273)  time: 1.0740  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [4980/5782]  eta: 0:14:56  lr: 0.0016881031717938162  loss: 1.3121 (1.5268)  time: 1.0746  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [4990/5782]  eta: 0:14:45  lr: 0.0016691238260117917  loss: 1.2926 (1.5264)  time: 1.0754  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [5000/5782]  eta: 0:14:33  lr: 0.0016501204702683927  loss: 1.2327 (1.5261)  time: 1.0745  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [5010/5782]  eta: 0:14:22  lr: 0.0016310927661601711  loss: 1.1884 (1.5258)  time: 1.0743  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [5020/5782]  eta: 0:14:11  lr: 0.0016120403660597278  loss: 1.2191 (1.5252)  time: 1.0753  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [5030/5782]  eta: 0:14:00  lr: 0.0015929629127396838  loss: 1.2589 (1.5249)  time: 1.0749  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [5040/5782]  eta: 0:13:49  lr: 0.0015738600389760989  loss: 1.3009 (1.5244)  time: 1.0744  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [5050/5782]  eta: 0:13:37  lr: 0.001554731367129926  loss: 1.2751 (1.5240)  time: 1.0758  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [5060/5782]  eta: 0:13:26  lr: 0.0015355765087049847  loss: 1.2751 (1.5236)  time: 1.0770  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [5070/5782]  eta: 0:13:15  lr: 0.001516395063880762  loss: 1.2698 (1.5232)  time: 1.0769  data: 0.0058  max mem: 14987\n",
      "Epoch: [0]  [5080/5782]  eta: 0:13:04  lr: 0.001497186621018255  loss: 1.3278 (1.5232)  time: 1.0758  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [5090/5782]  eta: 0:12:52  lr: 0.0014779507561368912  loss: 1.3464 (1.5228)  time: 1.0761  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [5100/5782]  eta: 0:12:41  lr: 0.0014586870323603797  loss: 1.2357 (1.5222)  time: 1.0758  data: 0.0061  max mem: 14987\n",
      "Epoch: [0]  [5110/5782]  eta: 0:12:30  lr: 0.001439394999329161  loss: 1.1880 (1.5216)  time: 1.0738  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [5120/5782]  eta: 0:12:19  lr: 0.0014200741925769432  loss: 1.2241 (1.5212)  time: 1.0747  data: 0.0059  max mem: 14987\n",
      "Epoch: [0]  [5130/5782]  eta: 0:12:08  lr: 0.0014007241328685104  loss: 1.2352 (1.5207)  time: 1.0753  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [5140/5782]  eta: 0:11:56  lr: 0.0013813443254957986  loss: 1.2174 (1.5203)  time: 1.0744  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [5150/5782]  eta: 0:11:45  lr: 0.0013619342595288938  loss: 1.2174 (1.5198)  time: 1.0747  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [5160/5782]  eta: 0:11:34  lr: 0.0013424934070183265  loss: 1.1786 (1.5193)  time: 1.0778  data: 0.0060  max mem: 14987\n",
      "Epoch: [0]  [5170/5782]  eta: 0:11:23  lr: 0.001323021222144637  loss: 1.1786 (1.5188)  time: 1.0870  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [5180/5782]  eta: 0:11:11  lr: 0.0013035171403108132  loss: 1.1668 (1.5183)  time: 1.1005  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [5190/5782]  eta: 0:11:00  lr: 0.0012839805771727626  loss: 1.1616 (1.5178)  time: 1.1092  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [5200/5782]  eta: 0:10:49  lr: 0.001264410927602441  loss: 1.2394 (1.5174)  time: 1.1117  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [5210/5782]  eta: 0:10:38  lr: 0.001244807564577729  loss: 1.2281 (1.5168)  time: 1.1136  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [5220/5782]  eta: 0:10:27  lr: 0.0012251698379925247  loss: 1.2063 (1.5163)  time: 1.1165  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [5230/5782]  eta: 0:10:16  lr: 0.0012054970733797548  loss: 1.2275 (1.5158)  time: 1.1193  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [5240/5782]  eta: 0:10:05  lr: 0.001185788570539267  loss: 1.2275 (1.5153)  time: 1.1228  data: 0.0071  max mem: 14987\n",
      "Epoch: [0]  [5250/5782]  eta: 0:09:53  lr: 0.0011660436020616042  loss: 1.2645 (1.5149)  time: 1.1383  data: 0.0071  max mem: 14987\n",
      "Epoch: [0]  [5260/5782]  eta: 0:09:42  lr: 0.0011462614117376652  loss: 1.2915 (1.5146)  time: 1.1371  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [5270/5782]  eta: 0:09:31  lr: 0.0011264412128430513  loss: 1.2456 (1.5141)  time: 1.1232  data: 0.0068  max mem: 14987\n",
      "Epoch: [0]  [5280/5782]  eta: 0:09:20  lr: 0.0011065821862845777  loss: 1.2456 (1.5138)  time: 1.1227  data: 0.0069  max mem: 14987\n",
      "Epoch: [0]  [5290/5782]  eta: 0:09:09  lr: 0.0010866834785949182  loss: 1.2905 (1.5133)  time: 1.1207  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [5300/5782]  eta: 0:08:58  lr: 0.001066744199759559  loss: 1.2375 (1.5128)  time: 1.1197  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [5310/5782]  eta: 0:08:46  lr: 0.0010467634208582669  loss: 1.1458 (1.5122)  time: 1.1178  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [5320/5782]  eta: 0:08:35  lr: 0.0010267401715009761  loss: 1.2470 (1.5119)  time: 1.1156  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [5330/5782]  eta: 0:08:24  lr: 0.0010066734370352836  loss: 1.2998 (1.5115)  time: 1.1154  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [5340/5782]  eta: 0:08:13  lr: 0.0009865621554997035  loss: 1.3194 (1.5114)  time: 1.1145  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [5350/5782]  eta: 0:08:02  lr: 0.0009664052142932424  loss: 1.3038 (1.5110)  time: 1.1133  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [5360/5782]  eta: 0:07:51  lr: 0.0009462014465276432  loss: 1.2029 (1.5105)  time: 1.1137  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [5370/5782]  eta: 0:07:39  lr: 0.0009259496270238167  loss: 1.2360 (1.5101)  time: 1.1146  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [5380/5782]  eta: 0:07:28  lr: 0.0009056484679082129  loss: 1.2518 (1.5098)  time: 1.1144  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [5390/5782]  eta: 0:07:17  lr: 0.0008852966137581822  loss: 1.2689 (1.5093)  time: 1.1141  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [5400/5782]  eta: 0:07:06  lr: 0.0008648926362373579  loss: 1.2431 (1.5090)  time: 1.1162  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [5410/5782]  eta: 0:06:55  lr: 0.000844435028152683  loss: 1.2390 (1.5085)  time: 1.1201  data: 0.0068  max mem: 14987\n",
      "Epoch: [0]  [5420/5782]  eta: 0:06:44  lr: 0.000823922196853422  loss: 1.2318 (1.5083)  time: 1.1224  data: 0.0070  max mem: 14987\n",
      "Epoch: [0]  [5430/5782]  eta: 0:06:32  lr: 0.0008033524568789826  loss: 1.2411 (1.5079)  time: 1.1223  data: 0.0070  max mem: 14987\n",
      "Epoch: [0]  [5440/5782]  eta: 0:06:21  lr: 0.0007827240217461763  loss: 1.2307 (1.5073)  time: 1.1219  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [5450/5782]  eta: 0:06:10  lr: 0.0007620349947469335  loss: 1.1800 (1.5069)  time: 1.1208  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [5460/5782]  eta: 0:05:59  lr: 0.0007412833586036234  loss: 1.2731 (1.5066)  time: 1.1208  data: 0.0068  max mem: 14987\n",
      "Epoch: [0]  [5470/5782]  eta: 0:05:48  lr: 0.0007204669638000584  loss: 1.2718 (1.5061)  time: 1.1221  data: 0.0070  max mem: 14987\n",
      "Epoch: [0]  [5480/5782]  eta: 0:05:37  lr: 0.0006995835153704425  loss: 1.1688 (1.5056)  time: 1.1224  data: 0.0070  max mem: 14987\n",
      "Epoch: [0]  [5490/5782]  eta: 0:05:26  lr: 0.0006786305578843126  loss: 1.1960 (1.5052)  time: 1.1231  data: 0.0070  max mem: 14987\n",
      "Epoch: [0]  [5500/5782]  eta: 0:05:14  lr: 0.0006576054583104016  loss: 1.2734 (1.5048)  time: 1.1242  data: 0.0070  max mem: 14987\n",
      "Epoch: [0]  [5510/5782]  eta: 0:05:03  lr: 0.0006365053863733538  loss: 1.2437 (1.5044)  time: 1.1222  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [5520/5782]  eta: 0:04:52  lr: 0.0006153272919300263  loss: 1.1809 (1.5039)  time: 1.1215  data: 0.0069  max mem: 14987\n",
      "Epoch: [0]  [5530/5782]  eta: 0:04:41  lr: 0.0005940678787811027  loss: 1.2219 (1.5036)  time: 1.1223  data: 0.0069  max mem: 14987\n",
      "Epoch: [0]  [5540/5782]  eta: 0:04:30  lr: 0.0005727235741912586  loss: 1.2808 (1.5032)  time: 1.1230  data: 0.0070  max mem: 14987\n",
      "Epoch: [0]  [5550/5782]  eta: 0:04:19  lr: 0.0005512904932064826  loss: 1.2720 (1.5027)  time: 1.1246  data: 0.0073  max mem: 14987\n",
      "Epoch: [0]  [5560/5782]  eta: 0:04:07  lr: 0.0005297643966154476  loss: 1.2417 (1.5024)  time: 1.1233  data: 0.0071  max mem: 14987\n",
      "Epoch: [0]  [5570/5782]  eta: 0:03:56  lr: 0.000508140641082235  loss: 1.2620 (1.5020)  time: 1.1226  data: 0.0068  max mem: 14987\n",
      "Epoch: [0]  [5580/5782]  eta: 0:03:45  lr: 0.0004864141195499775  loss: 1.2897 (1.5017)  time: 1.1216  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [5590/5782]  eta: 0:03:34  lr: 0.00046457918943534286  loss: 1.2301 (1.5012)  time: 1.1204  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [5600/5782]  eta: 0:03:23  lr: 0.0004426295853375482  loss: 1.2301 (1.5008)  time: 1.1230  data: 0.0070  max mem: 14987\n",
      "Epoch: [0]  [5610/5782]  eta: 0:03:12  lr: 0.00042055831187500935  loss: 1.2665 (1.5005)  time: 1.1396  data: 0.0070  max mem: 14987\n",
      "Epoch: [0]  [5620/5782]  eta: 0:03:00  lr: 0.0003983575106880442  loss: 1.2599 (1.5002)  time: 1.1384  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [5630/5782]  eta: 0:02:49  lr: 0.00037601829337179283  loss: 1.2110 (1.4997)  time: 1.1223  data: 0.0068  max mem: 14987\n",
      "Epoch: [0]  [5640/5782]  eta: 0:02:38  lr: 0.00035353052875142596  loss: 1.2142 (1.4993)  time: 1.1232  data: 0.0071  max mem: 14987\n",
      "Epoch: [0]  [5650/5782]  eta: 0:02:27  lr: 0.0003308825678566631  loss: 1.2754 (1.4991)  time: 1.1283  data: 0.0070  max mem: 14987\n",
      "Epoch: [0]  [5660/5782]  eta: 0:02:16  lr: 0.0003080608821305418  loss: 1.2992 (1.4987)  time: 1.1284  data: 0.0070  max mem: 14987\n",
      "Epoch: [0]  [5670/5782]  eta: 0:02:05  lr: 0.00028504957794417375  loss: 1.2511 (1.4984)  time: 1.1238  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [5680/5782]  eta: 0:01:53  lr: 0.00026182972995204693  loss: 1.2644 (1.4981)  time: 1.1223  data: 0.0067  max mem: 14987\n",
      "Epoch: [0]  [5690/5782]  eta: 0:01:42  lr: 0.00023837844063473166  loss: 1.2039 (1.4975)  time: 1.1155  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [5700/5782]  eta: 0:01:31  lr: 0.0002146674702570531  loss: 1.2179 (1.4972)  time: 1.1170  data: 0.0069  max mem: 14987\n",
      "Epoch: [0]  [5710/5782]  eta: 0:01:20  lr: 0.0001906611618720946  loss: 1.3449 (1.4970)  time: 1.1171  data: 0.0068  max mem: 14987\n",
      "Epoch: [0]  [5720/5782]  eta: 0:01:09  lr: 0.0001663131437429793  loss: 1.3044 (1.4965)  time: 1.1118  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [5730/5782]  eta: 0:00:58  lr: 0.00014156075814531751  loss: 1.1797 (1.4961)  time: 1.1158  data: 0.0062  max mem: 14987\n",
      "Epoch: [0]  [5740/5782]  eta: 0:00:46  lr: 0.00011631485736293165  loss: 1.2288 (1.4957)  time: 1.1175  data: 0.0063  max mem: 14987\n",
      "Epoch: [0]  [5750/5782]  eta: 0:00:35  lr: 9.043889467479328e-05  loss: 1.2698 (1.4954)  time: 1.1171  data: 0.0064  max mem: 14987\n",
      "Epoch: [0]  [5760/5782]  eta: 0:00:24  lr: 6.369818922777975e-05  loss: 1.2698 (1.4952)  time: 1.1170  data: 0.0065  max mem: 14987\n",
      "Epoch: [0]  [5770/5782]  eta: 0:00:13  lr: 3.5594519777576054e-05  loss: 1.2513 (1.4949)  time: 1.1162  data: 0.0066  max mem: 14987\n",
      "Epoch: [0]  [5780/5782]  eta: 0:00:02  lr: 4.112725476320525e-06  loss: 1.2311 (1.4944)  time: 1.1136  data: 0.0065  max mem: 14987\n",
      "Epoch: [0] Total time: 1:47:37\n",
      "Test:  [   0/5000]  eta: 0:57:53    time: 0.6947  data: 0.5490  max mem: 14987\n",
      "Test:  [ 100/5000]  eta: 0:03:58    time: 0.0408  data: 0.0014  max mem: 14987\n",
      "Test:  [ 200/5000]  eta: 0:03:23    time: 0.0364  data: 0.0016  max mem: 14987\n",
      "Test:  [ 300/5000]  eta: 0:03:14    time: 0.0372  data: 0.0015  max mem: 14987\n",
      "Test:  [ 400/5000]  eta: 0:03:07    time: 0.0431  data: 0.0015  max mem: 14987\n",
      "Test:  [ 500/5000]  eta: 0:02:59    time: 0.0369  data: 0.0014  max mem: 14987\n",
      "Test:  [ 600/5000]  eta: 0:02:53    time: 0.0373  data: 0.0015  max mem: 14987\n",
      "Test:  [ 700/5000]  eta: 0:02:48    time: 0.0352  data: 0.0015  max mem: 14987\n",
      "Test:  [ 800/5000]  eta: 0:02:43    time: 0.0382  data: 0.0015  max mem: 14987\n",
      "Test:  [ 900/5000]  eta: 0:02:39    time: 0.0360  data: 0.0016  max mem: 14987\n",
      "Test:  [1000/5000]  eta: 0:02:34    time: 0.0387  data: 0.0015  max mem: 14987\n",
      "Test:  [1100/5000]  eta: 0:02:30    time: 0.0395  data: 0.0014  max mem: 14987\n",
      "Test:  [1200/5000]  eta: 0:02:26    time: 0.0365  data: 0.0015  max mem: 14987\n",
      "Test:  [1300/5000]  eta: 0:02:22    time: 0.0380  data: 0.0015  max mem: 14987\n",
      "Test:  [1400/5000]  eta: 0:02:17    time: 0.0360  data: 0.0014  max mem: 14987\n",
      "Test:  [1500/5000]  eta: 0:02:13    time: 0.0363  data: 0.0015  max mem: 14987\n",
      "Test:  [1600/5000]  eta: 0:02:09    time: 0.0379  data: 0.0016  max mem: 14987\n",
      "Test:  [1700/5000]  eta: 0:02:05    time: 0.0366  data: 0.0014  max mem: 14987\n",
      "Test:  [1800/5000]  eta: 0:02:01    time: 0.0420  data: 0.0015  max mem: 14987\n",
      "Test:  [1900/5000]  eta: 0:01:57    time: 0.0372  data: 0.0015  max mem: 14987\n",
      "Test:  [2000/5000]  eta: 0:01:54    time: 0.0390  data: 0.0017  max mem: 14987\n",
      "Test:  [2100/5000]  eta: 0:01:50    time: 0.0372  data: 0.0015  max mem: 14987\n",
      "Test:  [2200/5000]  eta: 0:01:46    time: 0.0380  data: 0.0014  max mem: 14987\n",
      "Test:  [2300/5000]  eta: 0:01:42    time: 0.0373  data: 0.0014  max mem: 14987\n",
      "Test:  [2400/5000]  eta: 0:01:38    time: 0.0387  data: 0.0015  max mem: 14987\n",
      "Test:  [2500/5000]  eta: 0:01:35    time: 0.0407  data: 0.0015  max mem: 14987\n",
      "Test:  [2600/5000]  eta: 0:01:31    time: 0.0414  data: 0.0015  max mem: 14987\n",
      "Test:  [2700/5000]  eta: 0:01:27    time: 0.0385  data: 0.0014  max mem: 14987\n",
      "Test:  [2800/5000]  eta: 0:01:23    time: 0.0378  data: 0.0015  max mem: 14987\n",
      "Test:  [2900/5000]  eta: 0:01:20    time: 0.0387  data: 0.0014  max mem: 14987\n",
      "Test:  [3000/5000]  eta: 0:01:16    time: 0.0379  data: 0.0014  max mem: 14987\n",
      "Test:  [3100/5000]  eta: 0:01:12    time: 0.0387  data: 0.0014  max mem: 14987\n",
      "Test:  [3200/5000]  eta: 0:01:08    time: 0.0392  data: 0.0015  max mem: 14987\n",
      "Test:  [3300/5000]  eta: 0:01:05    time: 0.0392  data: 0.0014  max mem: 14987\n",
      "Test:  [3400/5000]  eta: 0:01:01    time: 0.0380  data: 0.0014  max mem: 14987\n",
      "Test:  [3500/5000]  eta: 0:00:57    time: 0.0390  data: 0.0014  max mem: 14987\n",
      "Test:  [3600/5000]  eta: 0:00:53    time: 0.0394  data: 0.0014  max mem: 14987\n",
      "Test:  [3700/5000]  eta: 0:00:49    time: 0.0393  data: 0.0014  max mem: 14987\n",
      "Test:  [3800/5000]  eta: 0:00:46    time: 0.0395  data: 0.0016  max mem: 14987\n",
      "Test:  [3900/5000]  eta: 0:00:42    time: 0.0391  data: 0.0014  max mem: 14987\n",
      "Test:  [4000/5000]  eta: 0:00:38    time: 0.0397  data: 0.0014  max mem: 14987\n",
      "Test:  [4100/5000]  eta: 0:00:34    time: 0.0396  data: 0.0015  max mem: 14987\n",
      "Test:  [4200/5000]  eta: 0:00:30    time: 0.0414  data: 0.0013  max mem: 14987\n",
      "Test:  [4300/5000]  eta: 0:00:27    time: 0.0396  data: 0.0014  max mem: 14987\n",
      "Test:  [4400/5000]  eta: 0:00:23    time: 0.0403  data: 0.0014  max mem: 14987\n",
      "Test:  [4500/5000]  eta: 0:00:19    time: 0.0397  data: 0.0014  max mem: 14987\n",
      "Test:  [4600/5000]  eta: 0:00:15    time: 0.0406  data: 0.0014  max mem: 14987\n",
      "Test:  [4700/5000]  eta: 0:00:11    time: 0.0411  data: 0.0014  max mem: 14987\n",
      "Test:  [4800/5000]  eta: 0:00:07    time: 0.0400  data: 0.0014  max mem: 14987\n",
      "Test:  [4900/5000]  eta: 0:00:03    time: 0.0403  data: 0.0013  max mem: 14987\n",
      "Test: Total time: 0:03:14\n",
      "global correct: 90.7\n",
      "average row correct: ['94.0', '80.1', '70.0', '64.4', '60.6', '44.3', '74.1', '57.9', '87.3', '40.4', '75.4', '65.1', '75.2', '80.2', '76.4', '85.4', '43.4', '84.7', '64.2', '85.8', '58.7']\n",
      "IoU: ['89.7', '66.0', '56.9', '55.2', '43.6', '32.9', '67.0', '48.2', '72.7', '29.7', '62.4', '34.9', '58.4', '62.7', '66.6', '75.3', '29.2', '62.6', '47.1', '67.0', '50.6']\n",
      "mean IoU: 56.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvasserm/dev/ml_acceleration/assignment4/task_quantization/deeplab_quantization_ready/utils.py:295: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 1:50:52\n",
      "Test:  [  0/417]  eta: 0:35:56    time: 5.1713  data: 1.0757  max mem: 14987\n",
      "Test:  [100/417]  eta: 0:20:07    time: 3.7658  data: 0.0013  max mem: 14987\n",
      "Test:  [200/417]  eta: 0:13:35    time: 3.4748  data: 0.0013  max mem: 14987\n",
      "Test:  [300/417]  eta: 0:07:16    time: 3.4550  data: 0.0014  max mem: 14987\n",
      "Test:  [400/417]  eta: 0:01:03    time: 3.5409  data: 0.0013  max mem: 14987\n",
      "Test: Total time: 0:25:52\n",
      "global correct: 90.8\n",
      "average row correct: ['95.1', '76.2', '61.2', '65.0', '53.5', '40.5', '68.2', '51.6', '86.5', '29.5', '70.0', '51.5', '76.6', '76.4', '71.3', '84.6', '41.1', '80.6', '56.8', '83.7', '60.8']\n",
      "IoU: ['89.9', '63.8', '52.7', '54.3', '41.8', '31.5', '63.1', '43.7', '72.0', '24.0', '60.2', '32.1', '54.1', '59.5', '63.4', '74.5', '25.9', '67.6', '42.8', '63.9', '50.2']\n",
      "mean IoU: 53.8\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Print current GPU memory usage\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "\n",
    "print(f'Total: {t}, Reserved: {r}, Allocated: {a}, Free: {f}')\n",
    "\n",
    "# Вытащил дефолтные аргументы, чтобы не упражняться с argparse в ноутбуке\n",
    "with Path('./torch_default_args.pickle').open('rb') as file:\n",
    "    args = pickle.load(file)\n",
    "\n",
    "# Подобирайте под ваше железо\n",
    "args.data_path = '/home/gvasserm/data/coco2017/'\n",
    "args.epochs = 1\n",
    "args.batch_size = 16\n",
    "args.workers = 8\n",
    "\n",
    "print(args)\n",
    "\n",
    "model = deeplabv3_mobilenet_v3_large(weights=DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "if args.output_dir:\n",
    "    utils.mkdir(args.output_dir)\n",
    "\n",
    "utils.init_distributed_mode(args)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "dataset_test, num_classes = get_dataset(args, is_train=False)\n",
    "\n",
    "dataset_train, num_classes = get_dataset(args, is_train=True)\n",
    "\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "train_sampler = torch.utils.data.SequentialSampler(dataset_train)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=24, sampler=test_sampler, num_workers=args.workers, collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=24, sampler=train_sampler, num_workers=args.workers, collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "qat_model = fake_quantization(model, data_loader_train)\n",
    "qat_model.cuda()\n",
    "\n",
    "train(qat_model, model, args)\n",
    "\n",
    "# Инференс делаем на cpu, предварительно конвертируя модельку на CPU\n",
    "qat_model.cpu()\n",
    "int_qat_model = convert_fx(qat_model)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=12, sampler=test_sampler, num_workers=args.workers, collate_fn=utils.collate_fn\n",
    ")\n",
    "confmat = evaluate(int_qat_model, data_loader_test, device='cpu', num_classes=num_classes)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    FP32    Static  QAT     QAT + KD\n",
    "\n",
    "IOU     56.4    48.4        50.1        53.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
