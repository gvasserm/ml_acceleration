{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb5a756-de82-4cac-b32c-1439cb983410",
   "metadata": {},
   "source": [
    "## Установим зависимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0744c951-1366-4669-afe1-118ab51f9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pip -U\n",
    "# !pip install torch==2.2.* torchvision==0.17\n",
    "# !pip install polygraphy==0.49.9\n",
    "# !pip install tensorrt==8.6.* --extra-index-url https://pypi.nvidia.com\n",
    "# !pip install onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e326bc7f-80e7-4ef1-a2d1-685baed4e15d",
   "metadata": {},
   "source": [
    "## Подготовим полезные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b640a2f1-f012-417a-8dd4-3a9159d0fe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imagenette2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvasserm/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 79.21%\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader \n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.imagenette import Imagenette\n",
    "\n",
    "import os\n",
    "\n",
    "CLASSES_MAPPING = {\n",
    "    0: 0,\n",
    "    1: 217,\n",
    "    2: 848,\n",
    "    3: 491,\n",
    "    4: 497,\n",
    "    5: 566,\n",
    "    6: 569,\n",
    "    7: 571,\n",
    "    8: 574,\n",
    "    9: 701,\n",
    "}\n",
    "\n",
    "root_dir = \"/home/gvasserm/dev/ml_acceleration/imagenette/\"\n",
    "print(os.listdir(root_dir))\n",
    "def imagenette_dataloader(batch_size, height, width, split=\"val\"):    \n",
    "    dataset = Imagenette(\n",
    "        root=root_dir, \n",
    "        split=split, \n",
    "        download=False,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize((height, width)), \n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], inplace=True),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "def validate(model, batch_size, height, width):\n",
    "    val_dataloder = imagenette_dataloader(batch_size, height, width)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        acc = []\n",
    "        for images, labels in val_dataloder:\n",
    "            output = model(images.cuda())\n",
    "            _, predicted_labels = torch.max(output, dim=1)\n",
    "            predicted_labels = predicted_labels.cpu().tolist()\n",
    "            \n",
    "            acc += [predicted_label == CLASSES_MAPPING[label] for predicted_label, label in zip(predicted_labels, labels.tolist())]\n",
    "            \n",
    "        print(f\"acc = {sum(acc) * 100 / len(acc):.2f}%\")\n",
    "\n",
    "from torchvision.models import mobilenet_v2\n",
    "from torchvision.models import MobileNetV2\n",
    "\n",
    "\n",
    "model = mobilenet_v2(weights=MobileNetV2).eval().cuda()\n",
    "validate(model, 16, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802605d-c34f-48d5-92f8-004bcd861ed1",
   "metadata": {},
   "source": [
    "## Воспроизвидите функцию для замера latency из лекции (10 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b74157-6822-461f-bf8e-5c1e31a91344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "def latency_benchmark(model, test_input, warmup_n=10, benchmark_n=100):\n",
    "    # model - модель для замеров\n",
    "    # test_input - тестовый пример\n",
    "    # warmup_n - кол-во шагов для warmup\n",
    "    # benchmark_n - кол-во шагов для замера латенси\n",
    "\n",
    "    # Warm-up phase: run the model several times to stabilize performance\n",
    "    for _ in range(warmup_n):\n",
    "        model(test_input)\n",
    "        torch.cuda.synchronize()  # Wait for CUDA to finish\n",
    "\n",
    "    bsz = np.float64(test_input.shape[0])\n",
    "\n",
    "    # Benchmark phase: collect execution times\n",
    "    timings = []\n",
    "    for _ in range(benchmark_n):\n",
    "        start_time = time.time()\n",
    "        model(test_input)\n",
    "        torch.cuda.synchronize()  # Ensure model has finished processing\n",
    "        end_time = time.time()\n",
    "        elapsed_time = (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "        timings.append(elapsed_time/bsz)\n",
    "\n",
    "    # Calculate mean and standard deviation of the timings\n",
    "    mean_ms = np.mean(timings)\n",
    "    std_ms = np.std(timings)\n",
    "    \n",
    "    print(f\"{mean_ms:.3f}ms +- {std_ms:.3f}ms\")\n",
    "\n",
    "    assert (std_ms / mean_ms) < 0.1, \"слишком большое отклонение в измерения (> 10%), проверте код, возможно стоит поднять кол-во запусков\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41fd9d-397e-433b-939e-9fcf3e54c87b",
   "metadata": {},
   "source": [
    "## Проверяем как работает функция замера latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54543867-a23a-4fc1-8ce5-7e0ef313eae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.321ms +- 0.216ms\n"
     ]
    }
   ],
   "source": [
    "# запускаем под no_grad, чтобы минимизировать потребление памяти (исключает выделение памяти под градиенты)\n",
    "with torch.no_grad():\n",
    "    latency_benchmark(\n",
    "        model, \n",
    "        torch.ones(1, 3, 640, 480, device=\"cuda\"), \n",
    "        warmup_n=10, \n",
    "        benchmark_n=100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c9f393-be5c-4d77-a6da-04c17413c093",
   "metadata": {},
   "source": [
    "## Напишите функцию для записи CUDA graph (10 баллов)\n",
    "\n",
    "Функция должна вернуть объект CUDAGraph с записанным графом, входной тензор для передачи данных и выходной тензор для копирования результатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b80e4a8-ada9-4aba-8ff9-3a79688f67d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_CUDA_graph(model, batch_size, height, width, warmup_n=10):\n",
    "    # model - модель для записи cuda Graph\n",
    "    # batch_size - размер батча входных данных\n",
    "    # height - высота картинки\n",
    "    # width - ширина картинки\n",
    "    # warmup_n - кол-во шагов для warmup\n",
    "\n",
    "    # Move the model to GPU and set it to evaluation mode\n",
    "    model.to('cuda').eval()\n",
    "\n",
    "    # Create a random input tensor with the specified dimensions on CUDA\n",
    "    input_tensor = torch.randn(batch_size, 3, height, width, device='cuda')\n",
    "\n",
    "    # Create a new CUDA stream for capturing the graph\n",
    "    stream = torch.cuda.Stream()\n",
    "\n",
    "    # Warm-up phase: run the model several times on the non-default stream to stabilize performance\n",
    "    with torch.cuda.stream(stream):\n",
    "        for _ in range(warmup_n):\n",
    "            model(input_tensor)\n",
    "        # Synchronize the stream to ensure all operations are completed\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Instantiate a CUDA graph and start capturing on the specified stream\n",
    "    with torch.cuda.stream(stream):\n",
    "        graph = torch.cuda.CUDAGraph()\n",
    "        graph.capture_begin()\n",
    "        # Perform the computation you want to capture\n",
    "        output_tensor = model(input_tensor)\n",
    "        graph.capture_end()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    return graph, input_tensor, output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097415bd-4e33-42dd-b38c-52486608edf1",
   "metadata": {},
   "source": [
    "## Проверяем как работает функция записи CUDA graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bbbc31f-2bf6-483f-8152-17f9a668c04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success cuda graph\n"
     ]
    }
   ],
   "source": [
    "bsz, ch, height, width = 1, 3, 224, 224\n",
    "\n",
    "graph, input_placeholder, output_placeholder = record_CUDA_graph(model, bsz, height, width, warmup_n=10)\n",
    "\n",
    "test_data = torch.ones(bsz, ch, height, width, device=\"cuda\")\n",
    "# запускаем под no_grad, чтобы минимизировать потребление памяти (исключает выделение памяти под градиенты)\n",
    "with torch.no_grad():\n",
    "    # запускаем исходную модель\n",
    "    model_output = model(test_data)\n",
    "    \n",
    "    # запускаем graph\n",
    "    input_placeholder.copy_(test_data)\n",
    "    graph.replay()\n",
    "    graph_output = output_placeholder.clone()\n",
    "    \n",
    "    # сравниваем выходы\n",
    "    assert torch.all(model_output == graph_output), \"выход оригинальной модели и CUDA graph не совпадают\"\n",
    "\n",
    "    print(\"Success cuda graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36440acf-e0bb-43c2-ad8d-55f2dd9ce6aa",
   "metadata": {},
   "source": [
    "## Сравниваем latency оригинальной модели и CUDA graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9183fa5-47fa-4783-980e-5245f9ced99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.523ms +- 0.019ms\n",
      "0.852ms +- 0.073ms\n"
     ]
    }
   ],
   "source": [
    "def graph_runner(input_data):\n",
    "    input_placeholder.copy_(input_data)\n",
    "    graph.replay()\n",
    "    return output_placeholder\n",
    "\n",
    "bsz, ch, height, width = 32, 3, 224, 224\n",
    "test_data = torch.ones(bsz, ch, height, width, device=\"cuda\")\n",
    "\n",
    "# запускаем под no_grad, чтобы минимизировать потребление памяти (исключает выделение памяти под градиенты)\n",
    "with torch.no_grad():\n",
    "    latency_benchmark(\n",
    "        model, \n",
    "        test_data, \n",
    "        warmup_n=20, \n",
    "        benchmark_n=500,\n",
    "    )\n",
    "\n",
    "    bsz, ch, height, width =  1, 3, 224, 224\n",
    "    test_data = torch.ones(bsz, ch, height, width, device=\"cuda\")\n",
    "    latency_benchmark(\n",
    "        graph_runner, \n",
    "        test_data, \n",
    "        warmup_n=20, \n",
    "        benchmark_n=500,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e146e6-057f-48ea-8da5-3230ea17279a",
   "metadata": {},
   "source": [
    "## Экспортируйте torchvision модель в onnx файл (10 баллов)\n",
    "\n",
    "Для тестов нам потребуется 2 варианта onnx:\n",
    "1. все входные оси фиксированны (1, 3, 224, 224). Файл назовите \"my-model-ssss.onnx\".\n",
    "1. размер батча, высота и ширина динамические, количество входных каналов фиксированное (1, 3, height, width). Файл назовите \"my-model-dsdd.onnx\".\n",
    "\n",
    "Имя входа должно быть \"x\", имя выхода \"output\".\n",
    "\n",
    "Докуметацию по экспорту в onnx можно почитать [тут](https://pytorch.org/docs/stable/onnx_torchscript.html#torch.onnx.export)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be2ea214-190a-437e-ac97-9c4cb9adf94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "\n",
    "sample_input = torch.randn(1, 3, 224, 224, device='cpu')\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output\"]\n",
    "output_file = \"my-model-ssss.onnx\"\n",
    "\n",
    "torch.onnx.export(model,               # model being run\n",
    "                sample_input,        # model input (or a tuple for multiple inputs)\n",
    "                output_file,         # where to save the model (can be a file or file-like object)\n",
    "                export_params=True,  # store the trained parameter weights inside the model file\n",
    "                opset_version=12,    # the ONNX version to export the model to\n",
    "                do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                input_names=input_names,   # the model's input names\n",
    "                output_names=output_names)\n",
    "\n",
    "output_file = \"my-model-dsdd.onnx\"\n",
    "\n",
    "# Define dynamic axes\n",
    "dynamic_axes = {\n",
    "    'input': {0: 'batch_size', 2: 'height', 3: 'width'},  # Dynamically adjust batch size, height, and width\n",
    "    'output': {0: 'batch_size'}  # Assume output has dynamic batch size; adjust depending on model architecture\n",
    "}\n",
    "\n",
    "torch.onnx.export(model,               # model being run\n",
    "                sample_input,        # model input (or a tuple for multiple inputs)\n",
    "                output_file,         # where to save the model (can be a file or file-like object)\n",
    "                export_params=True,  # store the trained parameter weights inside the model file\n",
    "                opset_version=12,    # the ONNX version to export the model to\n",
    "                do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                input_names=input_names,   # the model's input names\n",
    "                output_names=output_names,\n",
    "                dynamic_axes=dynamic_axes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd400f-21ba-42db-9af6-0e43218111f4",
   "metadata": {},
   "source": [
    "## Скомпилируйте простейший вариант модели без динамических осей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54745689-3830-46f8-8d60-9b66553d3d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] 'colored' module is not installed, will not use colors when logging. To enable colors, please install the 'colored' module: python3 -m pip install colored\n",
      "[I] Configuring with profiles:[\n",
      "        Profile 0:\n",
      "            {input [min=[1, 3, 224, 224], opt=[1, 3, 224, 224], max=[1, 3, 224, 224]]}\n",
      "    ]\n",
      "[I] Building engine with configuration:\n",
      "    Flags                  | []\n",
      "    Engine Capability      | EngineCapability.DEFAULT\n",
      "    Memory Pools           | [WORKSPACE: 16116.69 MiB, TACTIC_DRAM: 16116.69 MiB]\n",
      "    Tactic Sources         | [CUBLAS, CUBLAS_LT, CUDNN, EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
      "[I] Finished engine building in 19.461 seconds\n",
      "[I] Saving engine to my-model-ssss.engine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorrt_bindings.tensorrt.ICudaEngine at 0x791b3df837b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from polygraphy.backend.trt import CreateConfig, Profile, Calibrator\n",
    "from polygraphy.comparator import DataLoader as DL\n",
    "from polygraphy.backend.trt import engine_from_network\n",
    "from polygraphy.backend.trt import NetworkFromOnnxPath\n",
    "from polygraphy.backend.trt import save_engine\n",
    "from polygraphy.backend.trt import TrtRunner, EngineFromBytes\n",
    "\n",
    "\n",
    "model_ssss = NetworkFromOnnxPath(\"my-model-ssss.onnx\")\n",
    "config = CreateConfig()\n",
    "\n",
    "engine = engine_from_network(model_ssss, config=config)\n",
    "save_engine(engine, path=\"my-model-ssss.engine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a5ab1e-a3f6-4407-bf35-b0880522a1b0",
   "metadata": {},
   "source": [
    "## Проверти что точность не просела\n",
    "\n",
    "**ВАЖНО:**<br>\n",
    "Опция ``copy_outputs_to_host=False`` позволяет пропустить копирование данных с GPU на CPU.<br>\n",
    "Вместо numpy array мы получим, torch.Tensor, что бывает очень полезно и экономит время на копировании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc5dd258-b355-44e0-b150-f171eff5ee2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 79.24%\n"
     ]
    }
   ],
   "source": [
    "from polygraphy.backend.trt import TrtRunner\n",
    "with open(\"my-model-ssss.engine\", \"rb\") as f:\n",
    "    engine_bytes = f.read()\n",
    "        \n",
    "engine = EngineFromBytes(engine_bytes)\n",
    "with TrtRunner(engine) as trt_runner:\n",
    "    def validation_trt_runner(input_data):\n",
    "        # пропустим копирование на CPU copy_outputs_to_host=False\n",
    "        output = trt_runner.infer(feed_dict={\"input\": input_data}, copy_outputs_to_host=False)\n",
    "        return output['output']\n",
    "\n",
    "    validate(validation_trt_runner, batch_size=1, height=224, width=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ec542-1f1b-4d3b-a4a4-b2b35f1c53de",
   "metadata": {},
   "source": [
    "## Сделайте замер latency с помощью ранее написанной функции latency_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "637d7f0a-de26-4d6c-bc4a-937f1456de6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.610ms +- 0.291ms\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "слишком большое отклонение в измерения (> 10%), проверте код, возможно стоит поднять кол-во запусков",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     output \u001b[38;5;241m=\u001b[39m trt_runner\u001b[38;5;241m.\u001b[39minfer(feed_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_data}, copy_outputs_to_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m \u001b[43mlatency_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_trt_runner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbenchmark_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m, in \u001b[0;36mlatency_benchmark\u001b[0;34m(model, test_input, warmup_n, benchmark_n)\u001b[0m\n\u001b[1;32m     28\u001b[0m std_ms \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(timings)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_ms\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mms +- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_ms\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (std_ms \u001b[38;5;241m/\u001b[39m mean_ms) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mслишком большое отклонение в измерения (> 10\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m), проверте код, возможно стоит поднять кол-во запусков\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: слишком большое отклонение в измерения (> 10%), проверте код, возможно стоит поднять кол-во запусков"
     ]
    }
   ],
   "source": [
    "with TrtRunner(engine) as trt_runner:\n",
    "    def validation_trt_runner(input_data):\n",
    "        # пропустим копирование на CPU copy_outputs_to_host=False\n",
    "        output = trt_runner.infer(feed_dict={\"input\": input_data}, copy_outputs_to_host=False)\n",
    "        return output['output']\n",
    "    latency_benchmark(validation_trt_runner, test_input=torch.ones(1, 3, 224, 224), warmup_n=10, benchmark_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597295b4-6b63-4119-9ada-315c02a351ba",
   "metadata": {},
   "source": [
    "## Теперь на основе примера выше скомпилируйте модель с динамическим batch size в диапазоне [1, 64] (5 баллов)\n",
    "**ВАЖНО:**<br>\n",
    "Как задать конфиг для динамических осей? Читаем доку [тут](https://docs.nvidia.com/deeplearning/tensorrt/polygraphy/docs/backend/trt/profile.html#optimization-profile) и добавляем профиль в config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7128ffc4-20b7-4a48-8a65-0753f727ad9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] Configuring with profiles:[\n",
      "        Profile 0:\n",
      "            {input [min=(1, 3, 224, 224), opt=(32, 3, 224, 224), max=(64, 3, 640, 640)]}\n",
      "    ]\n",
      "[I] Building engine with configuration:\n",
      "    Flags                  | []\n",
      "    Engine Capability      | EngineCapability.DEFAULT\n",
      "    Memory Pools           | [WORKSPACE: 16116.69 MiB, TACTIC_DRAM: 16116.69 MiB]\n",
      "    Tactic Sources         | [CUBLAS, CUBLAS_LT, CUDNN, EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
      "[I] Finished engine building in 32.129 seconds\n",
      "[I] Saving engine to my-model-dsdd.engine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorrt_bindings.tensorrt.ICudaEngine at 0x791b3d787330>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dsdd = NetworkFromOnnxPath(\"my-model-dsdd.onnx\")\n",
    "profiles=[\n",
    "    Profile().add('input', min=(1, 3, 224, 224), opt=(32, 3, 224, 224), max=(64, 3, 640, 640))\n",
    "]\n",
    "config = CreateConfig(profiles=profiles)\n",
    "engine = engine_from_network(model_dsdd, config=config)\n",
    "save_engine(engine, path=\"my-model-dsdd.engine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1547209-eac1-4a68-9d18-53a1411aa4d5",
   "metadata": {},
   "source": [
    "## Проверте что точность не просела и замерте latency для batch size 1 и 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ace62c09-5d23-47bc-86ec-6ffea03ccf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 79.24%\n",
      "acc = 79.24%\n"
     ]
    }
   ],
   "source": [
    "with open(\"my-model-dsdd.engine\", \"rb\") as f:\n",
    "    engine_bytes = f.read()\n",
    "    \n",
    "engine = EngineFromBytes(engine_bytes)\n",
    "\n",
    "with TrtRunner(engine) as trt_runner:\n",
    "    def validation_trt_runner(input_data):\n",
    "        # пропустим копирование на CPU copy_outputs_to_host=False\n",
    "        output = trt_runner.infer(feed_dict={\"input\": input_data}, copy_outputs_to_host=False)\n",
    "        return output['output']\n",
    "\n",
    "    validate(validation_trt_runner, batch_size=1, height=224, width=224)\n",
    "    validate(validation_trt_runner, batch_size=64, height=224, width=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113074c3-e8d0-48c4-b0a7-dd059d1cc772",
   "metadata": {},
   "source": [
    "## Скомпилируйте квантованный вариант engine c динамическим batch size [1, 64] (15 балов)\n",
    "\n",
    "Для этого вам потребуется на основе кода ``imagenette_val_dataloader`` сделать свой калибратор. Документацию на калибратор можно найти [тут](https://docs.nvidia.com/deeplearning/tensorrt/polygraphy/docs/backend/trt/calibrator.html#polygraphy.backend.trt.calibrator.Calibrator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d031fee2-3a94-476d-8c7e-03d8ca278b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] Configuring with profiles:[\n",
      "        Profile 0:\n",
      "            {input [min=(1, 3, 224, 224), opt=(32, 3, 224, 224), max=(64, 3, 640, 640)]}\n",
      "    ]\n",
      "[I] Using calibration profile: {input [min=(1, 3, 224, 224), opt=(32, 3, 224, 224), max=(64, 3, 640, 640)]}\n",
      "[W] TensorRT does not currently support using dynamic shapes during calibration. The `OPT` shapes from the calibration profile will be used for tensors with dynamic shapes. Calibration data is expected to conform to those shapes. \n",
      "[I] Building engine with configuration:\n",
      "    Flags                  | [INT8]\n",
      "    Engine Capability      | EngineCapability.DEFAULT\n",
      "    Memory Pools           | [WORKSPACE: 16116.69 MiB, TACTIC_DRAM: 16116.69 MiB]\n",
      "    Tactic Sources         | [CUBLAS, CUBLAS_LT, CUDNN, EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
      "    Calibrator             | Calibrator(<generator object polygraphy_compatible_loader at 0x791b3de6be60>, cache='calibration.cache', BaseClass=<class 'tensorrt_bindings.tensorrt.IInt8EntropyCalibrator2'>)\n",
      "[I] Loading calibration cache from calibration.cache\n",
      "[I] Finished engine building in 49.985 seconds\n",
      "[I] Saving engine to my-model-int8.engine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorrt_bindings.tensorrt.ICudaEngine at 0x791b3d736d70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def polygraphy_compatible_loader(dataloader):\n",
    "    for images, _ in dataloader:\n",
    "        yield  {\"input\": images.numpy()}  # Assuming the model only requires the image data, not labels.\n",
    "\n",
    "model_int8 = NetworkFromOnnxPath(\"my-model-dsdd.onnx\")\n",
    "profiles=[\n",
    "    Profile().add('input', min=(1, 3, 224, 224), opt=(32, 3, 224, 224), max=(64, 3, 640, 640))\n",
    "]\n",
    "\n",
    "data_loader = imagenette_dataloader(32, 224, 224, split=\"train\")\n",
    "\n",
    "calibrator = Calibrator(\n",
    "    data_loader=polygraphy_compatible_loader(data_loader),\n",
    "    cache='calibration.cache',\n",
    ")\n",
    "\n",
    "config = CreateConfig(\n",
    "    int8=True,\n",
    "    calibrator=calibrator,\n",
    "    profiles=profiles\n",
    ")\n",
    "\n",
    "engine = engine_from_network(model_int8, config=config)\n",
    "save_engine(engine, path=\"my-model-int8.engine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320c7a06-117e-476d-b592-afa9c578b42d",
   "metadata": {},
   "source": [
    "## Проверте что точность не просела и замерте latency для batch size 1 и 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d555e6e4-533e-4252-8195-8266d7f67e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 77.38%\n",
      "0.065ms +- 0.003ms\n"
     ]
    }
   ],
   "source": [
    "with open(\"my-model-int8.engine\", \"rb\") as f:\n",
    "    engine_bytes = f.read()\n",
    "    \n",
    "engine = EngineFromBytes(engine_bytes)\n",
    "\n",
    "bsz, ch, height, width = 64, 3, 224, 224\n",
    "test_data = torch.ones(bsz, ch, height, width, device=\"cuda\")\n",
    "\n",
    "with TrtRunner(engine) as trt_runner:\n",
    "    def validation_trt_runner(input_data):\n",
    "        # пропустим копирование на CPU copy_outputs_to_host=False\n",
    "        output = trt_runner.infer(feed_dict={\"input\": input_data}, copy_outputs_to_host=False)\n",
    "        return output['output']\n",
    "\n",
    "    validate(validation_trt_runner, batch_size=64, height=224, width=224)\n",
    "\n",
    "    latency_benchmark(\n",
    "        validation_trt_runner, \n",
    "        test_data, \n",
    "        warmup_n=10, \n",
    "        benchmark_n=100,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
